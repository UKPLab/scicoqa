{"paper_url":"https://arxiv.org/abs/2305.18803","paper_url_versioned":"https://arxiv.org/pdf/2305.18803v2.pdf","code_url":"https://github.com/thuml/Koopa","code_url_versioned":"https://github.com/thuml/Koopa/tree/a2e0bb77ec7c1a25e8e0579ba517ffb41358b844","code_license":"MIT License","discrepancy_date":"2024-09-04T17:15:20.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/thuml/Koopa/issues/22","origin_discrepancy_text":"About Code\nHello, I think there is a piece of code that does not match the description in the article:\r\n`        # Koopman Forecasting\r\n        residual, forecast = x_enc, None\r\n        for i in range(self.num_blocks):\r\n            time_var_input, time_inv_input = self.disentanglement(residual)\r\n            time_inv_output = self.time_inv_kps[i](time_inv_input)\r\n            time_var_backcast, time_var_output = self.time_var_kps[i](time_var_input)\r\n            residual = residual - time_var_backcast\r\n            if forecast is None:\r\n                forecast = (time_inv_output + time_var_output)\r\n            else:\r\n                forecast += (time_inv_output + time_var_output)`\r\nHere, residual is not the residual of time_var_input, but rather time_var_input with the time-invariant part added. So, I think you should write residual = time_var_input before performing the subtraction. The article describes it this way\uff1a\r\nUnlike KAEs [45, 44] that introduce a loss term for rigorous reconstruction of the lookback-window series, we feed the residual $X^{(b+1)}$ as the input of next block for learning a corrective operator. And the model forecast $Y$ is the sum of predicted components $Y_{\\text{var}}^{(b)}, Y_{\\text{inv}}^{(b)}$ gathered from all Koopa Blocks:\n\n$$\nX^{(b+1)}=X_{\\text{var}}^{(b)} - \\hat{X}_{\\text{var}}^{(b)}, \\quad Y=\\sum\\left(Y_{\\text{var}}^{(b)}+Y_{\\text{inv}}^{(b)}\\right)\n$$\r\nI\u2019m not sure if my understanding is correct, and I look forward to your response.\r\n","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper states that the next block should receive only the time-variant residual X_var^(b) - X\u0302_var^(b), whereas the code updates the residual by subtracting the time-variant backcast from the entire residual (which contains both time-variant and time-invariant parts), thereby passing the time-invariant component into the next block as well.","discrepancy_description":"The paper describes a residual update between Koopa blocks where the next block\u2019s input is the residual of only the time-variant component: X^(b+1) = X_var^(b) \u2212 X\u0302_var^(b). This follows from a block-level disentanglement X^(b) \u2192 (X_var^(b), X_inv^(b)), with Time-variant KP outputting both a fitted lookback X\u0302_var^(b) and a forecast Y_var^(b), and Time-invariant KP producing Y_inv^(b). In the code, each block disentangles residual into (time_var_input, time_inv_input), then computes a time-variant backcast and sets residual = residual \u2212 time_var_backcast, i.e., subtracting the backcast from the full residual (time_var + time_inv). As a result, the code\u2019s new residual equals X_inv^(b) + (X_var^(b) \u2212 X\u0302_var^(b)), not just X_var^(b) \u2212 X\u0302_var^(b). Therefore, the code retains and propagates the time-invariant component into subsequent blocks, contrary to the paper\u2019s stated residual definition.","relevant_paper_sections":["Unlike KAEs [45, 44] that introduce a loss term for rigorous reconstruction of the lookback-window series, we feed the residual X^{(b+1)} as the input of next block for learning a corrective operator. And the model forecast Y is the sum of predicted components Y_{\text{var}}^{(b)}, Y_{\text{inv}}^{(b)} gathered from all Koopa Blocks:\n\nX^{(b+1)}=X_{\text{var}}^{(b)} - \\hat{X}_{\text{var}}^{(b)}, \\quad Y=\\sum\\left(Y_{\text{var}}^{(b)}+Y_{\text{inv}}^{(b)}\right)","Our proposed Fourier Filter conducts disentanglement at the beginning of each block:\n\nX_{\text{var}}^{(b)}, \\quad X_{\text{inv}}^{(b)} = \text{FourierFilter}(X^{(b)}).","Time-variant KP simultaneously outputs the fitted input \\hat{X}_{\text{var}}^{(b)}, Y_{\text{var}}^{(b)} = \text{TimeVarKP}(X_{\text{var}}^{(b)})."],"relevant_code_files":["models/Koopa.py"],"discrepancy_id":"3f098eb0","removed_in_postproc":false}
{"paper_url":"https://doi.org/10.1038/s41597-023-02020-6","paper_url_versioned":"https://www.nature.com/articles/s41597-023-02020-6.pdf","code_url":"https://github.com/inventec-ai-center/bp-benchmark","code_url_versioned":"https://github.com/inventec-ai-center/bp-benchmark/tree/813e1a6a938e2698d348129f2bcc01de44450df9","code_license":"MIT License","discrepancy_date":"2025-06-20T12:06:14.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/inventec-ai-center/bp-benchmark/issues/11","origin_discrepancy_text":"Nested cross validation for hyperparameter tuning\nGood afternoon,\nThe related paper states in the hyperparameter tuning section that nested cross validation is used:\n\nHyperparameter tuning. Training and hyperparameter tuning were done using nested 5 -fold CV, stratified by subject SBP and DBP, except for the UCI dataset with HOO. We tuned ML models by grid searching the parameter-search-space shown in Table 4 and monitoring the MAE performance of validation sets. For the DL models, we used the Mean Squared Error (MSE) as the loss function, the Adam optimizer, and early stopping with the patience of 15 epochs in the validation loss. Their hyperparameters were greedily searched using the Optuna Toolkit ${ }^{62}$ to monitor the MAE performance. Table 4 lists the tuned hyperparameters.\n\nbut running the code it seems to me that tune.py script calls solver.evaluate() that uses a single cross validation loop, as generated by get_nested_fold_idx function, here's the output of generator returned by the function:\n\nFold 0:\nTrain folds: [2, 3, 4]\nValidation fold: [1]\nTest fold: [0]\nFold 1:\nTrain folds: [0, 3, 4]\nValidation fold: [2]\nTest fold: [1]\nFold 2:\nTrain folds: [0, 1, 4]\nValidation fold: [3]\nTest fold: [2]\nFold 3:\nTrain folds: [0, 1, 2]\nValidation fold: [4]\nTest fold: [3]\nFold 4:\nTrain folds: [1, 2, 3]\nValidation fold: [0]\nTest fold: [4]\n\nSo, I'd like to ask you if every Optuna trial use a single (train, validation, test) cross validation split and if the optimized metric is the one relative to the validation set or the test set (paper says validation, but tune.py script returns metrics['test/sbp_mae']).  Is that code not updated?\nWouldn't we have a data leakage if we use test metrics, as Optuna would use the test set to set model's hyperparameters and DL architectures?\n\nThanks for your consideration and congratulations for the great work you've done, it has made model comparison a lot easier!\n\n\n\n","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The tuning scripts return the test-set MAE to the Optuna/Hydra optimizer as the objective, whereas the paper states that hyperparameters are tuned by monitoring validation-set MAE. This means the code optimizes hyperparameters on test metrics, which contradicts the paper\u2019s description.","discrepancy_description":"The paper states that training and hyperparameter tuning are performed with nested 5-fold cross-validation and that hyperparameters are selected by monitoring MAE on the validation sets. In the repository, the tuning entry points (for both DL and ML) call solver.evaluate(), which runs across all folds with a train/val/test split per fold, but then return the test-set MAE to the Hydra-Optuna sweeper as the optimization objective. Concretely, tune.py returns cv_metrics[\"test/sbp_mae\"] and tune_ml.py returns cv_metrics[\"ts/{target}_mae\"], not the validation MAE. While the fold iterator (get_nested_fold_idx) does provide separate validation and test folds per outer fold (i.e., a nested split), the objective used by Optuna/Hydra is based on test-set performance rather than validation-set performance. This differs from the paper\u2019s stated procedure of selecting hyperparameters using validation MAE.","relevant_paper_sections":["Hyperparameter tuning. Training and hyperparameter tuning were done using nested 5 -fold CV, stratified by subject SBP and DBP, except for the UCI dataset with HOO. We tuned ML models by grid searching the parameter-search-space shown in Table 4 and monitoring the MAE performance of validation sets. For the DL models, we used the Mean Squared Error (MSE) as the loss function, the Adam optimizer, and early stopping with the patience of 15 epochs in the validation loss. Their hyperparameters were greedily searched using the Optuna Toolkit ${ }^{62}$ to monitor the MAE performance. Table 4 lists the tuned hyperparameters.","Validation strategies. \u2026 Finally, we split the subjects with their label distributions into K folds by iterative stratification for multi-label data ${ }^{43,44}$. This partitioning strategy is applicable for K-fold CV and HOO."],"relevant_code_files":["code/train/tune.py","code/train/tune_ml.py","code/train/core/solver_s2l.py","code/train/core/solver_f2l.py","code/train/core/utils.py"],"discrepancy_id":"183bd763","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2106.09685","paper_url_versioned":"https://arxiv.org/pdf/2106.09685v2.pdf","code_url":"https://github.com/microsoft/LoRA","code_url_versioned":"https://github.com/microsoft/LoRA/tree/a0d5efec36d74b5dce257492cc6943402573c4f3","code_license":"MIT License","discrepancy_date":"2023-07-10T03:22:51.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/microsoft/LoRA/issues/98","origin_discrepancy_text":"AB matrix initialization in layers.py does not conform to the description of the paper\n\"We use a random Gaussian initialization for A and zero for B,\u201d in paper but:\r\n`\r\n    def reset_parameters(self):\r\n\r\n        nn.Embedding.reset_parameters(self)\r\n\r\n        if hasattr(self, 'lora_A'):\r\n\r\n            # initialize A the same way as the default for nn.Linear and B to zero\r\n\r\n            nn.init.zeros_(self.lora_A)\r\n\r\n            nn.init.normal_(self.lora_B)\r\n`\r\nin layers.py","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper states that A should be randomly initialized (Gaussian) and B set to zero, but in the code the Embedding LoRA layer initializes A to zeros and B with a normal distribution, which reverses the roles. Other layers (Linear, MergedLinear, Conv) do set B to zero but use Kaiming uniform (not Gaussian) for A. Therefore, the reported mismatch exists (especially for Embedding) and is a valid discrepancy.","discrepancy_description":"In Section 4.1, the paper specifies an initialization scheme for LoRA where matrix A is given a random Gaussian initialization and matrix B is initialized to zero, ensuring \u0394W = BA is zero at the start of training. In the code, the Linear, MergedLinear, and Conv LoRA modules initialize A with Kaiming uniform (a random initialization) and B with zeros, which mostly aligns with \u201cB to zero\u201d but not the exact Gaussian distribution. However, in the Embedding LoRA module, the initialization is reversed: A is set to zeros and B is initialized with a normal (Gaussian) distribution, contradicting the paper\u2019s prescription and even the comment in the code. Thus, the Embedding implementation does not conform to the paper, and overall the code does not strictly follow the stated \u201cGaussian A, zero B\u201d initialization.","relevant_paper_sections":["We use a random Gaussian initialization for A and zero for B, so \u0394W=BA is zero at the beginning of training. We then scale \u0394W x by \u03b1/r, where \u03b1 is a constant in r. (Section 4.1)"],"relevant_code_files":["loralib/layers.py"],"discrepancy_id":"63197a77","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2106.01342","paper_url_versioned":"https://arxiv.org/pdf/2106.01342v1.pdf","code_url":"https://github.com/ogunlao/saint","code_url_versioned":"https://github.com/ogunlao/sain/tree/275834bf7372d53dade11daca5bffdad8e058b42","code_license":"MIT License","discrepancy_date":"2023-04-03T11:00:50.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/ogunlao/saint/issues/3","origin_discrepancy_text":"[def intersample() question]\nWhen reshaping queries, keys, and values in the intersample() function, shouldn\u2019t they be changed to (1, h, b, n*d)?\r\n\r\nThe code has a structure in which 8 heads per batch (records) self-attention, but the contents of the paper are self-attention by batches per head.\r\n\r\nPlease reply!","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The implementation of intersample attention reshapes q/k/v so that attention is computed across heads (h) instead of across samples (batch b), contrary to the paper\u2019s description and pseudo-code which compute attention over samples per head.","discrepancy_description":"The paper defines intersample attention as computing attention across samples in a batch: features of each sample are concatenated and attention is applied over the batch dimension, per head. Algorithm 1 explicitly reshapes to 1 \u00d7 b \u00d7 (n*d) so that the softmax attends over samples. In the code, intersample() reshapes q, k, v to 1 \u00d7 b \u00d7 h \u00d7 (n*d) and calls the generic attention, which applies softmax on the last dimension after the key transpose; with that layout, the attention is taken across the head dimension (h), not across samples. Thus, the code effectively makes heads attend to each other within each sample, instead of making samples attend to each other per head as described in the paper. To follow the paper, q/k/v should be permuted and reshaped so that the sequence dimension passed to attention is b (e.g., 1 \u00d7 h \u00d7 b \u00d7 (n*d)), with a corresponding transpose back after attention.","relevant_paper_sections":["We introduce intersample attention (a type of row attention) where the attention is computed across different data points (rows of a tabular data matrix) in a given batch rather than just the features of a single data point. Specifically, we concatenated the embeddings of each feature for a single data point, then compute attention over samples (rather than features).","Algorithm 1 PyTorch-style pseudo-code for intersample attention... def intersample_attention(x): # x is bznxd; b,n,d = x.shape; x = reshape(x, (1,b,n*d)) # reshape x to 1xbx(n*d); x = self_attention(x) # the output x is 1xbx(n*d); out = reshape(x,(b,n,d)) # out is bznxd; return out"],"relevant_code_files":["models/saint_i.py","models/transformer.py","models/saint.py"],"discrepancy_id":"61926641","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2106.12423","paper_url_versioned":"https://arxiv.org/pdf/2106.12423v2.pdf","code_url":"https://github.com/rosinality/alias-free-gan-pytorch","code_url_versioned":"https://github.com/rosinality/alias-free-gan-pytorch/tree/69850873a1907f65d2f790fc328ff5c60b0f22c1","code_license":"Other","discrepancy_date":"2021-08-11T08:49:30.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/rosinality/alias-free-gan-pytorch/issues/22","origin_discrepancy_text":"Jinc function\nHi. I wonder why you use just the jinc(||x||), whereas in the original article they use jinc(2*fc*||x||).","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The code\u2019s radially symmetric jinc-based low-pass filter uses jinc(||x||) with only an external scaling factor, whereas the paper specifies the low-pass filter as jinc(2\u00b7f_c\u00b7||x||); the argument scaling by 2\u00b7f_c (relative to the sampling rate) is missing in the code.","discrepancy_description":"The paper states that for rotation-equivariant downsampling, the ideal radially symmetric low-pass filter should be \u03c8(x) = (2 f_c)^2 \u00b7 jinc(2 f_c \u00b7 ||x||) (and its windowed/practical variant), i.e., the jinc argument must be scaled by 2\u00b7f_c times the continuous radius. In the implementation, the jinc-based filter is constructed as lowpass = (2\u00b7cutoff/sr)^2 \u00b7 jinc(||indices||) \u00b7 window, i.e., the jinc argument is the unscaled discrete radius in samples and does not multiply by 2\u00b7cutoff/sr inside the jinc call. Thus, the code omits the required argument scaling jinc(2\u00b7f_c\u00b7||x||) and instead applies only an external amplitude factor, which is not equivalent after normalization. This leads to a mismatch between the filter shape described in the paper and the one actually implemented in the code.","relevant_paper_sections":["The ideal such filter [9] is given by \u03c6_s^{\u2218}(x) = jinc(s ||x||) = 2 J1(\u03c0 s ||x||) / (\u03c0 s ||x||), where J1 is the first order Bessel function of the first kind.","The ideal radially symmetric low-pass filter [9] is given by \u03c8_s^{\u2218}(x) = (2 f_c)^2 \u00b7 jinc(2 f_c \u00b7 ||x||). The jinc function, also known as besinc, sombrero function, or Airy disk, is defined as jinc(x) = 2 J1(\u03c0 x) / (\u03c0 x)."],"relevant_code_files":["model.py"],"discrepancy_id":"7cd4b3d7","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/pdf/2202.04200.pdf","paper_url_versioned":"https://arxiv.org/pdf/2202.04200v1.pdf","code_url":"https://github.com/dome272/MaskGIT-pytorch","code_url_versioned":"https://github.com/dome272/MaskGIT-pytorch/tree/cff485ad3a14b6ed5f3aa966e045ea2bc8c68ad8","code_license":"MIT License","discrepancy_date":"2022-11-08T23:55:55.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/dome272/MaskGIT-pytorch/issues/14","origin_discrepancy_text":"Isn't loss only supposed to be calculated on masked tokens?\nIn the training loop we have:\r\n\r\n```\r\nimgs = imgs.to(device=args.device)\r\nlogits, target = self.model(imgs)\r\nloss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), target.reshape(-1))\r\nloss.backward()\r\n```\r\n\r\nHowever, the output of the transformer is:\r\n```\r\n  _, z_indices = self.encode_to_z(x)\r\n.\r\n.\r\n.\r\n  a_indices = mask * z_indices + (~mask) * masked_indices\r\n\r\n  a_indices = torch.cat((sos_tokens, a_indices), dim=1)\r\n\r\n  target = torch.cat((sos_tokens, z_indices), dim=1)\r\n\r\n  logits = self.transformer(a_indices)\r\n\r\n  return logits, target\r\n```\r\n\r\nwhich means the returned target is the original _unmasked_ image tokens.\r\n\r\nThe MaskGIT paper seems to suggest that loss was only calculated on the **masked tokens**\r\n\r\nDenote $Y_{\\mathrm{M}}$ the result after applying mask $\\mathbf{M}$ to $\\mathbf{Y}$. The training objective is to minimize the negative log-likelihood of the masked tokens:\n    \n$$\n\\mathcal{L}_{\\text {mask }}=-\\underset{\\mathbf{Y} \\in \\mathcal{D}}{\\mathbb{E}}\\left[\\sum_{\\forall i \\in[1, N], m_{i}=1} \\log p\\left(y_{i} \\mid Y_{\\overline{\\mathbf{M}}}\\right)\\right]\n$$\n\nConcretely, we feed the masked $Y_{\\overline{\\mathbf{M}}}$ into a multi-layer bidirectional transformer to predict the probabilities $P\\left(y_{i} \\mid Y_{\\overline{\\mathbf{M}}}\\right)$ for each masked token, where the negative log-likelihood is computed as the cross-entropy between the ground-truth one-hot token and predicted token. Notice the key difference to autoregressive modeling: the conditional dependency in MVTM has two directions, which allows image generation to utilize richer contexts by attending to all tokens in the image.\r\n","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper defines the loss only over masked tokens, whereas the code computes cross-entropy over all token positions (including unmasked and the SOS token) without masking the loss.","discrepancy_description":"The paper\u2019s training objective for MaskGIT minimizes the negative log-likelihood only over the masked token positions, i.e., it sums the cross-entropy loss across indices where the mask m_i = 1. Concretely, it feeds the masked sequence into a bidirectional transformer and computes cross-entropy for each masked token. In the code, the model constructs an input sequence by replacing some positions with a mask token and prepending an SOS token, but the target is the full unmasked sequence (including SOS). The training loop then computes F.cross_entropy over all positions by flattening logits and targets, with no masking applied to restrict the loss to masked positions. Thus, the implementation differs from the paper by penalizing predictions at both masked and unmasked positions (and the SOS token), rather than only the masked positions as described in the paper.","relevant_paper_sections":["Denote $Y_{\\mathrm{M}}$ the result after applying mask $\\mathbf{M}$ to $\\mathbf{Y}$. The training objective is to minimize the negative log-likelihood of the masked tokens:\n\n$$\n\\mathcal{L}_{\text {mask }}=-\\underset{\\mathbf{Y} \\in \\mathcal{D}}{\\mathbb{E}}\\left[\\sum_{\forall i \\in[1, N], m_{i}=1} \\log p\\left(y_{i} \\mid Y_{\\overline{\\mathbf{M}}}\right)\right]\n$$","Concretely, we feed the masked $Y_{\\overline{\\mathbf{M}}}$ into a multi-layer bidirectional transformer to predict the probabilities $P\\left(y_{i} \\mid Y_{\\overline{\\mathbf{M}}}\right)$ for each masked token, where the negative log-likelihood is computed as the cross-entropy between the ground-truth one-hot token and predicted token."],"relevant_code_files":["training_transformer.py","transformer.py"],"discrepancy_id":"7c8468c8","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/1904.00152","paper_url_versioned":"https://arxiv.org/pdf/1904.00152v2.pdf","code_url":"https://github.com/marrrcin/rsrlayer-pytorch","code_url_versioned":"https://github.com/marrrcin/rsrlayer-pytorch/tree/bc611cfdd4cb74398ca232aec90f68f96593f6e8","code_license":"","discrepancy_date":"2022-03-28T13:39:57.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/marrrcin/rsrlayer-pytorch/issues/1","origin_discrepancy_text":"Normalize inputs to decoder\nHi, In the paper it says. \"The output of the RSR layer is L2 normalized before applying the decoder\".  \r\n\r\nTherefore in the [code](https://github.com/marrrcin/rsrlayer-pytorch/blob/bc611cfdd4cb74398ca232aec90f68f96593f6e8/rsr_layer_pytorch.py#L73), The line should be \r\n\r\n`dec = self.decoder(F.normalize(latent, p=2))`.\r\n\r\nIs it correct or you are normalizing it somewhere else?","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper explicitly states that the output of the RSR layer is L2-normalized before it is fed into the decoder, while the provided code passes the latent (RSR output) directly to the decoder without any such normalization and does not normalize it elsewhere.","discrepancy_description":"The paper specifies that after the RSR layer projects the latent representation, the resulting vector should be L2-normalized before being input to the decoder. In the code, the RSRAutoEncoder forward pass computes enc = encoder(x), latent = rsr(enc), and then dec = decoder(latent) without applying any L2 normalization to latent. There is no other normalization step applied to the latent before the decoder, and the only post-processing seen is a sigmoid on the decoder output for the reconstruction loss. Thus, the implementation diverges from the paper by omitting the required L2 normalization step at the RSR-to-decoder interface.","relevant_paper_sections":["Batch normalization is applied to each layer of the encoders and the decoders. The output of the RSR layer is \u21132-normalized before applying the decoder.","For the preprocessed document datasets or the deep features of Tiny Imagenet, the encoder is a fully connected network with size (32, 64, 128), the RSR layer linearly maps the output of the encoder to dimension 10, and the decoder is a fully connected network with size (128, 64, 32, D) where D is the dimension of the input. Batch normalization is applied to each layer of the encoders and the decoders. The output of the RSR layer is \u21132-normalized before applying the decoder."],"relevant_code_files":["rsr_layer_pytorch.py","RSR_Layers_PyTorch.py"],"discrepancy_id":"e6dfa9fa","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/pdf/2107.06263.pdf","paper_url_versioned":"https://arxiv.org/pdf/2107.06263v3.pdf","code_url":"https://github.com/ggjy/CMT.pytorch","code_url_versioned":"https://github.com/ggjy/CMT.pytorch/tree/07e72b7db7b22b2dfc1c43eb97ec496d05c16c99","code_license":"","discrepancy_date":"2022-07-16T10:02:16.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/ggjy/CMT.pytorch/issues/2","origin_discrepancy_text":"About the sequence of the last 1x1 conv and avgpooling\nIn your paper the last 1x1 conv is behind the avgpooling, but in this implementation the last 1x1 conv is before the avgpooling. Is this a new trick or a mistake\uff1f\r\n<img width=\"550\" alt=\"image\" src=\"https://user-images.githubusercontent.com/46547949/179350026-54ffa1fe-31df-4e00-82fa-3bd44342f746.png\">\r\nx = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\nx, (H, W) = self.patch_embed_d(x)\nfor i, blk in enumerate(self.blocks_d):\n        x = blk(x, H, W, self.relative_pos_d)\n\nB, N, C = x.shape\nx = self._fc(x.permute(0, 2, 1).reshape(B, C, H, W))\nx = self._bn(x)\nx = self._swish(x)\nx = self._avg_pooling(x).flatten(start_dim=1)\nx = self._drop(x)\nx = self.pre_logits(x)\nreturn x\r\n\r\n","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper explicitly states the model ends with global average pooling followed by a projection layer, while the code applies a 1x1 convolution (with BN and activation) before global average pooling. This ordering difference confirms the discrepancy.","discrepancy_description":"The paper describes the CMT classification head as ending with a global average pooling layer, followed by a projection layer, and then the final classifier. Table 1 also indicates that both the projection and classifier operate at 1\u00d71 resolution, implying pooling happens before the projection. In the code, however, the sequence is 1\u00d71 convolution (projection) \u2192 BatchNorm \u2192 Swish \u2192 AdaptiveAvgPool2d(1) \u2192 Dropout \u2192 Linear head. Thus, the implementation applies the projection before pooling and additionally includes BN and activation prior to pooling, which differs from the paper\u2019s stated order (pooling first, then projection).","relevant_paper_sections":["The model ends with a global average pooling layer, a projection layer, and a 1000-way classification layer with softmax.","Finally, the average pooling is used to replace the class token in ViT for better classification results.","Table 1. Architectures for ImageNet classification. ... |  1 \u00d7 1 | Projection | 1 \u00d7 1,1280 | ... |  1 \u00d7 1 | Classifier | Fully Connected Layer, 1000 |"],"relevant_code_files":["cmt.py"],"discrepancy_id":"c9a26055","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2104.07012","paper_url_versioned":"https://arxiv.org/pdf/2104.07012v2.pdf","code_url":"https://github.com/lucidrains/rela-transformer","code_url_versioned":"https://github.com/lucidrains/rela-transformer/tree/d5b03354b9973c75c2a946e337b84d268700c5c4","code_license":"MIT License","discrepancy_date":"2022-04-01T17:15:49.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/lucidrains/rela-transformer/issues/1","origin_discrepancy_text":"LayerNorm/GatedRMS inconsistency \nHi!\r\nlooking through pipeline it seems there are some inconsistencies with normalisation\r\n```\r\n# ReLA\r\ninput to GRMSNorm\r\n# att code\r\noutput: Linear(inner_dim, dim) + GRMSNorm\r\n# next in FF module \r\ninput to LayerNorm\r\n```\r\nhere we have problem with double norm since we have last layer GRMSNorm in att and first layer LayerNorm in FF.\r\n\r\nlooking at the paper it seems that in ReLA GRMSNorm is applied to result of `mult(attn, v)` before output projection not after projection like in this code.\r\nI also confused about usage of LayerNorm in FF should it be GRMSNorm instead? not clear from the paper as well","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper specifies that ReLA applies (gated) RMSNorm to the attention outputs \u03b1V (on the concatenated multihead representation), whereas the code applies GatedRMSNorm both before attention (pre-norm on input x) and after the output projection (Linear \u2192 GatedRMSNorm). This placement differs from the paper\u2019s description and results in an extra normalization immediately followed by another LayerNorm at the start of the feed-forward block.","discrepancy_description":"The paper defines ReLA as applying a normalization LN to the attention outputs \u03b1V and clarifies that, in the multihead case, LN should be applied to the concatenated head representation. This implies that normalization is applied to \u03b1V (concatenated) and, by the math as written, before the standard output projection W_o. In the repository code, however, the ReLA block normalizes the input x with a GatedRMSNorm (pre-norm), then performs attention, and finally applies the output projection followed by another GatedRMSNorm (Linear \u2192 GatedRMSNorm). The following feed-forward block also starts with a LayerNorm (pre-norm). Thus, the code differs from the paper in two ways relevant to the issue: it adds a GatedRMSNorm to the attention input (not described in the paper) and applies GatedRMSNorm after the output projection rather than to \u03b1V before projection. The use of LayerNorm in the feed-forward block is not contradicted by the paper, which only specifies the added normalization for the attention outputs in ReLA and does not direct replacing FF LayerNorm with RMSNorm.","relevant_paper_sections":["ReLA(X, Y) = LN(\u03b1 V) with \u03b1 = ReLU(f(Q, K^T))","Note here, we describe our model by assuming only one attention head for clarity. In the multihead ReLA, we impose the normalization LN(\u00b7) on the concatenated head representation rather than each single head separately.","Stabilization with Normalization A common strategy in deep learning to stabilize neuron activations is to apply layer normalization (Ba et al., 2016). We follow this strategy and normalize each representation z \u2208 \u211d^{d_h} in the attention outputs (\u03b1V) with root mean square layer normalization (Zhang and Sennrich, 2019, RMSNorm): LN(z) = RMSNorm(z) = z / RMS(z) \u2299 g","ReLA-g adds a simple gating function to the normalization: LN(z) = \u03c3(w \u2299 z) \u2299 RMSNorm(z)","The only overhead due to ReLA, compared to SMATt, is the added normalization layer and it is marginal."],"relevant_code_files":["rela_transformer/rela_transformer.py"],"discrepancy_id":"567e0837","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2106.03004","paper_url_versioned":"https://arxiv.org/pdf/2106.03004v3.pdf","code_url":"https://github.com/stanislavfort/exploring_the_limits_of_OOD_detection","code_url_versioned":"https://github.com/stanislavfort/exploring_the_limits_of_OOD_detection/tree/0cbe0026737b661a0de8f8d554e57d39bf1b13ae","code_license":"MIT License","discrepancy_date":"2023-04-20T17:20:19.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/stanislavfort/exploring_the_limits_of_OOD_detection/issues/5","origin_discrepancy_text":"Calculation of Tied Covariance matrix for mahalanobis distance\nHello, as per the paper the way to calculate the tied variance is this\r\n\r\n$\\boldsymbol{\\Sigma}=\\frac{1}{N} \\sum_{c=1}^{K} \\sum_{i: y_{i}=c}\\left(f\\left(\\boldsymbol{x}_{i}\\right)-\\boldsymbol{\\mu}_{c}\\right)\\left(f\\left(\\boldsymbol{x}_{i}\\right)-\\boldsymbol{\\mu}_{c}\\right)^{\\top}$\r\n\r\nbut according to the code mentioned in the [notebook](https://github.com/stanislavfort/exploring_the_limits_of_OOD_detection/blob/main/ViT_for_strong_near_OOD_detection.ipynb)\r\n\r\nclass_means = []\nclass_cov_invs = []\nclass_covs = []\nfor c in range(indist_classes):\n\n  mean_now = np.mean(indist_train_embeds_in_touse[indist_train_labels_in == c],axis=0)\n\n  cov_now = np.cov((indist_train_embeds_in_touse[indist_train_labels_in == c]-(mean_now.reshape([1,-1]))).T)\n  class_covs.append(cov_now)\n  # print(c)\n\n  eps = 1e-8\n  cov_inv_now = np.linalg.inv(cov_now)\n\n  class_cov_invs.append(cov_inv_now)\n  class_means.append(mean_now)\n\n#the average covariance for class specific\nclass_cov_invs = [np.linalg.inv(np.mean(np.stack(class_covs,axis=0),axis=0))]*len(class_covs)\r\n\r\nHere, you are taking the average of individual covariance matrices which is not according to the formulae given the paper. So I want to know why the calculation is done this way ? Is there any specific reason ?\r\n\r\nThanks.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The code computes the shared (\u201ctied\u201d) covariance as an unweighted arithmetic mean of per-class sample covariance matrices (np.cov with ddof=n_c\u22121), whereas the paper defines the tied covariance as the pooled within-class covariance, i.e., a single scatter matrix summed over all samples with per-class centering and divided by the total number of samples. These are not equivalent in general (they only coincide up to a constant factor under restrictive conditions such as equal class sizes with ML, not unbiased, covariance), so the implementation differs from the paper\u2019s stated formula.","discrepancy_description":"The paper defines the Mahalanobis shared covariance as the pooled within-class covariance \u03a3 = (1/N) \u2211_c \u2211_{i:y_i=c} (f(x_i) \u2212 \u03bc_c)(f(x_i) \u2212 \u03bc_c)^T, i.e., a single covariance computed by summing class-centered outer products over all samples and dividing by the total sample count. In the notebook, the code first computes a sample covariance per class using np.cov with ddof=n_c\u22121, and then sets the tied covariance to the simple (unweighted) average of these per-class covariance matrices (and uses its inverse for all classes). This average-of-class-covariances (with unbiased per-class estimates and no weighting by class sizes) is not, in general, equal to the pooled covariance defined in the paper; it only matches up to a constant factor under equal class sizes with ML covariance, and otherwise differs, especially when class sizes vary or when using ddof=n_c\u22121.","relevant_paper_sections":["Mahalanobis distance Lee et al. [2018] proposed to fit a Gaussian distribution to the classconditional embeddings and use the Mahalanobis distance for OOD detection. Let f(x) denote the embedding... computing per-class mean \u03bc_c = (1/N) \u2211_{i:y_i=c} f(x_i) and a shared covariance matrix \u03a3 = (1/N) \u2211_{c=1}^{K} \u2211_{i: y_i=c}(f(x_i)\u2212\u03bc_c)(f(x_i)\u2212\u03bc_c)^\u22a4. The Mahalanobis score ... is then computed as ..."],"relevant_code_files":["ViT_for_strong_near_OOD_detection.py"],"discrepancy_id":"ac9ca85f","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2012.09841","paper_url_versioned":"https://arxiv.org/pdf/2012.09841v3.pdf","code_url":"https://github.com/CompVis/taming-transformers","code_url_versioned":"https://github.com/CompVis/taming-transformers/tree/9d17ea64b820f7633ea6b8823e1f78729447cb57","code_license":"MIT License","discrepancy_date":"2021-07-07T16:40:13.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/CompVis/taming-transformers/issues/73","origin_discrepancy_text":"Code Loss Formulation Vs in Paper\nHi, \r\n\r\nThanks for the awesome repo! I had a couple of questions about the implementation of the loss function. \r\n\r\nIn the paper you multiply the entire GAN loss by the adaptive weight, lambda. The adaptive loss is a function of the entire GAN loss (L_GAN). Two questions: \r\n\r\n1) In the code, only the generator loss is multiplied by the adaptive weight (https://github.com/CompVis/taming-transformers/blob/9d17ea64b820f7633ea6b8823e1f78729447cb57/taming/modules/losses/vqperceptual.py#L107). \r\n\r\n- For me, the adaptive weight gets fairly high and so \"loss\" is far higher than \"d_loss\". I think this is why some people are not seeing the discriminator loss decrease during training e.g. https://github.com/CompVis/taming-transformers/issues/44)\r\n\r\n2) In the code, the adaptive weight, lambda, is only a function of the generator loss (whereas I thought the GAN loss was a function of both the generator and discriminator loss). \r\n\r\nCould you offer any advice here?\r\n\r\n Thank you\r\n\r\n ","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper\u2019s Eq. (5) places the adaptive weight \u03bb in front of the entire adversarial term L_GAN inside a min\u2013max objective (thus, in principle, affecting both generator and discriminator updates), whereas the code applies the adaptive weight only to the generator\u2019s adversarial loss and not to the discriminator loss.","discrepancy_description":"In the paper, the training objective is written as a min\u2013max over generator/encoder/codebook and discriminator where the whole adversarial term L_GAN is multiplied by an adaptive weight \u03bb, and \u03bb is defined via the ratio of gradient norms with respect to the generator\u2019s last layer. In the code, the adaptive weight (d_weight) is computed from the reconstruction loss and the generator adversarial loss gradients and is applied only to the generator\u2019s adversarial loss term in the generator update; the discriminator update uses its loss multiplied by a fixed schedule factor (disc_factor) but not by d_weight/\u03bb. Therefore, the code deviates from the paper\u2019s written objective by not applying \u03bb to the discriminator\u2019s loss. Note that the second point from the issue (\u03bb depending \u201conly on the generator loss\u201d) is consistent with the paper: \u03bb is defined via \u2207_{G_L}[L_GAN], and the gradient of L_GAN with respect to generator parameters only involves the generator\u2019s \u201cfake\u201d term.","relevant_paper_sections":["The complete objective for finding the optimal compression model Q* = {E*, G*, Z*} then reads\n\nQ* = arg min_{E, G, Z} max_D E_{x ~ p(x)}[ L_VQ(E, G, Z) + \u03bb L_GAN({E, G, Z}, D) ]","where we compute the adaptive weight \u03bb according to\n\n\u03bb = \u2207_{G_L}[L_rec] / (\u2207_{G_L}[L_GAN] + \u03b4)\n\nwhere L_rec is the perceptual reconstruction loss, \u2207_{G_L}[\u00b7] denotes the gradient of its input w.r.t. the last layer L of the decoder, and \u03b4 = 10^{-6} is used for numerical stability.","We introduce an adversarial training procedure with a patch-based discriminator D that aims to differentiate between real and reconstructed images:\n\nL_GAN({E, G, Z}, D) = [log D(x) + log(1 \u2212 D(x\u0302))]"],"relevant_code_files":["taming/modules/losses/vqperceptual.py"],"discrepancy_id":"78911e70","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2209.15486","paper_url_versioned":"https://arxiv.org/pdf/2209.15486v3.pdf","code_url":"https://github.com/melifluos/subgraph-sketching","code_url_versioned":"https://github.com/melifluos/subgraph-sketchin/tree/3732cc75d8da216b41b972620dc545ff5aa1f6e1","code_license":"Apache License 2.0","discrepancy_date":"2023-06-07T16:46:23.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/melifluos/subgraph-sketching/issues/8","origin_discrepancy_text":"Possible mistake in ElphHashes\nI think the following code is incorrect: \r\n\r\nhttps://github.com/melifluos/subgraph-sketching/blob/3732cc75d8da216b41b972620dc545ff5aa1f6e1/src/hashing.py#L281-L282\r\n\r\nIt should be:\r\nfeatures[:, 7] = cards1[:, 1] - torch.sum(features[:, 0:4], dim=1) - features[:, 5]  # (2, 0)","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The code subtracts the (1,1) term twice when computing the (2,0) structural feature for k=2, which does not match the intended counting formula for B-type features described in the paper. The proposed fix removes the duplicate subtraction.","discrepancy_description":"The paper defines structural count features A_uv[d_u, d_v] and B_uv[d], where B counts nodes at distance d from one endpoint and at distance >k from the other. Specifically, B_uv[d] is computed recursively from N_d(u) by subtracting earlier B terms and A terms up to the appropriate distances. In the code path for max_hops == 2, the (2,0) feature is intended to equal |N_2(u)| minus the sum of all A terms with v-distance \u2264 2 and u-distance \u2264 2, and minus the B term (1,0). However, the implementation sets features[:, 7] = cards1[:, 1] - features[:, 0] - torch.sum(features[:, 0:4], dim=1) - features[:, 5], which subtracts features[:, 0] (the (1,1) term) twice because it is already inside sum(features[:, 0:4]). The corrected implementation should be features[:, 7] = cards1[:, 1] - torch.sum(features[:, 0:4], dim=1) - features[:, 5], which aligns with the paper\u2019s intended count decomposition.","relevant_paper_sections":["We compute \\mathcal{B}_{uv}[d] = \\sum_{d_v=k+1}^{\\infty} \\mathcal{A}_{uv}[d, d_v], counting the number of nodes at distance d from u and at distance >k from v.","\\mathcal{A}_{uv}[d_u, d_v] = |N_{d_u, d_v}(u, v)| - \\sum_{x \\le d_u, y \\le d_v, (x, y) \ne (d_u, d_v)} |N_{x, y}(u, v)|","\\mathcal{B}_{uv}[d] = |N_d(u)| - \\mathcal{B}_{uv}[d-1] - \\sum_{i=1}^{d} \\sum_{j=1}^{d} \\mathcal{A}_{uv}[i, j]","Overall, this results in k^2 counts for \\mathcal{A} and 2k counts for \\mathcal{B} (k for the source and k for the destination node)."],"relevant_code_files":["src/hashing.py"],"discrepancy_id":"0b8b9b43","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2012.09841","paper_url_versioned":"https://arxiv.org/pdf/2012.09841v2.pdf","code_url":"https://github.com/CompVis/taming-transformers","code_url_versioned":"https://github.com/CompVis/taming-transformers/tree/1bbc027acb6a47e4eb348d611f9af53f1038ffee","code_license":"MIT License","discrepancy_date":"2021-04-14T09:04:04.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/CompVis/taming-transformers/issues/40","origin_discrepancy_text":"Reconstruction loss\nIn the paper,\r\nthe authors describe that the recon loss is replaced with perceptual loss.\r\n\r\nHowever, in the code, the actual recon loss is L1 (not L2) + perceptual loss.\r\n\r\nhttps://github.com/LeeDoYup/taming-transformers/blob/1bbc027acb6a47e4eb348d611f9af53f1038ffee/taming/modules/losses/vqperceptual.py#L78-L87","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper states that the VQ-VAE reconstruction term is replaced by a perceptual loss, while the code computes a reconstruction loss as L1 pixel loss plus a perceptual (LPIPS) term. Thus, the implementation uses an additional pixel-wise term (and uses L1, not L2), which differs from the paper\u2019s description.","discrepancy_description":"In the paper, the authors first define the VQ-VAE reconstruction loss as an L2 pixel loss and then state that they replace this L2 reconstruction loss with a perceptual loss for VQGAN training. They further refer to this as the \u201cperceptual reconstruction loss\u201d when computing the adaptive weight for the adversarial term. In the code, however, the reconstruction term is implemented as the mean absolute difference (L1) between the input and reconstruction, to which a perceptual (LPIPS) penalty is added; this combined L1+LPIPS term is then used as the reconstruction/nll loss and in the adaptive weighting. Therefore, contrary to the paper\u2019s wording that suggests a pure perceptual replacement of L2, the code actually retains a pixel-wise loss (and uses L1 instead of L2) in addition to the perceptual loss.","relevant_paper_sections":["Here, L_rec = ||x \u2212 x\u0302||^2 is a reconstruction loss, sg[\u00b7] denotes the stop-gradient operation, and ||sg[z_q] \u2212 E(x)||_2^2 is the so-called \"commitment loss\" with weighting factor \u03b2 [62].","More specifically, we replace the L2 loss used in [62] for L_rec by a perceptual loss and introduce an adversarial training procedure with a patch-based discriminator D [25] that aims to differentiate between real and reconstructed images:","where we compute the adaptive weight \u03bb according to ... where L_rec is the perceptual reconstruction loss [70], \u2207_{G_L}[\u00b7] denotes the gradient of its input w.r.t. the last layer L of the decoder, and \u03b4 = 10^{-6} is used for numerical stability."],"relevant_code_files":["taming/modules/losses/vqperceptual.py","taming/models/vqgan.py"],"discrepancy_id":"cb60217c","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2303.13132","paper_url_versioned":"https://arxiv.org/pdf/2303.13132v1.pdf","code_url":"https://github.com/haoyuc/MaskedDenoising","code_url_versioned":"https://github.com/haoyuc/MaskedDenoisin/tree/9cd4c62a7a82178d86f197e11f2d0ba3ab1fbd5a","code_license":"","discrepancy_date":"2024-09-14T08:11:31.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/haoyuc/MaskedDenoising/issues/27","origin_discrepancy_text":"Implementation of Input Mask\nThe paper demonstrates that the Input Mask is used on feature after the first convolution, but the code shows the Input Mask is directly applied to the original image input (See utils/utils_mask.py function: input_mask_with_noise). Which one is right? Thanks.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper states that the Input Mask is applied to the feature tokens after the first 1\u00d71 convolution, whereas the released code applies the mask directly to the input image pixels in the data pipeline before the network. There is no code that masks the post-conv feature tokens as described.","discrepancy_description":"The paper\u2019s Method section explains that the Input Mask is applied to the \u201cfeature tokens embedded by the first convolution layer,\u201d implying masking occurs after the initial 1\u00d71 embedding conv so that tokens are replaced with a mask token in feature space. In the code, however, the input mask is implemented in the dataset loader (data/dataset_masked_denoising.py) via utils/utils_mask.py, which randomly masks input image pixels and adds noise before any network layers; the network then receives the masked image as input. There is no masking of the feature tokens immediately after conv_first in the network; only the attention mask is applied inside the self-attention layers. Therefore, the code implements input masking at the pixel level before the network, while the paper describes input masking at the feature-token level after the first embedding convolution.","relevant_paper_sections":["The Input Mask randomly masks out the feature tokens embedded by the first convolution layer, and encourages the network to complete the masked information during training.","The Transformer Architecture... a convolution layer with kernel size 1 is used as the feature embedding module to project the 3-channel pixel values into C-dimensional feature tokens. The 1 \u00d7 1 convolution layer ensures that pixels do not affect each other during feature embedding, which facilitates subsequent masking operations."],"relevant_code_files":["data/dataset_masked_denoising.py","utils/utils_mask.py","models/network_swinir.py"],"discrepancy_id":"a6c0db18","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2402.04997","paper_url_versioned":"https://arxiv.org/pdf/2402.04997v2.pdf","code_url":"https://github.com/jasonkyuyim/multiflow","code_url_versioned":"https://github.com/jasonkyuyim/multiflow/tree/6278899970523bad29953047e7a42b32a41dc813","code_license":"MIT License","discrepancy_date":"2025-05-07T19:59:48.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/jasonkyuyim/multiflow/issues/17","origin_discrepancy_text":"Questions on translation loss weight\nHi,\n\nThank you for the great work. I really enjoyed the paper particularly about how the discrete variables are treated.\n\nI have a minor question regarding to the losses about translation and rotations. In the paper, the denominator is (1-t),  where as in the code, it seems to be (1-t)^2, see the screenshot below.\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\sum_{d=1}^{D} & \\frac{\\left\\|\\hat{x}_{1}^{d}\\left(\\mathbf{T}_{t, \\tilde{t}}\\right)-x_{1}^{d}\\right\\|^{2}}{1-t}-\\log p_{\\theta}\\left(a_{1}^{d} \\mid \\mathbf{T}_{t, \\tilde{t}}\\right)\\right. \\\\\n& \\left.+\\frac{\\left\\|\\log _{r_{t}^{d}}\\left(\\hat{r}_{1}^{d}\\left(\\mathbf{T}_{t}\\right)\\right)-\\log _{r_{t}^{d}}\\left(r_{1}^{d}\\right)\\right\\|^{2}}{1-t}\\right]\n\\end{aligned}\n$$\n\n# Timestep used for normalization.\nr3_t = noisy_batch['r3_t'] # (B, 1)\nso3_t = noisy_batch['so3_t'] # (B, 1)\ncat_t = noisy_batch['cat_t'] # (B, 1)\nr3_norm_scale = 1 - torch.min(\n        r3_t[..., None], torch.tensor(training_cfg.t_normalize_clip)) # (B, 1, 1)\nso3_norm_scale = 1 - torch.min(\n        so3_t[..., None], torch.tensor(training_cfg.t_normalize_clip)) # (B, 1, 1)\n\ntrans_error = (gt_trans_1 - pred_trans_1) / r3_norm_scale * training_cfg.trans_scale\ntrans_loss = training_cfg.translation_loss_weight * torch.sum(\n        trans_error ** 2 * loss_mask[..., None],\n        dim=(-1, -2)\n) / loss_denom\ntrans_loss = torch.clamp(trans_loss, max=5)\n\nMy understanding is that (1-t)^2 in the code should be correct. Could you help double confirm it?\n\nThanks,\nRuijiang","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper\u2019s training objective divides the squared position/rotation errors by (1 \u2212 t), whereas the code divides the error by (1 \u2212 t) first and then squares it, which is equivalent to dividing by (1 \u2212 t)^2. This is a direct mismatch between the written loss in the paper and the implemented loss in the code.","discrepancy_description":"The paper describes a flow-matching training loss that predicts clean translations and rotations at time t and penalizes squared errors with a normalization factor of 1/(1 \u2212 t). Concretely, it uses terms of the form ||hat{x}_1 \u2212 x_1||^2/(1 \u2212 t) and ||log_{r_t}(hat{r}_1) \u2212 log_{r_t}(r_1)||^2/(1 \u2212 t). In the code, the implementation first divides the translation and rotation errors by (1 \u2212 t) (with clipping) and then squares them, i.e., it computes ((error)/(1 \u2212 t))^2. This makes the effective normalization 1/(1 \u2212 t)^2, not 1/(1 \u2212 t) as in the paper. The same pattern is used for both translations (r3_t) and rotations (so3_t).","relevant_paper_sections":["In order for these to match their optimum values given in Prop. 4.2, we minimize the following loss\n\n\begin{aligned}\n\\mathbb{E}\\left[\\sum_{d=1}^{D} & \frac{\\left\\|\\hat{x}_{1}^{d}\\left(\\mathbf{T}_{t, \tilde{t}}\right)-x_{1}^{d}\right\\|^{2}}{1-t}-\\log p_{\theta}\\left(a_{1}^{d} \\mid \\mathbf{T}_{t, \tilde{t}}\right) \\\\\n& \\left.+\frac{\\left\\|\\log _{r_{t}^{d}}\\left(\\hat{r}_{1}^{d}\\left(\\mathbf{T}_{t}\right)\right)-\\log _{r_{t}^{d}}\\left(r_{1}^{d}\right)\right\\|^{2}}{1-t}\right]\n\\end{aligned}","We parameterize the unconditional velocities and rate matrix in terms of these predicted quantities.\n\n\begin{aligned}\n& \nu_{x}^{d}\\left(\\mathbf{T}_{t, \tilde{t}}\right)=\frac{\\hat{x}_{1}^{d}\\left(\\mathbf{T}_{t, \tilde{t}}\right)-x_{t}^{d}}{1-t}, \\quad \nu_{r}^{d}\\left(\\mathbf{T}_{t, \tilde{t}}\right)=\frac{\\log _{r_{t}^{d}}\\left(\\hat{r}_{1}^{d}\\left(\\mathbf{T}_{t, \tilde{t}}\right)\right)}{1-t}\n\\end{aligned}"],"relevant_code_files":["multiflow/models/flow_module.py"],"discrepancy_id":"41c16fd1","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2006.12834","paper_url_versioned":"https://arxiv.org/pdf/2006.12834v2.pdf","code_url":"https://github.com/fra31/sparse-rs","code_url_versioned":"https://github.com/fra31/sparse-rs/tree/5cd68858b39088d3ae2cf7285b441de83f88b9b2","code_license":"MIT License","discrepancy_date":"2021-08-03T08:03:50.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/fra31/sparse-rs/issues/5","origin_discrepancy_text":"Paper's schedules are inconsistent with code\nHi, I am still having issues replicating your results by following the instructions and algorithms in the paper.\r\n\r\nI have found that the schedules described in the paper are not consistent with what is present in the code, and I think that schedule optimization is a very relevant issue for performance. Indeed, there is high variability of fooling rates between schedules.\r\n\r\nAs an example, this is the schedule for Patch-RS as described in the paper (page 15):\r\niteration $j \\in\\{10,50,200,500,1000,2000,4000,6000,8000\\}$\r\n\r\nAnd this is what I have found in the code (rs_attack.py 200):\r\nif 'patches' in self.norm:\n    if 10 < it <= 50:\n        p = self.p_init / 2\n    elif 50 < it <= 200:\n        p = self.p_init / 4\n    elif 200 < it <= 500:\n        p = self.p_init / 8\n    elif 500 < it <= 1000:\n        p = self.p_init / 16\n    elif 1000 < it <= 2000:\n        p = self.p_init / 32\n    elif 2000 < it <= 4000:\n        p = self.p_init / 64\n    elif 4000 < it <= 6000:\n        p = self.p_init / 128\n    elif 6000 < it <= 8000:\n        p = self.p_init / 256\n    elif 8000 < it:\n        p = self.p_init / 512\n    else:\n        p = self.p_init\r\n","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The patch-update schedule implemented in the code halves the parameter at additional iteration points (2000 and 6000) and uses different boundary conditions compared to what the paper specifies; thus the schedules are not exactly the same.","discrepancy_description":"The paper describes Patch-RS\u2019s patch-size schedule as a piecewise-constant halving of \u03b1 at specific iterations: \u03b1 starts at \u03b1_init and is halved at j \u2208 {10, 50, 200, 500, 1000, 4000, 8000} (for N = 10,000), with updates rescaled for other budgets. In contrast, the code\u2019s p_selection() for 'patches' halves p at a broader set of thresholds: after 10, 50, 200, 500, 1000, 2000, 4000, and 6000, with a further halving after 8000 (i.e., p_init/512), and uses conditions like 10 < it \u2264 50 (off-by-one at the listed thresholds). Therefore, the code adds extra halving points (2000 and 6000) and slightly different boundary handling compared to the paper\u2019s stated schedule. While both rescale the schedule to 10k queries, the implemented schedule is not identical to the one described in the paper.","relevant_paper_sections":["Patch updates. The patch is initialized by superposing at random positions on a black image 1000 squares of random size and color in {0,1}^3. Then, we update the patch following the scheme of Square Attack [4], that is sampling random squares with color one of the corners of the color cube and accept the candidate if it improves the target loss. The size of the squares is decided by a piecewise constant schedule, which we inherit from [4]. Specifically, at iteration i the square-shaped updates of a patch with size s \u00d7 s have side length s^{(i)} = sqrt(\u03b1^{(i)}) \u00b7 s, where the value of \u03b1^{(i)} starts with \u03b1_init and then is halved at iteration j \u2208 {10,50,200,500,1000,4000,8000} if the query limit is N = 10,000, otherwise the values of j are linearly rescaled accordingly."],"relevant_code_files":["rs_attacks.py"],"discrepancy_id":"96d7973b","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2403.07332","paper_url_versioned":"https://arxiv.org/pdf/2403.07332v2.pdf","code_url":"https://github.com/wjh892521292/LKM-UNet","code_url_versioned":"https://github.com/wjh892521292/LKM-UNe/tree/66157750a2ad9e1cd7de5b48d8f69bb0262244ce","code_license":"","discrepancy_date":"2024-07-24T10:08:43.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/wjh892521292/LKM-UNet/issues/5","origin_discrepancy_text":"Inconsistency Between Paper Description (Pixel-level SSM) and Code Implementation (BiPixelMambaLayer)\nHello, thank you for your inspiring work! However, I have a little confusion.\r\n\r\nIn the paper's description of Pixel-level SSM, it states, \"when these sub-kernels are sent into a Mamba layer, the local adjacent pixels will be input continuously into SSM.\" However, from [LKMUNet.py#L123](https://github.com/wjh892521292/LKM-UNet/blob/main/lkmunet/nnunetv2/nets/LKMUNet.py#L123), it appears that the BiPixelMambaLayer actually forms a token sequence with pixels of stride self.p.\r\n\r\nCould you please explain this discrepancy?","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper states that Pixel-level SSM divides the feature map into contiguous sub-windows so that local adjacent pixels are fed continuously into the Mamba sequence. In the code, BiPixelMambaLayer instead performs a space-to-depth style rearrangement that groups strided pixels (stride self.p) into submaps and then flattens them into sequences, so adjacent tokens in the SSM correspond to pixels that are p pixels apart in the original grid, not locally adjacent. Thus, the implementation does not match the paper\u2019s description.","discrepancy_description":"The paper\u2019s Pixel-level SSM (PiM) claims to split the feature map into non-overlapping contiguous sub-kernels (sub-windows) of size m\u00d7n so that local adjacent pixels are input continuously into the Mamba sequence, thereby better modeling neighborhood relations. In the code, BiPixelMambaLayer reshapes the feature into a space-to-depth representation: it rearranges the p\u00d7p spatial grid into the batch dimension and creates submaps of size (H//p, W//p) (and analogously in 3D) that consist of every p-th pixel (strided sampling). It then flattens each submap into a sequence and feeds it to Mamba. Consequently, the SSM processes sequences of strided pixels; adjacent tokens in the sequence correspond to pixels p apart in the original image, not to contiguous local neighbors. This differs from the paper\u2019s description of contiguous window partitioning that preserves continuous local adjacency within each sub-window.","relevant_paper_sections":["Pixel-level SSM (PiM). Since Mamba is a continuous model, the discrete nature of input pixels can weaken the correlation modeling of locally adjacent pixels. Hence, we propose a pixel-level SSM to split the feature map into multiple large sub-kernels (sub-windows) and perform SSM operations on the sub-kernels. We first equally divide a whole feature map into non-overlapping sub-kernels for 2D or sub-cubes for 3D. Take 2D for example. Given an input of H \u00d7 W resolution, we divide the feature map into sub-kernels of size m \u00d7 n each (m and n can be up to 40)... Under this scheme, when these sub-kernels are sent into a Mamba layer, the local adjacent pixels will be input continuously into SSM; thus, the relation between local-neighborhood pixels can be better modeled."],"relevant_code_files":["lkmunet/nnunetv2/nets/LKMUNet.py"],"discrepancy_id":"1dd22bb4","removed_in_postproc":false}
{"paper_url":"https://openreview.net/forum?id=B7ZbqNLDn-_","paper_url_versioned":"https://openreview.net/forum?id=B7ZbqNLDn-_","code_url":"https://github.com/shams-sam/FedOptim","code_url_versioned":"https://github.com/shams-sam/FedOptim/tree/381a63ea4317f5aeacb5f1d7e36a7377450e9eae","code_license":"","discrepancy_date":"2023-08-24T03:28:53.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/shams-sam/FedOptim/issues/1","origin_discrepancy_text":"\u03c4 local updates \nThe paper states that workers can perform \u03c4 local updates, accumulate the gradients before using the LBGM algorithm for calculation. But in the code, it seems that workers update without the process of accumulating gradients  every batch, which is inconsistent with the paper. May I ask if the existing code just considers \u03c4= 1? If there is the latest code, please could you upload it? Thanks!","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper explicitly describes performing \u03c4 local updates at each client and accumulating those gradients before applying LBGM, whereas the released code processes only a single minibatch per round (breaks after the first batch) and does not accumulate across \u03c4 steps; \u03c4 is present as an argument but not actually used to perform multiple local updates. This makes the implementation effectively \u03c4=1.","discrepancy_description":"The paper\u2019s FL formulation and Algorithm 1 state that each worker performs \u03c4 local updates per round and accumulates the resulting gradients g_k^(t,b) into an accumulated stochastic gradient g_k^(t)=\u2211_{b=0}^{\u03c4\u22121} g_k(\u03b8_k^{(t,b)}), which is then used by LBGM for projection and communication. In the code, the federated worker loop processes only one minibatch and immediately breaks, with the lines that would accumulate gradients across batches commented out; an assertion on \u03c4 is present but no actual multi-step accumulation is performed. Consequently, the implemented training uses only a single local update per round (effectively \u03c4=1) rather than accumulating \u03c4 local updates as described in the paper.","relevant_paper_sections":["During a vanilla FL aggregation, the global model parameters are updated as \u03b8^{(t+1)} \u2190 \u03b8^{(t)} \u2212 \u03b7 \u2211_{k=1}^{K} \u03c9_k g_k^{(t)}, where g_k^{(t)} = \u2211_{b=0}^{\u03c4\u22121} g_k(\u03b8_k^{(t,b)}) is the accumulated stochastic gradient (ASG) at worker k.","Algorithm 1 LBGM: Look-back Gradient Multiplier ... for b=0 to (\u03c4\u22121) do ... compute g_k^{(t,b)} ... Update local parameters ... and accumulate gradient: g_k^{(t)} \u2190 g_k^{(t)} + g_k^{(t,b)} ... end for"],"relevant_code_files":["src/models/train.py","src/train_federated.py","src/train_distributed.py","src/common/argparser.py"],"discrepancy_id":"b55880a0","removed_in_postproc":false}
{"paper_url":"https://openreview.net/pdf?id=HyxY6JHKwr","paper_url_versioned":"https://openreview.net/pdf?id=HyxY6JHKwr","code_url":"https://github.com/COMP6248-Reproducability-Challenge/Yoto","code_url_versioned":"https://github.com/COMP6248-Reproducability-Challenge/Yoto/tree/d2db298654f0e20f162ab22ff28b4e991d9e0b32","code_license":"","discrepancy_date":"2022-09-21T22:30:24.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/COMP6248-Reproducability-Challenge/Yoto/issues/1","origin_discrepancy_text":"Sigmoid activation on FiLM layer outputs\nThere is a sigmoid activation on gamma and beta in the FiLM layers. This makes the affine transformation only able to shift in the positive direction and the scaling becomes very limited. In the paper they actually tested trying different activations on the affine transformation variables and they all hurt performance. If you just leave the output as is without any activation you should see significant improvement. ","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper describes FiLM conditioning where the scale (\u03c3) and shift (\u03bc) are produced by MLPs and applied linearly to features, with no output nonlinearity on \u03c3 or \u03bc, while the code applies a sigmoid activation to both, constraining them to (0,1). This differs from the paper\u2019s description.","discrepancy_description":"The paper conditions layers using FiLM by mapping the loss-parameter vector \u03bb through two MLPs to obtain \u03c3 and \u03bc and then applying the affine transform f\u0303 = \u03c3 \u2299 f + \u03bc; there is no mention of any output activation on \u03c3 or \u03bc, and in the style transfer details it explicitly states that \u03c3 and \u03bc are computed via affine maps. In the repository, the FiLMBlock applies a sigmoid activation to both \u03bc and \u03c3 before using them, thereby constraining both to the (0,1) range. Thus, the code restricts the FiLM coefficients to be positive and bounded, whereas the paper\u2019s description implies unconstrained affine coefficients produced by linear layers. This is a clear deviation between the implementation and the method described in the paper.","relevant_paper_sections":["In our experiments the model F is a convolutional network. To condition the network on the loss parameters, we use Feature-wise Linear Modulation (FiLM) (Perez et al., 2018). First, we select the layers of the network to be conditioned (can be all layers or a subset). Next, we condition each of the layers on the given weight parameters \u03bb. Assume the layer outputs a feature map f of dimensions W \u00d7 H \u00d7 C, with W and H corresponding to the spatial dimensions and C to the channels. We feed the parameter vector \u03bb to two multi-layer perceptrons (MLPs) M\u03c3 and M\u03bc to generate two vectors, \u03c3 and \u03bc, of dimensionality C each. We then multiply the feature map channel-wise by \u03c3 and add \u03bc to get the transformed feature map f\u0303:  f\u0303ijk = \u03c3k fijk + \u03bck,  \u03c3 = M\u03c3(\u03bb),  \u03bc = M\u03bc(\u03bb)","We condition the models by 1) sending the logarithms of the weights to an MLP with a single hidden layer of size 512 with ReLu activations and an output dimension of size, 2) concatenating these inputs to the inception features of the style image, 3) for each layer modulation we use an MLP with a single layer of dimension 256 and ReLU activations, on which two affine maps are applied to compute the scale and shift."],"relevant_code_files":["CIFAR10Models.py","SHAPESModels.py"],"discrepancy_id":"cb60b416","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2407.02398","paper_url_versioned":"https://arxiv.org/pdf/2407.02398v1.pdf","code_url":"https://github.com/YangLing0818/consistency_flow_matching","code_url_versioned":"https://github.com/YangLing0818/consistency_flow_matchin/tree/81000db385ad21fd702d0bd6ab53d45678f0d50f","code_license":"MIT License","discrepancy_date":"2025-04-28T21:19:36.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/YangLing0818/consistency_flow_matching/issues/12","origin_discrepancy_text":"some problem about ema model\nIn your paper, you use ema model during computer loss, while in your code, it seems that no ema model used in loss function\n\nVelocity Consistency Loss While Condition 1 directly constraints the vector field to be consistent, learning vector fields that only satisfy Condition 1 may lead to trivial solutions. On the other hand, Condition 2 ensures the consistency of the vector field from a trajectory viewpoint, offering a way to directly define straight flows. Motivated by this, Consistency-FM learns a consistency vector field to satisfy both conditions:\n\n$$\n\\begin{aligned}\n& \\mathcal{L}_{\\theta}=E_{t \\sim \\mathcal{U}} E_{x_{t}, x_{t+\\Delta t}}\\left\\|f_{\\theta}\\left(t, x_{t}\\right)-f_{\\theta^{-}}\\left(t+\\Delta t, x_{t+\\Delta t}\\right)\\right\\|_{2}^{2}+\\alpha\\left\\|v_{\\theta}\\left(t, x_{t}\\right)-v_{\\theta^{-}}\\left(t+\\Delta t, x_{t+\\Delta t}\\right)\\right\\|_{2}^{2} \\\\\n& f_{\\theta}\\left(t, x_{t}\\right)=x_{t}+(1-t) * v_{\\theta}\\left(t, x_{t}\\right)\n\\end{aligned}\n$$\n\nwhere $\\mathcal{U}$ is the uniform distribution on $[0,1-\\Delta t], \\alpha$ is a positive scalar, $\\Delta t$ denotes a time interval which is a small and positive scalar. $\\theta^{-}$denotes the running average of past values of $\\theta$ using exponential moving average (EMA), $x_{t}$ and $x_{t+\\Delta t}$ follows a pre-defined distribution which can be efficiently sampled, for example, VP-SDE [1] or OT path [7]. Note that by setting $t=1$, Condition 2 implies that $\\gamma_{x}(t)+(1-t) * v\\left(t, \\gamma_{x}(t)\\right)=\\gamma_{x}(1) \\sim p_{1}$, and thus training with $\\mathcal{L}_{\\theta}$ can not only regularize the velocity but also learn the data distribution. Furthermore, if Condition 2 is met, then the straight flows $\\gamma_{x}(t)+(1-t) * v\\left(t, \\gamma_{x}(t)\\right)$ can directly predict $x_{1}$ from each time point $t$ [15].","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper\u2019s main loss explicitly uses an EMA teacher network (\u03b8\u207b) on the right-hand side targets, while the training code computes both sides with the same online model parameters and never swaps in EMA weights during loss computation. EMA is only maintained and applied for evaluation/sampling, not for forming the training targets.","discrepancy_description":"In the paper, the velocity consistency loss is defined with a teacher-student formulation that uses an EMA copy of the network parameters \u03b8\u207b for the target at time t+\u0394t: both f_{\u03b8\u207b}(t+\u0394t, x_{t+\u0394t}) and v_{\u03b8\u207b}(t+\u0394t, x_{t+\u0394t}) are computed with \u03b8\u207b. The paper explicitly states that \u03b8\u207b is the running average of past \u03b8 via EMA. In the code, the training loss function get_consistency_flow_matching_loss_fn computes vt = model(xt, t) and vr = model(xr, r) using the same current model parameters; vr is computed under torch.no_grad() but still with the online model, and the EMA is not copied into the model before computing loss. EMA is updated after each optimization step and used for evaluation/sampling (by copying EMA weights into the model), but not as a teacher in the loss. Therefore, there is a mismatch: the paper describes an EMA-teacher-based loss, while the implementation trains with an online-only loss (\u03b8\u207b = \u03b8). Although the paper later analyzes the no-EMA case (\u03b8\u207b = \u03b8) in Theorem 1, the main loss description uses EMA, which the training code does not.","relevant_paper_sections":["Velocity Consistency Loss ... Consistency-FM learns a consistency vector field to satisfy both conditions:\n\nL_\u03b8 = E_{t\u223cU} E_{x_t, x_{t+\u0394t}} || f_\u03b8(t, x_t) \u2212 f_{\u03b8\u207b}(t+\u0394t, x_{t+\u0394t}) ||_2^2 + \u03b1 || v_\u03b8(t, x_t) \u2212 v_{\u03b8\u207b}(t+\u0394t, x_{t+\u0394t}) ||_2^2,\n f_\u03b8(t, x_t) = x_t + (1\u2212t) * v_\u03b8(t, x_t)\n\n... \u03b8\u207b denotes the running average of past values of \u03b8 using exponential moving average (EMA) ...","Theorem 1. Consider no exponential moving average, i.e., \u03b8\u207b = \u03b8. ..."],"relevant_code_files":["losses.py","run_lib.py","run_lib_pytorch.py","models/ema.py"],"discrepancy_id":"cb3be166","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2412.11803","paper_url_versioned":"https://arxiv.org/pdf/2412.11803v2.pdf","code_url":"https://github.com/AmourWaltz/UAlign","code_url_versioned":"https://github.com/AmourWaltz/UAlign/tree/a40d21decece8e8e06bc752c520ea10b93e3c1b8","code_license":"","discrepancy_date":"2025-07-27T12:38:11.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/AmourWaltz/UAlign/issues/2","origin_discrepancy_text":"A question about the collection process of confidence and SE.\nThe confidence formula in the paper ( formula (1) ) is like:\n\n$$\nc_{i}=\\operatorname{Conf}\\left(\\boldsymbol{x}_{i}\\right)=\\frac{1}{K} \\sum_{k=1}^{K} \\mathbb{1}\\left(\\boldsymbol{\\hat{y}}_{i}=\\boldsymbol{y}_{i}{{}^{(k)}}\\right)\n$$\n\nI think the confidence score of the each question $x_i$ is the mean of \"greedy scores\", while you implement it by maximization:\n\n```python\ndef calculate_confidence(probabilities):\n    \"\"\"Calculate the confidence of a probability distribution.\"\"\"\n    return max(probabilities)\n```\n\nhttps://github.com/AmourWaltz/UAlign/blob/main/code/uncertainty.py#L66\n\n","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper defines confidence as the average correctness over K samples, while the code computes confidence as the maximum over the binary correctness list (i.e., 1 if any sample is correct, else 0). This is a clear mismatch between the paper\u2019s definition and the implementation.","discrepancy_description":"The paper specifies \u201caccuracy-based confidence\u201d for a question as the mean accuracy across K sampled responses, i.e., the fraction of correct generations among the K samples. In the code, however, the function calculate_confidence takes the list of per-sample correctness indicators (greedy_scores) and returns max(greedy_scores), which reduces confidence to a binary value: 1 if any sample is correct, 0 otherwise. Moreover, while the code also computes greedy_scores_avg (the intended mean), it does not use that for the confidence fed into the downstream datasets; instead, the binary max value is stored as \"confidence\" and used for reward/align data. Thus, the implemented confidence deviates from the paper\u2019s definition by using maximization rather than averaging.","relevant_paper_sections":["Accuracy-based Confidence A natural idea of aggregating varied responses is to measure the accuracy among the candidate outputs to denote confidence scores (Manakul et al., 2023; Xiong et al., 2024). Given a question x_i, the accuracy of candidate responses in Y_i by comparing with the ground-truth answer y\u0302_i serves as the confidence score c_i, computed as follows.\n\nc_i = Conf(x_i) = (1/K) \u2211_{k=1}^{K} 1(y\u0302_i = y_i^{(k)})","In the k-th sampling process for the i-th question x_i, we employ each few-shot exemplar p_k\u2208P with the question x_i to the LLM to generate the k-th response y_i^{(k)}. By taking K times of the sampling process, we can obtain an answer set Y_i = {y_i^{(k)}}_{k=1}^{K} to x_i."],"relevant_code_files":["code/uncertainty.py","code/sample.py","code/eval.py"],"discrepancy_id":"8e4dfba9","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2010.01196","paper_url_versioned":"https://arxiv.org/pdf/2010.01196v1.pdf","code_url":"https://github.com/choderalab/espaloma","code_url_versioned":"https://github.com/choderalab/espaloma/tree/d80d280acd608dc04c93966afe15cc3cb74f65a8","code_license":"MIT License","discrepancy_date":"2020-11-01T02:58:28.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/choderalab/espaloma/issues/56","origin_discrepancy_text":"Incorrect Lennard-Jones functional form\nThere\u2019s a bug in the Lennard-Jones functional form [here](https://github.com/choderalab/espaloma/blob/master/espaloma/mm/functional.py#L228-L230)\r\n```\r\n    return epsilon * (\r\n        coefficients[0] * sigma_over_x ** order[0]\r\n        - coefficients[1] * sigma_over_x ** order[1]\r\n```\r\nThe prefactor should be `4*epsilon`, not just `epsilon`.\r\nThis wouldn\u2019t impact error statistics for fitting energies, but if you\u2019re looking at MM parameter RMSE for LJ, it would matter.\r\nLikely does not impact any experiments you\u2019re running now.\r\n\r\n","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper defines the Lennard-Jones 12-6 potential with a 4\u00b7epsilon prefactor, while the code implements it with only epsilon as the prefactor.","discrepancy_description":"The paper explicitly specifies the nonbonded Lennard-Jones 12-6 interaction as U_ij(r) = 4\u00b7epsilon_ij [ (sigma_ij/r)^12 - (sigma_ij/r)^6 ]. In the code, the generic Lennard-Jones functional (used by both lj_12_6 and lj_9_6 wrappers) returns epsilon * (coefficients[0]*(sigma/r)^{order[0]} - coefficients[1]*(sigma/r)^{order[1]}), i.e., without the factor of 4 for the 12-6 case. Consequently, the code\u2019s lj_12_6 implementation omits the factor of 4 described in the paper. This is a direct mismatch between the documented MM functional form and the implemented function.","relevant_paper_sections":["U(\\mathbf{x} ; \\Phi_{\text{FF}}) = \\sum_{i<j} 4\\epsilon_{ij} \\left[ \\left( \frac{\\sigma_{ij}}{r} \right)^{12} - \\left( \frac{\\sigma_{ij}}{r} \right)^6 \right] + C \frac{q_i q_j}{r} + \\sum_{\text{bonds}} \frac{k_r}{2} (r - r_0)^2 + \\sum_{\text{angles}} \frac{k_\theta}{2} (\theta - \theta_0)^2 + \\sum_{\text{torsions}} \\sum_{n} k_{\\phi,n} \\cos [n(\\phi - \\phi_n)]."],"relevant_code_files":["espaloma/mm/functional.py","espaloma/mm/nonbonded.py"],"discrepancy_id":"9069a833","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2006.12621","paper_url_versioned":"https://arxiv.org/pdf/2006.12621v4.pdf","code_url":"https://github.com/nvedant07/Fairness-Through-Robustness","code_url_versioned":"https://github.com/nvedant07/Fairness-Through-Robustness/tree/68067f4bcde0af91eb67f4b6e5430c11126d6580","code_license":"","discrepancy_date":"2022-02-21T10:46:18.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/nvedant07/Fairness-Through-Robustness/issues/1","origin_discrepancy_text":"Problem in CIFAR10 dataloading\nhttps://github.com/nvedant07/Fairness-Through-Robustness/blob/68067f4bcde0af91eb67f4b6e5430c11126d6580/util/data_loader.py#L493-L496\r\n\r\nShouldn't it be `torchvision.datasets.CIFAR10` instead of `torchvision.datasets.CIFAR100` ?","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The CIFAR10 dataloader in the repository incorrectly instantiates torchvision.datasets.CIFAR100 instead of torchvision.datasets.CIFAR10, conflicting with the paper\u2019s stated use of the CIFAR-10 dataset.","discrepancy_description":"The paper states that experiments are conducted on CIFAR-10 (and CIFAR-100) datasets. In the code, the class named CIFAR10 (util/data_loader.py) constructs its training and test sets using torchvision.datasets.CIFAR100, not CIFAR10. This means that any code paths relying on this CIFAR10 dataloader would actually load CIFAR-100 data and class labels, contradicting the intended dataset choice described in the paper. Therefore, there is a clear mismatch between the paper\u2019s description and the implementation in this part of the repository.","relevant_paper_sections":["Datasets and Model Architectures:. We perform these tests of the datasets CIFAR-10 [34], CIFAR-100 [34] (using both 100 classes and 20 super classes), Adience [16], and UTKFace [62].","A ADDITIONAL MODEL AND DATASET DETAILS\n\nCIFAR-10, CIFAR-100, CIFAR100Super. These are standard deep learning benchmark datasets. Both CIFAR-10 and CIFAR-100 contain 60,000 images in total which are split into 50,000 train and 10, 000 test images."],"relevant_code_files":["util/data_loader.py"],"discrepancy_id":"55432721","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/1902.03642","paper_url_versioned":"https://arxiv.org/pdf/1902.03642v1.pdf","code_url":"https://github.com/sverdoot/qp-wgan","code_url_versioned":"https://github.com/sverdoot/qp-wgan/tree/4a7027ac1ded9b82770b18abe4d75b6fddeff7d2","code_license":"","discrepancy_date":"2022-11-28T10:22:54.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/sverdoot/qp-wgan/issues/1","origin_discrepancy_text":"Error in Penalty2 implementation\nHi, I think there is an error in the implementation of the P2 penalty factor\r\n\r\nhttps://github.com/sverdoot/qp-wgan/blob/4a7027ac1ded9b82770b18abe4d75b6fddeff7d2/src/qpwgan.py#L61-L62\r\nhttps://github.com/sverdoot/qp-wgan/blob/4a7027ac1ded9b82770b18abe4d75b6fddeff7d2/src/qpwgan.py#L284\r\n\r\nI think it should be \r\n```return torch.mean(torch.clamp(full_xi_vals, max=0)**2)```\r\n\r\nif you're following the formula from the paper \r\n$$\n\\begin{aligned}\n& P_{1}(\\varphi)=\\lambda_{1} \\sum_{i, j=1}^{m} \\xi\\left(x_{i}, y_{j}\\right)^{2} \\\\\n& P_{2}(\\varphi)=\\lambda_{2} \\sum_{x, y \\in B_{x} \\cup B_{y}} \\min (\\xi(x, y), 0)^{2}\n\\end{aligned}\n$$\n\r\n\r\nAm I right?","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The code penalizes positive \u03be values by clamping with min=0, whereas the paper defines P2 to penalize negative \u03be values via min(\u03be, 0)^2; this is the opposite sign and thus a genuine code-paper mismatch.","discrepancy_description":"The paper defines the second penalty P2 to enforce the admissibility constraints (\u03c6, \u03c6^c) \u2208 ADM(c) by penalizing only negative values of \u03be(x,y) = c(x,y) \u2212 \u03c6(x) \u2212 \u03c6^c(y), specifically using min(\u03be(x,y), 0)^2. In the code, P2 is implemented in admissable_penalty as torch.mean(torch.clamp(full_xi_vals, min=0)**2), which penalizes positive \u03be values (since negative values are clamped to zero). Therefore, the implementation reverses the intended sign: the paper penalizes violations where \u03be < 0, while the code penalizes cases where \u03be > 0. This directly contradicts the paper\u2019s definition for P2.","relevant_paper_sections":["Enforcing the constraints. Define\n\n$$\n\\xi(x, y)=c(x, y)-\\varphi_{\\omega^{\\prime}}(x)-\\varphi_{\\omega^{\\prime}}^{c}(y)\n$$\n\nThen, when training the discriminator, we add two penalty terms given by\n\n$$\n\begin{aligned}\n& P_{1}(\\varphi)=\\lambda_{1} \\sum_{i, j=1}^{m} \\xi\\left(x_{i}, y_{j}\right)^{2} \\\\\n& P_{2}(\\varphi)=\\lambda_{2} \\sum_{x, y \\in B_{x} \\cup B_{y}} \\min (\\xi(x, y), 0)^{2}\n\\end{aligned}\n$$\n\nHere $P_{2}$ enforces $(\\varphi, \\varphi^{c}) \\in \\operatorname{ADM}(c)$ over all elements in $B_{x} \\cup B_{y}$, and $P_{1}$ encourages pairs $(x_i, y_j)$ to belong in the support of the optimal plan."],"relevant_code_files":["src/qpwgan.py"],"discrepancy_id":"3c206242","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2105.00634","paper_url_versioned":"https://arxiv.org/pdf/2105.00634v1.pdf","code_url":"https://github.com/deepcam-cn/FaceQuality","code_url_versioned":"https://github.com/deepcam-cn/FaceQuality/tree/dfbf66d41707add18613247e5c43fd5d9564956c","code_license":"","discrepancy_date":"2022-01-14T04:07:44.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/deepcam-cn/FaceQuality/issues/10","origin_discrepancy_text":"Learning rate scheduler doesn't match as stated in the paper.\nIn the paper, you said that it would be decayed by 10 after 30, 60, 90 epoch for total of 100 epochs. But in the code, I saw that you were using CosineAnnealingRate, which doesn't have the effect as the above.\r\nAnd also, I saw that u pass T_max hard-code 10 epochs (10 * len(train_loader)) -> is this intentional? Cause this would make the LR varies in a cyclical way.\r\nThank you for reading and answering.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper specifies a step-wise LR decay at fixed epoch milestones (30/60/90 for 100 epochs; 5/10 for 15 epochs), whereas the released code uses a cosine-based per-iteration scheduler with T_max set to 10 epochs (10 * len(train_loader)), which does not match the stated schedule.","discrepancy_description":"The paper\u2019s Implementation Details state that training uses a step-wise learning rate schedule: in Step 1 and Step 3, LR starts at 0.1 and is decayed by a factor of 10 at epochs 30, 60, and 90 over 100 total epochs; in Step 2, LR starts at 0.01 and is decayed by 10 at epochs 5 and 10 over 15 total epochs. In the code, both train_feature.py and train_quality.py employ a custom cosine decay scheduler (CosineDecayLR) that updates the LR every iteration with T_max set to 10 * len(train_loader), i.e., effectively 10 epochs, plus a 1-epoch warmup, and not milestone-based decays. Therefore, the code\u2019s LR scheduling (cosine per-iteration with T_max=10 epochs) differs from the paper\u2019s described step-decay milestones (30/60/90 for 100 epochs and 5/10 for 15 epochs), confirming the discrepancy.","relevant_paper_sections":["In Step 1 of our training, the learning rate is initially set to 0.1, and decays by 10 at 30, 60, 90 epochs for a total 100 epochs. In Step 2, the learning rate is initially set to 0.01, and decays by 10 at 5, 10 epochs for a total 15 epochs. The learning rate scheme in Step 3 is same as in Step 1."],"relevant_code_files":["train_feature.py","train_quality.py","util/cosine_lr_scheduler.py","config.py"],"discrepancy_id":"47339aed","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2105.12931","paper_url_versioned":"https://arxiv.org/pdf/2105.12931v3.pdf","code_url":"https://github.com/deepcam-cn/yolov5-face","code_url_versioned":"https://github.com/deepcam-cn/yolov5-face/tree/152c688d551aefb973b7b589fb0691c93dab3564","code_license":"GNU General Public License v3.0","discrepancy_date":"2025-07-10T09:47:46.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/deepcam-cn/yolov5-face/issues/297","origin_discrepancy_text":"The confusion matrix update code is wrong\nWhen updating the confusion matrix, the order of rows and columns is reversed. \n\nThe rows should be predictions, and the columns should be ground truth. So the coordinate values \u200b\u200bof self.matrix[] should be swapped.\n\nHope you can fix the bug, since it takes me a lot of time to find it.\n\n\nn = matches.shape[0] > 0\nm0, m1, _ = matches.transpose().astype(np.int16)\nfor i, gc in enumerate(gt_classes):\n    j = m0 == i\n    if n and sum(j) == 1:\n        self.matrix[gc, detection_classes[m1[j]]] += 1  # correct\n    else:\n        self.matrix[gc, self.nc] += 1  # background FP\n\nif n:\n    for i, dc in enumerate(detection_classes):\n        if not any(m1 == i):\n            self.matrix[self.nc,","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The confusion matrix is updated as [ground-truth, prediction] (rows=GT, columns=Pred) while the plotting code and axis labels treat columns as \"True\" (ground-truth) and rows as \"Predicted\". This mismatch indicates the row/column order is reversed relative to the stated labels, so the matrix indices should be swapped when updating (or the labels/norm should be changed).","discrepancy_description":"The paper does not describe the confusion matrix, but the repository\u2019s plotting code labels the confusion matrix x-axis as \u201cTrue\u201d and y-axis as \u201cPredicted.\u201d In process_batch() (utils/metrics.py), the matrix is updated using self.matrix[gc, detection_classes[...] ] and for unmatched detections/labels it updates [gc, self.nc] and [self.nc, dc], which makes rows correspond to ground-truth and columns correspond to predicted classes. However, the plotting code normalizes by column sum and labels the x-axis as \u201cTrue\u201d and y-axis as \u201cPredicted,\u201d implying columns should be ground-truth and rows predicted. This is inconsistent with the updates. To match the labels and normalization, the updates should be swapped to index [pred, gt], i.e., self.matrix[detection_class, gc] for matched pairs, and the unmatched background updates swapped similarly.","relevant_paper_sections":[null],"relevant_code_files":["utils/metrics.py"],"discrepancy_id":"08fdd5ce","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2104.07359","paper_url_versioned":"https://arxiv.org/pdf/2104.07359v3.pdf","code_url":"https://github.com/takuomatsubara/KSD-Bayes","code_url_versioned":"https://github.com/takuomatsubara/KSD-Bayes/tree/da22c1206edf37f71e4427159ba15a4daf39ad10","code_license":"","discrepancy_date":"2025-01-07T19:29:27.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/takuomatsubara/KSD-Bayes/issues/1","origin_discrepancy_text":"Inconsistency in the Computation of out.mx in the M_Liu Function\nI was reviewing the MATLAB implementation of the [M_Liu](https://github.com/takuomatsubara/KSD-Bayes/blob/main/applications/Liu/M_Liu.m) function and noticed something that seems inconsistent in the computation of out.mx (the derivative matrix).\r\n\r\n```matlab\r\nout.mx = [- x' * (1 + x(1)^2 + x(2)^2 + x(3)^2 + x(4)^2 + x(5)^2)^(-3/2); ...\r\n          - [x(1), x(2), 0, 0, 0] * (1 + x(1)^2 + x(2)^2)^(-3/2); ...\r\n          - [x(1), 0, x(3), 0, 0] * (1 + x(1)^2 + x(2)^2)^(-3/2); ...\r\n          - [x(1), 0, 0, x(4), 0] * (1 + x(1)^2 + x(2)^2)^(-3/2); ...\r\n          - [x(1), 0, 0, 0, x(5)] * (1 + x(1)^2 + x(2)^2)^(-3/2)];\r\n ```\r\n\r\n\r\nI noticed that rows 3\u20135 reuse the denominator `(1 + x(1)^2 + x(2)^2)`, which seems inconsistent with the corresponding preconditioner formulas. Shouldn't these rows use their respective pairwise norms? For example:\r\n\r\n- **Row 3**: `(1 + x(1)^2 + x(3)^2)`\r\n- **Row 4**: `(1 + x(1)^2 + x(4)^2)`\r\n- **Row 5**: `(1 + x(1)^2 + x(5)^2)`\r\n\r\nIs this reuse of `(1 + x(1)^2 + x(2)^2)` intentional, or could it be an oversight? If intentional, could you clarify the reasoning behind using this shared denominator for rows 3\u20135?\r\n\r\n\r\n\r\n","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"In M_Liu.m the Jacobian out.mx reuses the denominator (1 + x(1)^2 + x(2)^2) for rows 3\u20135, but the corresponding components of out.m depend on (1 + x(1)^2 + x(3)^2), (1 + x(1)^2 + x(4)^2), and (1 + x(1)^2 + x(5)^2). Therefore, the derivatives in rows 3\u20135 should use their own pairwise denominators; the current code is inconsistent with the defined preconditioner and is almost certainly a copy-paste error.","discrepancy_description":"The paper specifies a robust weighting function M(x) for the Liu et al. experiment where each diagonal entry is a different function: m1(x) depends on the full norm over all five coordinates; m2(x) on (x1, x2); m3(x) on (x1, x3); m4(x) on (x1, x4); and m5(x) on (x1, x5). In the code, out.m correctly implements these five distinct functions. However, the Jacobian out.mx uses the denominator (1 + x(1)^2 + x(2)^2)^(-3/2) not only for the derivative of m2(x), but also incorrectly for the derivatives of m3(x), m4(x), and m5(x). Mathematically, the gradients of m3, m4, m5 must use (1 + x(1)^2 + x(3)^2)^(-3/2), (1 + x(1)^2 + x(4)^2)^(-3/2), and (1 + x(1)^2 + x(5)^2)^(-3/2), respectively, matching how those components are defined. Thus, the code\u2019s derivative matrix is inconsistent with the preconditioner described in the paper and with the code\u2019s own definition of out.m.","relevant_paper_sections":["Finally, in the right column of Figure 4 we display the robust generalised posterior obtained with weighting function\n\nM(x)=diag((1 + x_{(1)}^{2} + \\cdots + x_{(5)}^{2})^{-1/2}, (1 + x_{(1)}^{2} + x_{(2)}^{2})^{-1/2}, \\ldots,(1 + x_{(1)}^{2} + x_{(5)}^{2})^{-1/2))\n\nwhich ensures the criteria for bias-robustness in Theorem 3."],"relevant_code_files":["applications/Liu/M_Liu.m","KSD_Bayes.m"],"discrepancy_id":"1ffbe3cb","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2003.06221","paper_url_versioned":"https://arxiv.org/pdf/2003.06221v2.pdf","code_url":"https://github.com/rosinality/semantic-pyramid-pytorch","code_url_versioned":"https://github.com/rosinality/semantic-pyramid-pytorch/tree/6d4796cbb2abdbe9afaafb8dae75b60a3b619d2d","code_license":"MIT License","discrepancy_date":"2020-06-08T04:31:09.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/rosinality/semantic-pyramid-pytorch/issues/2","origin_discrepancy_text":"mask in reconstruction loss\nI think your reconstruction loss is missing mask, which should be applied after subtracting two features as in (3) of the paper.\r\n$$\n\\mathcal{L}_{\\text{rec}} = \\sum_{l \\in \\text{layers}} \\left\\| (C_l^* (x) - C_l^*(G(z, \\mathcal{F}, \\mathcal{M})) \\cdot m_l \\right\\|_1 \\tag{3}\n$$\r\n\r\nhttps://github.com/rosinality/semantic-pyramid-pytorch/blob/6d4796cbb2abdbe9afaafb8dae75b60a3b619d2d/train.py#L146","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper specifies that the semantic reconstruction loss should be masked by m_l (i.e., computed only on unmasked regions/levels), while the implementation computes the L1 loss over the entire feature maps and never applies the mask in the reconstruction loss loop.","discrepancy_description":"The paper defines the semantic reconstruction loss to be applied only where the mask m_l passes features, formally multiplying the loss by the spatial mask so masked-out regions do not contribute. In the code, the reconstruction loss is computed as the plain L1 distance between VGG features of the generated and real images (with 2\u00d72 max-pooling), and although the loop receives the mask tensor m for each layer, it is not used in the loss computation. Therefore, the implemented loss does not respect the masking described in the paper, leading to a discrepancy where masked regions/levels still contribute to the reconstruction loss in the code.","relevant_paper_sections":["The second term L_rec is a semantic reconstruction loss, which encourages the output image to preserve the feature information used to generate it. ... To allow the model to generate diverse image samples from high level features, we apply this loss only to the features at levels that are used for generation (not masked out). Formally, L_rec = \\sum_{l \\in \text{layers}} \\left\\| (C_l^* (x) - C_l^*(G(z, \\mathcal{F}, \\mathcal{M})) \\cdot m_l \right\\|_1 (3)","The mask can either pass the entire feature map (all ones), mask-out the entire feature map (all zeros) or pass regions from it.","At each level, the feature map f_l is first multiplied by its input mask m_l. The masked feature map then undergoes a convolution layer and the result is summed with the result of the corresponding generator block."],"relevant_code_files":["train.py","mask.py","model.py"],"discrepancy_id":"c5dec3b3","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2210.01276","paper_url_versioned":"https://arxiv.org/pdf/2210.01276v2.pdf","code_url":"https://github.com/ToniRV/NeRF-SLAM","code_url_versioned":"https://github.com/ToniRV/NeRF-SLAM/tree/4e407e8cb1378c6ece18e621b19ccd5be982b7dd","code_license":"BSD 2-Clause \"Simplified\" License","discrepancy_date":"2024-10-18T10:26:55.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/ToniRV/NeRF-SLAM/issues/77","origin_discrepancy_text":"Depth covariance computation\nDear author,\r\n\r\nI am confused by the lines for depth [covariance computation](https://github.com/ToniRV/NeRF-SLAM/blob/master/slam/visual_frontends/visual_frontend.py#L1215-#L1217) quoted below,\r\n\r\n```\r\n                F = torch.matmul(Q_ * E_sum.t(), L_inv) # K*HW x D*P\r\n                F2 = torch.pow(F, 2)\r\n                delta_cov = F2.sum(dim=-1) # K*HW\r\n```\r\nMy expected form is like below according to your probabilistic fusion paper eq 5,\r\n\r\n```\r\nF = torch.matmul(Q_ * E_sum.t(), L_inv.t()) # K*HW x D*P\r\n```\r\nNote I think there is a transpose for L_inv. Can you please clarify this?\r\n\r\n","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper derives \u03a3_d = P^{-1} + P^{-T} E^T L^{-T} L^{-1} E P^{-1} = P^{-1} + F^T F with F = L^{-1} E P^{-1}, which implies that when computing the diagonal term via the equivalent form diag(Q E^T \u03a3_T E Q) with \u03a3_T = (L L^T)^{-1}, the implementation must use L^{-T} (i.e., L_inv.t()) on the right. The code instead computes F = (Q E^T) L_inv and then diag(F F^T), which corresponds to using L^{-1} L^{-T} = (L^T L)^{-1}, not (L L^T)^{-1}. Therefore, the missing transpose on L_inv in the code is a real mismatch with the paper\u2019s formula.","discrepancy_description":"The paper\u2019s Section 3.2 derives the depth covariance from the Schur complement via the Cholesky factor L of the reduced camera matrix: \u03a3_d = P^{-1} + P^{-T} E^T L^{-T} L^{-1} E P^{-1}, and equivalently \u03a3_d = P^{-1} + F^T F with F = L^{-1} E P^{-1}. This entails that when computing the added covariance term using Q = P^{-1} and \u03a3_T = (L L^T)^{-1}, one must use L^{-T} in the product (Q E^T) L^{-T}. In the code, after obtaining the Cholesky factor L and L_inv = L^{-1}, the implementation forms F = (Q E_sum^T) @ L_inv and then uses diag(F F^T); this uses L_inv L_inv^T = (L^T L)^{-1} instead of L_inv^T L_inv = (L L^T)^{-1}. Because torch.linalg.cholesky returns the lower-triangular factor L (H = L L^T), the correct implementation requires L_inv.t() in that multiplication. Thus, the code\u2019s use of L_inv instead of L_inv.t() is inconsistent with the paper\u2019s derivation.","relevant_paper_sections":["3.2. Inverse Depth Uncertainty Estimation\n\n\u03a3_d = P^{-1} + P^{-1} E^T \u03a3_T E P^{-1}\n\u03a3_T = (H / P)^{-1}\n\n... = P^{-1} + P^{-T} E^T L^{-T} L^{-1} E P^{-1}\n= P^{-1} + F^T F\n\nwhere F = L^{-1} E P^{-1}.\n","[\u03a3_d]_i = \u03c3_{d_i}^2 = P_i^{-1} + {F^T F}_i = P_i^{-1} + \u2211_k F_{k i}^2\n"],"relevant_code_files":["slam/visual_frontends/visual_frontend.py"],"discrepancy_id":"b881aa62","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2211.07609","paper_url_versioned":"https://arxiv.org/pdf/2211.07609v1.pdf","code_url":"https://github.com/chen742/PiPa","code_url_versioned":"https://github.com/chen742/PiPa/tree/2a90d4f848d15ff642ad940950bd0a8fddd78dac","code_license":"","discrepancy_date":"2023-06-22T15:15:13.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/chen742/PiPa/issues/13","origin_discrepancy_text":"Have you used imagenet feature distance?\nFrom the code you released, the imagenet feature distance is used in your work. But in your paper, the fd loss is not included in the total loss. Can you explain it. Thank you very much!","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The released training code enables and optimizes an ImageNet feature distance (FD) loss, while the paper\u2019s \"Total Loss\" section only lists source/target cross-entropy plus the two proposed PiPa losses and does not include an FD term.","discrepancy_description":"In the paper, the total training objective is described as the sum of source-domain cross-entropy, target-domain cross-entropy, and the two proposed self-supervised losses (pixel-wise and patch-wise), with weights \u03b1 and \u03b2. In the code, the training pipeline additionally enables the ImageNet feature distance (FD) loss from the DAFormer/HRDA baseline, controlled by imnet_feature_dist_lambda and computed against an ImageNet-pretrained model; this term is backpropagated during training. The difference is that the paper\u2019s loss formula omits the FD term, whereas the provided configs and implementation include it (e.g., via dacs_a999_fdthings.py), meaning the code uses an extra regularization loss not explicitly reflected in the paper\u2019s stated total loss.","relevant_paper_sections":["Total Loss. The overall training objective is the combination of pixel-level cross-entropy loss and the proposed PiPa:\n\n\\( \\mathcal{L}_{\text {total }}=\\mathcal{L}_{\\mathrm{ce}}^{S}+\\mathcal{L}_{\\mathrm{ce}}^{T}+\\alpha \\mathcal{L}_{\text {Pixel }}+\beta \\mathcal{L}_{\text {Patch }} \\)\n\nwhere \\(\\alpha\\) and \\(\beta\\) are the weights for pixel-wise contrast and patch-wise contrast, respectively.","Implementation details. ... We adopt AdamW as the optimizer, ... ClassMix ... and empirically set pseudo labels threshold 0.968 following [43]. ... The hyperparameters of the loss function are chosen empirically \\(\\alpha=\beta=0.1\\)."],"relevant_code_files":["mmseg/models/uda/dacs.py","configs/_base_/uda/dacs_a999_fdthings.py","PiPa_DAFormer/mmseg/models/uda/dacs.py","PiPa_DAFormer/configs/_base_/uda/dacs_a999_fdthings.py","PiPa_DAFormer/configs/pipa/gta2cs_uda_warm_fdthings_rcs_croppl_a999_daformer_mitb5_s0.py","configs/pipa/gtaHR2csHR_hrda.py"],"discrepancy_id":"881beca2","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/1902.09513","paper_url_versioned":"https://arxiv.org/pdf/1902.09513v2.pdf","code_url":"https://github.com/measaverb/FEELVOS","code_url_versioned":"https://github.com/measaverb/FEELVOS/tree/e9f497753ea52a91c6180e9f45fb87810898d309","code_license":"MIT License","discrepancy_date":"2021-03-30T03:56:31.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/measaverb/FEELVOS/issues/5","origin_discrepancy_text":"Inconsistent implementation with paper\nHi, \r\n\r\nI found that there is an inconsistent implementation of Eq. (1). As the [distance](https://github.com/kim-younghan/FEELVOS/blob/master/feelvos/models/Matching.py#L9) defines, the distance in Equation (1) is actually:\r\n$$d(p, q) = 1 - \\frac{2}{1 + \\exp(\\|e_p - e_q\\|^2)}.\\tag{1}$$\r\n\r\nI would like to know any differences?\r\n\r\nBest,\r\nJia\r\n","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The code computes the exponential term using the absolute difference of squared norms (|||e_p||^2 \u2212 ||e_q||^2|) instead of the squared L2 norm of the vector difference (||e_p \u2212 e_q||^2) as defined in Eq. (1) of the paper.","discrepancy_description":"The paper defines the pixel-wise embedding distance as d(p, q) = 1 \u2212 2/(1 + exp(||e_p \u2212 e_q||^2)), i.e., it uses the squared Euclidean distance between embedding vectors inside the exponential. In the code, the distance function first computes ps = sum(p*p) and qs = sum(q*q), i.e., the squared norms of each embedding, and then uses torch.norm(ps - qs) (effectively the absolute difference between these scalars) inside the exponential: 1 \u2212 2/(1 + exp(| ||p||^2 \u2212 ||q||^2 |)). Thus, the implementation uses the difference of squared norms rather than the squared norm of the difference vector. This deviates from Eq. (1) and constitutes an inconsistency between the paper and the code.","relevant_paper_sections":["Similar to Fathi et al. [12], we define the distance between pixels p and q in terms of their corresponding embedding vectors e_p and e_q by d(p, q) = 1 - \frac{2}{1 + \\exp(\\|e_p - e_q\\|^2)}. (1)","The distance values are always between 0 and 1. For identical pixels, the embedding distance is d(p, p) = 1 \u2212 2/(1 + exp(0)) = 0, and for pixels which are very far away in the embedding space, we have d(p, q) = 1 \u2212 2/(1 + exp(\u221e)) = 1."],"relevant_code_files":["feelvos/models/Matching.py"],"discrepancy_id":"ec7b02db","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2304.01933","paper_url_versioned":"https://arxiv.org/pdf/2304.01933v3.pdf","code_url":"https://github.com/AGI-Edgerunners/LLM-Adapters","code_url_versioned":"https://github.com/AGI-Edgerunners/LLM-Adapters/tree/816657208af4db747803f87ba40a4c71383fed7a","code_license":"Apache License 2.0","discrepancy_date":"2025-02-06T13:57:37.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/AGI-Edgerunners/LLM-Adapters/issues/76","origin_discrepancy_text":"Prompt templates in your paper did not align with your code.\nTake PIQA for example, in your paper:\n\n> Please choose the correct solution to the question: [QUESTION]\n> Solution1: [SOLUTION_1]\n> Solution2: [SOLUTION_2]\n> Answer format: solution1/solution2\n> the correct answer is [ANSWER]\n\n\nbut in your code:\n\n> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n> \n> ### Instruction:\n> Please choose the correct solution to the question: [QUESTION]\n> Solution1: [SOLUTION_1]\n> Solution2: [SOLUTION_2]\n> ### Response:\n> the correct answer is [ANSWER]\n\n\nSince you are fine-tuning base model (not instruction-tuned), It makes quite a huge difference, please help me clarify this, thanks!","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper\u2019s Appendix A.2 shows dataset-specific templates (e.g., for PIQA) without any outer instruction wrapper, while the released code wraps every prompt inside an Alpaca-style instruction preamble with \"Below is an instruction...\" and \"### Instruction/### Response\" markers for both training and evaluation.","discrepancy_description":"The paper describes commonsense fine-tuning data using concise, dataset-specific templates; for PIQA, it shows a prompt that includes the question, two solutions, an \u201cAnswer format\u201d line, and then the target line \u201cthe correct answer is [ANSWER]\u201d. In the code, however, these dataset-specific instructions are further wrapped inside an Alpaca-like meta-instruction: \u201cBelow is an instruction that describes a task...\u201d, with \u201c### Instruction:\u201d preceding the dataset-specific text and \u201c### Response:\u201d preceding the answer, both during training (finetune.py) and evaluation (commonsense_evaluate.py). Thus, the actual prompts used in code contain an additional instruction preamble not documented in the paper\u2019s template table, which is a mismatch between the paper\u2019s stated templates and the code\u2019s implemented prompt format.","relevant_paper_sections":["To facilitate fine-tuning in the domain of commonsense reasoning, we construct fine-tuning data by formatting the training sets from BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, and OBQA with pre-defined templates. As each dataset in the commonsense reasoning domain entails distinct tasks, we adopt a structured template by initially describing the task\u2019s goal, followed by the corresponding content and answer. The template utilized for creating the fine-tuning data can be found in A.2.","Table 7: The data template of each dataset used to create commonsense reasoning data for fine-tuning. PIQA | Please choose the correct solution to the question: [QUESTION] Solution1: [SOLUTION_1] Solution2: [SOLUTION_2] Answer format: solution1/solution2 the correct answer is [ANSWER]"],"relevant_code_files":["finetune.py","commonsense_evaluate.py","evaluate.py","generate.py"],"discrepancy_id":"fec5862c","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2305.16223","paper_url_versioned":"https://arxiv.org/pdf/2305.16223v2.pdf","code_url":"https://github.com/SHI-Labs/Prompt-Free-Diffusion","code_url_versioned":"https://github.com/SHI-Labs/Prompt-Free-Diffusion/tree/f4295fc7ad80763e2ab562eb9aab06abb979b278","code_license":"MIT License","discrepancy_date":"2023-11-21T06:07:24.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/SHI-Labs/Prompt-Free-Diffusion/issues/23","origin_discrepancy_text":"Incorrect implementation of self-attention\nYour paper specifies that the Decoder section performs a stacked multi-head self-attention operation, however I have found in the code that the behavior of the DecoderLayer class is inconsistent with the above description. By printing the attn_output_weights of the self_attn module, I found attention map shaped '([L, 1, 1])', and there is clearly a problem with such an attention computation. #22 ","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The Decoder in the code feeds tensors with shape [B, N, C] directly into nn.MultiheadAttention without setting batch_first=True or transposing to [L, N, C], so attention is computed across the batch dimension rather than the token sequence. This yields degenerate attention weights of shape (~N, 1, 1) when B=1, contrary to the paper\u2019s description of stacked self-attention over the concatenated token sequence.","discrepancy_description":"The paper states that the SeeCoder Decoder concatenates flattened multi-scale features and passes them through six stacked multi-head self-attention layers with appropriate projections and LayerNorms. In the code, DecoderLayer uses PyTorch\u2019s nn.MultiheadAttention but passes inputs in [B, N, C] format without batch_first=True or transposing to [L, N, C], which is the default expected format. As a result, MultiheadAttention interprets batch size as sequence length and token count as batch size, so the attention is computed across the batch axis (often B=1), producing attention maps of shape (N, 1, 1) instead of (B, N, N). This behavior is inconsistent with the paper\u2019s self-attention over the token sequence and explains the observed attention map shape '([L, 1, 1])'.","relevant_paper_sections":["Specifically speaking, the Decoder takes features from different levels; uses convolutions to equalize channels; concatenates all flattened features; then passes it through 6 multi-head self-attention modules [58] with linear projections and LayerNorms [1]. The final outputs are split and shaped back into 2D, then sum with lateral-linked input features."],"relevant_code_files":["lib/model_zoo/seecoder.py","configs/model/seecoder.yaml"],"discrepancy_id":"7e9f9955","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2310.02391","paper_url_versioned":"https://arxiv.org/pdf/2310.02391v4.pdf","code_url":"https://github.com/DreamFold/FoldFlow","code_url_versioned":"https://github.com/DreamFold/FoldFlow/tree/20abc40dc241bbed408c5aa35a2a39b7778d6372","code_license":"Other","discrepancy_date":"2024-11-05T18:39:44.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/DreamFold/FoldFlow/issues/8","origin_discrepancy_text":"Inquiry about the vector field calculation of the SO3FM function\n\r\nHey,  \r\nFirst of all, thank you very much for the excellent work by the authors. I am currently reading through the paper and the code, and I have encountered a small issue that has left me a bit puzzled. Specifically, the problem is as follows. Due to the differences in the setup of Flow Matching in FoldFlow (where $t = 0$ is the target distribution and $t = 1$ is the random distribution), I noticed a discrepancy when calculating the SO(3) vector field. In the paper, it is written:\r\n\r\nSpecifically, we calculate the $\\mathfrak{s o}(3)$ element corresponding to the relative rotation between $r_{0}$ and $r_{t}$, given by $r_{t}^{\\top} r_{0}$. We divide by $t$ to get a vector which is an element of $\\mathfrak{s o}(3)$ and corresponds to the skew-symmetric matrix representation of the velocity vector pointing towards the target $r_{1}$. Finally, we parallel-transport the velocity vector to the tangent space $\\mathcal{T}_{r_{t}} \\mathrm{SO}(3)$ using left matrix multiplication by $r_{t}$. These operations can be concisely\r\n\r\n\r\n\r\nHowever, after reviewing the code example, the implementation in the code looks like this:\r\n\r\n```python\r\ndef vectorfield(self, rot_0, rot_t, t):\r\n    ...\r\n    rot_t_minus_0 = rot_0.transpose(-1, -2) @ rot_t\r\n    if self.inference_scaling < 0:\r\n        u_t = rot_t @ (\r\n            log(rot_t_minus_0)\r\n            / torch.clamp(t[:, None, None], min=-self.inference_scaling)\r\n        )\r\n    else:\r\n        u_t = rot_t @ (log(rot_t_minus_0) * self.inference_scaling)\r\n    ...\r\n    return None, u_t\r\n```\r\n\r\nThe issue I don\u2019t fully understand is that the paper describes the computation as $r_t^T r_0$, while the code implements it as $r_0^T r_t$. This seems to differ from what is described in the paper.\r\n\r\nAnother point I\u2019m confused about is that the paper mentions this computation can be simplified to $\\frac{\\log_{r_t} (r_0)}{t}$. Would it be possible to directly compute it as `log_not_from_identity(r_t, r_0})/ t`?\r\n\r\nLastly, considering the difference in the notation of $t$ between the paper (where $t = 0$ is the random distribution and $t = 1$ is the target distribution) and the usual flow matching notation, if we were to change the notation to the usual one, could the interpolation on SO(3) be represented as:\r\n\r\n$$\r\nr_t = \\exp_{r_1} \\left( t \\log_{r_1} (r_0) \\right)?\r\n$$\r\n\r\n\r\nThank you for your time!\r\n","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper states the relative rotation as r_t^T r_0 (leading to u_t \u221d r_t log(r_t^T r_0)/t), while the code computes rot_0^T rot_t (u_t \u221d r_t log(rot_0^T rot_t)/t). These two matrices are inverses, so their logs differ by a sign. The implemented code\u2019s choice yields the correct forward geodesic velocity direction; the paper\u2019s text (r_t^T r_0) would flip the sign. Thus, there is a real paper\u2013code mismatch in the transposition order (and sign).","discrepancy_description":"1) What the paper describes: In Section 3.1, the paper says the instantaneous SO(3) velocity at r_t is obtained by taking the relative rotation r_t^T r_0, applying the matrix logarithm, dividing by t, and then parallel-transporting to T_{r_t}SO(3) by left multiplication with r_t. It summarizes this as log_{r_t}(r_0)/t (i.e., r_t log(r_t^T r_0)/t).\n\n2) What the code implements: In FoldFlow/models/so3_fm.py, the vector field is computed as u_t = rot_t @ (log(rot_0^T rot_t) / t) (or scaled variant). This uses the relative rotation rot_0^T rot_t instead of r_t^T r_0.\n\n3) The difference: r_t^T r_0 = (r_0^T r_t)^{-1}, so log(r_t^T r_0) = -log(r_0^T r_t). Therefore, the paper\u2019s expression implies a velocity pointing in the opposite direction relative to the implemented one. The code\u2019s choice (rot_0^T rot_t) yields u_t consistent with the forward geodesic from r_0 to r_1 (u_t = r_t log(r_0^T r_1)), while the paper\u2019s stated r_t^T r_0 would introduce a sign flip. Hence, the discrepancy exists and is a sign/orientation error in the paper\u2019s description.\n","relevant_paper_sections":["Specifically, we calculate the \\mathfrak{so}(3) element corresponding to the relative rotation between r_{0} and r_{t}, given by r_{t}^{\top} r_{0}. We divide by t to get a vector which is an element of \\mathfrak{so}(3) and corresponds to the skew-symmetric matrix representation of the velocity vector pointing towards the target r_{1}. Finally, we parallel-transport the velocity vector to the tangent space \\mathcal{T}_{r_{t}}\\mathrm{SO}(3) using left matrix multiplication by r_{t}. These operations can be concisely written as \\log_{r_{t}}(r_{0})/t.","Given r_{t} ... These operations can be concisely written as \\log_{r_{t}}(r_{0})/t. The closed form expression of the loss to train the SO(3) component ... is ... \\| v_\theta(t, r_t) - \\log_{r_t}(r_0)/t \\|^2."],"relevant_code_files":["FoldFlow/models/so3_fm.py"],"discrepancy_id":"810540d2","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2403.20127","paper_url_versioned":"https://arxiv.org/pdf/2403.20127v1.pdf","code_url":"https://github.com/kaito25atugich/Detector","code_url_versioned":"https://github.com/kaito25atugich/Detector/tree/e475ec560905632be91e6442b3813f4e3832e7a3","code_license":"","discrepancy_date":"2025-05-21T04:04:53.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/kaito25atugich/Detector/issues/1","origin_discrepancy_text":"How did you use Phi1.5 for Binoculars?\nHi, in your paper, you claim:\n\n> Due to GPU constraints, Binoculars employs the pre-trained and instruct-tuned Phi1.5 [27] instead of Falcon. \n\nHowever, I could not find the code for said implementation. The only invocations of `Binoculars` is like this:\n\nhttps://github.com/kaito25atugich/Detector/blob/e475ec560905632be91e6442b3813f4e3832e7a3/detector.py#L440-L446\n\n\u2026which does not use Phi.\n\nCould you please point out what model combination you used for Phi? This is confusing because Phi1.5 does not have an official finetuned version. Thanks!\n","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper states that Binoculars uses a pre-trained and instruct-tuned Phi-1.5 pair due to GPU constraints, but the released code instantiates Binoculars with OpenLLaMA (and previously Yi-6B) model pairs; there is no code path that uses Phi-1.5 for Binoculars.","discrepancy_description":"The paper states that, for Binoculars, the authors use a pre-trained and instruct-tuned Phi-1.5 pair as a substitute for Falcon because of GPU constraints. In the code, however, the Binoculars implementation constructs the observer/performer models using OpenLLaMA 3B v2 and its instruct-tuned variant (and earlier commented choices of Yi-6B), not Phi-1.5. There is no code that loads or references a Phi-1.5 model for Binoculars. The only references to any Phi family in the repo are for generating estimated prompts (phi-2), which is unrelated to Binoculars. Therefore, the paper\u2019s claim about using Phi-1.5 for Binoculars does not match the provided code.","relevant_paper_sections":["To begin, we utilize the GPT2-XL [23] as the detection model, excluding Binoculars. Due to GPU constraints, Binoculars employs the pre-trained and instruct-tuned Phi1.5 [27] instead of Falcon.","Hans et al. proposed Binoculars, a detection method utilizing two closely related language models, Falcon-7b [26] and Falcon-7b-instruct, by employing a metric called cross-perplexity [8]."],"relevant_code_files":["detector.py","tmp/detector_ep.py","detection.py"],"discrepancy_id":"4b9f947a","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/abs/2301.10540","paper_url_versioned":"https://arxiv.org/pdf/2301.10540v1.pdf","code_url":"https://github.com/david-knigge/ccnn","code_url_versioned":"https://github.com/david-knigge/ccnn/tree/19c12dd7ad8267faadf0dba65d63e368fe42ccba","code_license":"MIT License","discrepancy_date":"2022-10-19T04:38:54.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/david-knigge/ccnn/issues/7","origin_discrepancy_text":"Equivalent responses across input resolutions\nHello, I have 2 questions because I want to implement ccnn in my research paper.\r\n\r\n1. According to the paper, it stated in Equation 1 that the model is resolution agnostic. But I have searched through the code and cannot find the implementation. Would it be possible to point out where that (r1/r2) at? \r\n2. If I set the kernel size at 11, and I input two images of size (56,56) and (28,28). The kernel size is still 11 for both image. Is that correct? Because from my understanding, if the resolution is changed by 2x, the kernel size should have increased by 2x because the relative positions have also been 2x.\r\n `When presenting an input at a different\r\nresolution, e.g., higher resolution, it is sufficient to pass a finer grid of coordinates through the kernel generator network in\r\norder to construct the same kernel at the corresponding resolution.`\r\n\r\nThank you so much.\r\n\r\n","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper claims the model can produce equivalent responses across resolutions by sampling the continuous kernel on a finer grid at test time, but the code fixes the kernel\u2019s coordinate grid length to the training resolution and does not resample it when the input resolution changes. There is no explicit r1/r2 scaling in the code, and unless the model is re-instantiated or buffers are manually changed, the kernel size/coordinate grid does not adapt to new resolutions.","discrepancy_description":"The paper describes CCNNs as resolution-agnostic: since kernels are generated by a continuous function over coordinates, one can deploy the same trained model at different resolutions by \u201cpassing a finer grid of coordinates\u201d to the kernel generator, implying the kernel should be re-sampled proportionally and yield equivalent responses across input resolutions. In the code, kernel coordinates are normalized to [-1,1] but the grid length (number of samples) used to render kernels is computed once from the training input (or from an explicit integer kernel size) and stored in persistent buffers; it is not recomputed per-input at inference if the input resolution changes. Consequently, there is no explicit r1/r2 rescaling implementation, and with a fixed integer kernel size (e.g., 11), the code keeps the kernel size at 11 regardless of input resolution; even with \"same\" size, the grid length is tied to the training length and reused. The difference is that the paper promises resolution-agnostic behavior via re-sampling the kernel at test-time to match new resolutions, whereas the provided code does not implement this dynamic re-sampling and instead uses a grid fixed by the training resolution.","relevant_paper_sections":["CCNNs are defined on a continuous space. Consequently, it is possible to train a CCNN at one resolution and deploy it at other resolutions -a feat not achievable with discrete models (Romero et al., 2022a; Nguyen et al., 2022)-. To showcase this ability, we train CCNNs on Speech-Raw, and test them on a subsampled version of the dataset (Speech-0.5x). CCNNs not only generalize, but outperform existing models on zero-shot prediction over resolution changes.","Normalized relative positions. The kernel network \u03c6Kernel can, in principle, receive arbitrary coordinates as input. However, considering unitary step-wise relative positions, i.e., 0,1,2,\u2026,N, can be problematic from a numerical stability perspective as N may grow very large, e.g., N=16000 for the Speech Commands dataset. Consequently, based on insights from the Implicit Neural Representations, e.g., Sitzmann et al. (2020); Fathony et al. (2021), we normalize the coordinates such that they lie in the space [-1,1]^D for D-dimensional kernels. To this end, we map largest unitary positions seen during training [0, N] to a uniform linear space in [-1,1]. Note that any possible relative kernel positions outside of the trained kernel domain which may be encountered during inference are masked out.","CKConvs provide a continuous parameterization for convolutional kernels by using a small neural network \u03c6Kernel: \u211d^D \u2192 \u211d^{N_out \u00d7 N_in} as a kernel generator network. This network maps coordinates in the domain of the kernel x_i \u2208 \u211d^D to the values of the convolutional kernel at that position: k(x) \u2208 \u211d^{N_out \u00d7 N_in}."],"relevant_code_files":["ckconv/nn/ckconv.py","ckconv/utils/grids.py","ckconv/nn/flexconv.py","ckconv/nn/functional/conv.py","ckconv/nn/functional/causal_conv.py"],"discrepancy_id":"dfccf929","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/pdf/2409.11340","paper_url_versioned":"https://arxiv.org/pdf/2409.11340v1.pdf","code_url":"https://github.com/VectorSpaceLab/OmniGen","code_url_versioned":"https://github.com/VectorSpaceLab/OmniGen/tree/2ef5c32fa9b96993cacea34cd8e24ea04c800c59","code_license":"MIT License","discrepancy_date":"2024-10-31T08:56:19.000Z","origin_type":"GitHub Issue","origin_url":"https://github.com/VectorSpaceLab/OmniGen/issues/62","origin_discrepancy_text":"Image editing loss function\nAs mentioned in the research paper section 2.2, for image editing tasks, the loss function was modified to amplify the difference between the input and target image. \r\n\r\nHowever, it seems like the released code did not include that loss function when the task is `image_edit` or am I misunderstanding the code?\r\n\r\nCode snippet for reference:\r\nhttps://github.com/VectorSpaceLab/OmniGen/blob/2ef5c32fa9b96993cacea34cd8e24ea04c800c59/OmniGen/train_helper/loss.py#L51-L59\r\n\r\nLoss function in question:\r\nFor image editing tasks, the objective is to modify specific regions of the input image while keeping other areas unchanged. Therefore, the difference between the input image and the target image is often small, which allows the model to learn an unexpected shortcut: simply copying the input image as the output to make the related training loss very low. To mitigate this phenomenon, we amplify the loss in the regions of the image where changes occur. More specifically, we calculate the loss weights for each region based on latent representations of input image $\\mathbf{x}^{\\prime}$ and target image $\\mathbf{x}$ :\n\n$$\nw_{i, j}= \\begin{cases}1 & \\text { if } \\mathbf{x}_{i, j}=\\mathbf{x}_{i, j}^{\\prime} \\\\ \\frac{1}{\\left\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\right\\|^{2}} & \\text { if } \\mathbf{x}_{i, j} \\neq \\mathbf{x}_{i, j}^{\\prime}\\end{cases}\n$$\r\n\r\nThanks!","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper specifies a modified, region-weighted loss for image editing, but the released training code implements a uniform MSE loss with no task-specific weighting or use of input\u2013target differences in the loss.","discrepancy_description":"The paper states that for image editing tasks, the training loss is modified to amplify errors in regions where the target image differs from the input image by introducing per-region weights w_{i,j} computed from the latent representations of the input (x') and target (x). In the released code, the training objective is implemented as a standard rectified-flow MSE loss between the model\u2019s predicted velocity and the target velocity, with no weighting based on x vs. x' and no branch that detects an \"image_edit\" task. The training loss function simply computes a mean-squared error over all pixels/latents uniformly, and the data pipeline does not pass any task flag that would change the loss. Therefore, the code does not implement the region-weighted loss described in the paper for image editing.","relevant_paper_sections":["For image editing tasks, the objective is to modify specific regions of the input image while keeping other areas unchanged. Therefore, the difference between the input image and the target image is often small, which allows the model to learn an unexpected shortcut: simply copying the input image as the output to make the related training loss very low. To mitigate this phenomenon, we amplify the loss in the regions of the image where changes occur. More specifically, we calculate the loss weights for each region based on these latent representations of input image x' and target image x:","$$w_{i,j} = \begin{cases} 1 & \text{if } x_{i,j} = x'_{i,j} \\\\ \frac{1}{| | x - x' | |^2} & \text{if } x_{i,j} \neq x'_{i,j} \\end{cases} \tag{2}$$","Specifically, the objective is to minimize the mean squared error loss: $$\\mathcal{L} = \\mathbb{E}[| | (x - \\epsilon) - v_{\theta}(x_t, t, c) ||^2]. \tag{1}$$"],"relevant_code_files":["OmniGen/train_helper/loss.py","OmniGen/train_helper/data.py","train.py"],"discrepancy_id":"073fb272","removed_in_postproc":false}
{"paper_url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/cf836efd32fd53493e02d26670f04d46-Paper-Conference.pdf","paper_url_versioned":"https://proceedings.neurips.cc/paper_files/paper/2024/file/cf836efd32fd53493e02d26670f04d46-Paper-Conference.pdf","code_url":"https://github.com/vschiniah/ArXiv_Recommendation_Research","code_url_versioned":"https://github.com/vschiniah/ArXiv_Recommendation_Research/tree/4b22f57e57a2923fdd5f4ddbabb44480e2d8915c","code_license":null,"discrepancy_date":"2025-06-16T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://openreview.net/pdf?id=vltzxxhzLU","origin_discrepancy_text":"The repository contains a data-writing bug for logistic regression features: when storing similarity scores per author\u2013item pair, it restarts writing from the first item index for each author, effectively overwriting scores from previous authors. This contradicts the evaluation intent described in the paper, where each author\u2019s similarity scores should be preserved for the logistic validation. The replicators corrected this by writing scores starting at the correct offset for each author.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The TF\u2011IDF pipeline assigns the similarity_score column incorrectly by overwriting all previously written rows in each author iteration, so earlier authors\u2019 scores are replaced by the current author\u2019s scores. This contradicts the paper\u2019s described evaluation, which requires preserving each author\u2013item similarity for logistic regression.","discrepancy_description":"The paper validates recommendations by running a logistic regression where, for each user\u2013paper pair, the predictor is that pair\u2019s similarity score and the outcome is whether the user later cites the paper. In the code\u2019s TF-IDF implementation, per-author similarity scores are computed correctly, but when writing them into the combined recommendations table, the code reassigns the similarity_score column across all existing rows on each loop iteration over authors using only the current author\u2019s score map. Because the test set papers are shared across authors, this overwrites previously stored scores and leaves all rows with the last processed author\u2019s score for the given paper, losing the per-author values needed for logistic validation. The intended behavior is to preserve each author\u2019s similarity with each paper so the regression uses the true author\u2013item features; the code instead restarts writing from the top of the table for each author, effectively overwriting earlier authors\u2019 scores.","relevant_paper_sections":["We then examine how well the user-item similarity score generated by our recommendation engine predicts the presence of a citation. The recommendations effectively predict whether a user cites a paper with a high score after 2020. In Table 1 we show the results of a logistic regression between each similarity score and the presence of a citation, where the coefficient on the score is large and statistically significant for each model;","We evaluate each of the four approaches (TF-IDF and sentence transformers, each with the max or the mean scores) using citation data for 1,128 users (authors) and 14,307 papers. For each user-paper pair, we use the following as outcome data: User cites paper in the future... For each recommendation approach, we calculate the relationship between this citation data and the following similarity score measures. - Similarity score: The raw cosine similarity score.","Table 3: Logistic regression results for predicting whether user i cites paper j from the similarity score w_ij, for the Max score, TF-IDF model"],"relevant_code_files":["tfidf_authors.py","post_processing_evaluation.py","sentence_transformer_authors.py"],"discrepancy_id":"b1b5cc6e","removed_in_postproc":false}
{"paper_url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/984dd3db213db2d1454a163b65b84d08-Paper-Datasets_and_Benchmarks_Track.pdf","paper_url_versioned":"https://proceedings.neurips.cc/paper_files/paper/2024/file/984dd3db213db2d1454a163b65b84d08-Paper-Datasets_and_Benchmarks_Track.pdf","code_url":"https://github.com/S-Abdelnabi/LLM-Deliberation","code_url_versioned":"https://github.com/S-Abdelnabi/LLM-Deliberation/tree/78e11e04ba89b9d888cb006f57bad696584a32f1","code_license":"MIT License","discrepancy_date":"2025-06-20T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://openreview.net/pdf?id=MTrhFmkC45","origin_discrepancy_text":"Acceptance criterion mismatch between paper and code. The paper defines a deal as acceptable if an agent's score meets or exceeds its minimum threshold, but the official repository implemented this inconsistently\u2014requiring strictly greater-than in one part of the evaluation notebook and greater-or-equal elsewhere. This discrepancy changes whether boundary cases (scores exactly at the threshold) are counted as accepted and led to changes in some model evaluations after fixing.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The repository\u2019s evaluation code applies different acceptance criteria in different places (strictly greater-than vs greater-or-equal) while the paper\u2019s instructions to agents define acceptance as meeting at least the minimum threshold. This leads to boundary deals (exactly at the threshold) being treated inconsistently across the code and relative to the paper\u2019s stated rule.","discrepancy_description":"The paper repeatedly instructs agents that a deal is acceptable if their score is not less than their minimum threshold (i.e., equality is acceptable), and describes feasible deals as those that satisfy parties\u2019 thresholds. In the official code, the evaluation script evaluate_deals.py counts an agent as \u201cagreeing\u201d with a deal only if their score is strictly greater than the threshold, whereas other parts treat equality as acceptable (e.g., the \u201cwrong deals\u201d check treats scores equal to the threshold as valid, and adjust_games.py uses \u2265 to enumerate feasible deals). Consequently, the same boundary case (score exactly at the threshold) is treated as accepted in some computations and not accepted in others. This inconsistency affects whether certain deals are counted as passing and changes some evaluation outcomes once corrected.","relevant_paper_sections":["Scoring rules: - You cannot accept any deal with a score less than 55. This is the minimum score you can accept.","Feasible solutions. Each party p_i has a minimum threshold \u03c4_{p_i} for acceptance. A deal is feasible if it exceeds the thresholds of at least n-1 parties, which must include p_1 and p_2."],"relevant_code_files":["evaluation/evaluate_deals.py","evaluation/adjust_games.py","initial_prompts.py"],"discrepancy_id":"f234a0ee","removed_in_postproc":false}
{"paper_url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/984dd3db213db2d1454a163b65b84d08-Paper-Datasets_and_Benchmarks_Track.pdf","paper_url_versioned":"https://proceedings.neurips.cc/paper_files/paper/2024/file/984dd3db213db2d1454a163b65b84d08-Paper-Datasets_and_Benchmarks_Track.pdf","code_url":"https://github.com/S-Abdelnabi/LLM-Deliberation","code_url_versioned":"https://github.com/S-Abdelnabi/LLM-Deliberation/tree/78e11e04ba89b9d888cb006f57bad696584a32f1","code_license":"MIT License","discrepancy_date":"2025-06-17T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://openreview.net/pdf?id=BVH81SAAh2","origin_discrepancy_text":"The evaluation script in the original repository unconditionally relaxes player p1\u2019s threshold by 10 points, effectively making agreements easier. The paper only mentions a 10-point bonus for unanimity in prompts, and a variation noted in Appendix F applied to greedy games, not as a general evaluation rule. This inconsistency can inflate performance metrics by enlarging the feasible set; the reproducing authors therefore did not apply this relaxation.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The evaluation script implements a +10 adjustment to p1\u2019s acceptance in success checking, allowing deals to be counted as successful (and sometimes as all-agreement) when p1\u2019s raw score is below its threshold but within 10 points, provided all other parties meet their thresholds. The paper\u2019s main definition of feasibility requires deals to exceed thresholds (including p1 and p2) and does not state this relaxation as part of the general evaluation rule; the only mention of +10 appears as a bonus for unanimity in the prompts, not as an acceptance-threshold relaxation. Thus, the code\u2019s success criterion diverges from the paper\u2019s stated rules.","discrepancy_description":"The paper defines feasibility as deals that exceed the thresholds of at least n\u22121 parties, which must include p1 and p2, and when reporting 6-way (all-party) agreement, it implies all parties exceed their thresholds. The only mention of a \u201c+10\u201d appears in the prompts as a bonus for unanimity (when everyone agrees), not as a general rule for acceptance. In contrast, the evaluation script (evaluate_deals.py) applies a +10 adjustment to p1\u2019s acceptance checks: when all the other parties meet their thresholds (agreed = n\u22121) but p1\u2019s score is below its minimum, the script treats the deal as successful (and, in one branch, even as \u201call agreement\u201d) if p1\u2019s score + 10 meets the threshold. It also avoids counting such p1 proposals as \u201cwrong deals.\u201d This relaxes p1\u2019s threshold in evaluation beyond what the paper\u2019s feasibility definition describes and makes agreements easier across evaluations, not only in a specific greedy variant.","relevant_paper_sections":["Feasible solutions. Each party p_i has a minimum threshold \u03c4_{p_i} for acceptance. A deal is feasible if it exceeds the thresholds of at least n\u22121 parties, which must include p_1 and p_2.","End of negotiation. ... The achieved utility of each party becomes: U_{p_i} = S_{p_i}(\u03c0_{p_1}^{(R+1)}) if \u03c0_{p_1}^{(R+1)} \u2208 \u03a0_pass; BATNA otherwise.","Initial prompts (Appendix excerpts shown in the paper): ... To protect yourself from potential future lawsuits, you want to achieve unanimity; if all other 5 parties agree, you will get a bonus of 10 points."],"relevant_code_files":["evaluation/evaluate_deals.py","initial_prompts.py"],"discrepancy_id":"ef57fd41","removed_in_postproc":false}
{"paper_url":"https://openreview.net/pdf?id=rqq6Dh8t4d","paper_url_versioned":"https://openreview.net/pdf?id=rqq6Dh8t4d","code_url":"https://github.com/yolandalalala/GNNInterpreter","code_url_versioned":"https://github.com/yolandalalala/GNNInterpreter/tree/4afe38cf1b4f6cb808d30ff39a133255f88cf37d","code_license":"MIT License","discrepancy_date":"2024-06-06T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://openreview.net/pdf?id=8cYcR23WUo","origin_discrepancy_text":"The official code dynamically adjusts the weight of the budget penalty during training based on the current class probability and graph size, which is not described in the paper. Specifically, the repository reduces the budget penalty weight until the generated graph achieves a 0.9 target class probability, then increases it to shrink the graph. Different commits implement this schedule differently (multiplicative vs. additive updates). The paper presents a fixed budget penalty regularizer but does not mention any dynamic scheduling, making this an algorithmic divergence that can materially affect convergence and explanation size.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The official code implements a dynamic schedule that increases or decreases the budget penalty weight during training based on the current target class probability (and a target size), whereas the paper only defines the budget penalty and, at most, mentions a simple initial warm-up of its weight. The paper does not describe the conditional, on-the-fly adjustment policy driven by target probability or size that is implemented in the code.","discrepancy_description":"The paper defines a budget penalty regularizer to constrain the size of the generated explanation graph and presents it as part of a fixed set of regularization terms; in the appendix, it mentions only a 500-iteration warm-up that gradually raises the budget penalty weight from 0, and otherwise treats regularization weights as constant. In contrast, the official code trains with a dynamic budget penalty weight that is adjusted during optimization according to the current class probability and a target graph size: training scripts pass parameters such as target_probs, target_size, and w_budget_inc/w_budget_dec, indicating the weight is decreased until the generated graph reaches a target probability threshold (e.g., 0.9) and then increased to shrink the graph; the most recent code uses multiplicative updates (e.g., \u00d71.1, \u00d70.95). Thus, the repository uses a conditional scheduling policy for the budget penalty that is not described in the paper beyond a brief warm-up, constituting an algorithmic divergence.","relevant_paper_sections":["Budget Penalty. Budget penalty is employed in instance-level GNN explanation methods (i.e., PGExplainer (Luo et al., 2020) and GNNExplainer (Ying et al., 2019)) to generate compact and meaningful explanations. For the model-level explanation, the budget penalty can prevent the size of explanation graphs from growing unboundedly with repeated discriminative patterns. The budget penalty is defined as\n\nR_b = softplus(||sigmoid(\u03a9)||_1 \u2212 B)^2\n\nwhere B is the expected maximum number of edges in the explanation graph.","Note that we adopt a constant weight strategy for L1 regularization R_{L1}, L2 regularization R_{L2}, and connectivity constraint R_c throughout the training process. In contrast, for budget penalty R_b, we employ a 500-iteration initial warm-up period to gradually increase the weight starting from 0."],"relevant_code_files":["model_explanation_mutag.py","model_explanation_motif.py","model_explanation_shape.py","model_explanation_cyclicity.py"],"discrepancy_id":"4ddcad80","removed_in_postproc":false}
{"paper_url":"https://openreview.net/pdf?id=rqq6Dh8t4d","paper_url_versioned":"https://openreview.net/pdf?id=rqq6Dh8t4d","code_url":"https://github.com/yolandalalala/GNNInterpreter","code_url_versioned":"https://github.com/yolandalalala/GNNInterpreter/tree/4afe38cf1b4f6cb808d30ff39a133255f88cf37d","code_license":"MIT License","discrepancy_date":"2024-06-06T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://openreview.net/pdf?id=8cYcR23WUo","origin_discrepancy_text":"The authors observed additional features in the code that are absent from the paper\u2019s method description, including thresholding of qualitative results and adding a mean penalty to the weighting criterion. These undocumented components suggest further deviations between the implementation and the written methodology.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The released code introduces components that are not described in the paper\u2019s method section: (1) a threshold parameter used to discretize/visualize the learned probabilistic graphs during qualitative evaluation, and (2) an additional MeanPenalty term applied to the logits within the optimization criterion. The paper\u2019s objective and regularization only mention class-score maximization with an embedding-similarity term and three regularizations (L1/L2, budget penalty, connectivity incentive) and do not document thresholding or any \u201cmean penalty.\u201d","discrepancy_description":"The paper defines the learning objective as maximizing the target-class score plus a cosine-similarity term to the class-mean embedding, with three regularizers: L1/L2 penalties on latent parameters, a budget penalty on graph size, and a connectivity incentive via KL divergence. It further describes returning sampled discrete graphs via Bernoulli/Categorical distributions. In the provided code, the training criterion adds a MeanPenalty term applied to the logits, which is not discussed in the paper, and the qualitative evaluation uses a configurable threshold (e.g., 0.5) to binarize probabilities when rendering or selecting explanations, also not described in the paper. Thus, the implementation contains undocumented components\u2014thresholding in qualitative evaluation and a mean penalty in the optimization\u2014that deviate from the written methodology.","relevant_paper_sections":["To reveal the high-level decision-making process of the model, one effective approach is to construct an explanation graph that can trigger a specific response from the model as much as possible... we mathematically formulate our learning objective as follows,\n\nmax_G L(G)=max_{A,Z,X} phi_c(A,Z,X) + \u03bc sim_cos(\u03c8(A,Z,X), \u03c8\u0302_c)","In order to facilitate optimization and ensure that the explanation graphs are constrained to desired properties, we employ three types of regularization on the latent parameters. Firstly, we apply L1 and L2 regularization on all latent parameters... Secondly, ... a budget penalty to limit the size of the explanation graph. Lastly, the connectivity incentive term is introduced to encourage neighboring edge correlation. The equations and detailed explanations for all regularization terms are presented in Appendix A.","Algorithm 1 ... return G=(A, Z, X), where A ~ Bernoulli(\u0398), Z ~ Categorical(Q), X ~ Categorical(P)."],"relevant_code_files":["model_explanation_cyclicity.py","model_explanation_motif.py","model_explanation_mutag.py","model_explanation_shape.py"],"discrepancy_id":"8fbac23a","removed_in_postproc":false}
{"paper_url":"https://proceedings.neurips.cc/paper_files/paper/2022/file/b1e7f61f40d68b2177857bfcb195a507-Paper-Conference.pdf","paper_url_versioned":"https://proceedings.neurips.cc/paper_files/paper/2022/file/b1e7f61f40d68b2177857bfcb195a507-Paper-Conference.pdf","code_url":"https://github.com/hsouri/BayesianTransferLearning","code_url_versioned":"https://github.com/hsouri/BayesianTransferLearnin/tree/2bb409a25ab5154ed1fa958752c54842e34e9087","code_license":null,"discrepancy_date":"2024-05-23T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://openreview.net/pdf?id=BbvSU02jLg","origin_discrepancy_text":"The original code scales the low-rank component of the prior covariance incorrectly relative to what the paper specifies. The paper describes scaling the covariance uniformly by \u03bb (i.e., using N(w | \u03bc, \u03bb\u03a3)), but the code multiplies the low-rank factor Q by \u03bb before forming QQ\u1d40. This yields a covariance with the diagonal scaled by \u03bb and the low-rank part effectively scaled by \u03bb\u00b2, creating inconsistent influence between components. The repro authors corrected this by scaling Q by sqrt(\u03bb) to achieve consistent \u03bb scaling across both diagonal and low-rank parts.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper specifies rescaling the entire prior covariance by a single scalar \u03bb (i.e., \u03a3 -> \u03bb\u03a3), whereas the released code multiplies the low-rank factor Q by \u03bb before forming QQ\u1d40 while multiplying the diagonal variance directly by \u03bb. Because LowRankMultivariateNormal uses \u03a3 = FF\u1d40 + diag(\u00b7), scaling F by \u03bb scales the low-rank covariance by \u03bb\u00b2, leading to inconsistent scaling between the diagonal and low-rank components.","discrepancy_description":"The paper states that the learned Gaussian prior for downstream tasks should be rescaled by multiplying its covariance matrix by a scalar \u03bb, i.e., using N(w | \u03bc, \u03bb\u03a3). In the code, when loading the prior, the diagonal variance vector is scaled by prior_scale (\u03bb), but the low-rank factor (covariance \u201csquare root\u201d matrix) Q is also multiplied by prior_scale before being passed to LowRankMultivariateNormal. Since LowRankMultivariateNormal forms the covariance as FF\u1d40 + diag(variance), scaling F by \u03bb produces a low-rank covariance term scaled by \u03bb\u00b2, while the diagonal term is scaled by \u03bb. Thus, the code applies inconsistent scaling (\u03bb to the diagonal, \u03bb\u00b2 to the low-rank) instead of uniformly scaling the whole covariance by \u03bb. The intended fix would be to scale the low-rank factor by sqrt(\u03bb) so that the low-rank covariance scales by \u03bb, matching the paper\u2019s description.","relevant_paper_sections":["To address this consideration, we rescale the learned Gaussian prior by multiplying its covariance matrix by a scalar value. We select the highest performing scalar value across a grid on a holdout set from the downstream task.","We fit the SimCLR pre-training loss using a Gaussian prior with mean \u03bc and covariance matrix \u03a3... To rectify this problem, we instead assign prior covariance matrix \u03bb \u03a3 with \u03bb \u2265 1.","The SWAG approximate posterior distribution is given by N( w\u0304, 1/2 \u03a3_diag + 1/2 \u03a3_low-rank ), where ... \u03a3_low-rank = 1/(L-1) \u2211 (w_t - w\u0304)(w_t - w\u0304)\u1d40."],"relevant_code_files":["priorBox/sghmc/utils.py","priorBox/sghmc/losses.py"],"discrepancy_id":"106ad0b9","removed_in_postproc":false}
{"paper_url":"https://proceedings.neurips.cc/paper_files/paper/2022/file/b1e7f61f40d68b2177857bfcb195a507-Paper-Conference.pdf","paper_url_versioned":"https://proceedings.neurips.cc/paper_files/paper/2022/file/b1e7f61f40d68b2177857bfcb195a507-Paper-Conference.pdf","code_url":"https://github.com/hsouri/BayesianTransferLearning","code_url_versioned":"https://github.com/hsouri/BayesianTransferLearnin/tree/2bb409a25ab5154ed1fa958752c54842e34e9087","code_license":null,"discrepancy_date":"2024-05-23T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://openreview.net/pdf?id=BbvSU02jLg","origin_discrepancy_text":"The implementation applies weight decay to all parameters, including the backbone weights w, in addition to the informative prior, effectively imposing a redundant 'double prior' on w. The paper\u2019s description only specifies adding a zero-mean isotropic Gaussian prior on the added parameters (e.g., the classification head), not on the backbone (which already has the informed prior). The repro authors removed this extra weight decay on w to align the objective with the paper\u2019s description.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The code applies weight decay to all model parameters (backbone and head) via the optimizer, while the paper specifies using the learned prior on the backbone and adding only a zero-mean isotropic Gaussian prior to newly added parameters (e.g., the classification head). This creates a double prior on the backbone in the implementation, which is not described in the paper.","discrepancy_description":"The paper states that for downstream tasks, the learned (informative) prior should be applied to the backbone parameters, and a zero-mean isotropic Gaussian prior should be added only to the newly added parameters, such as the classification head. In the code, the informative prior is indeed applied to the backbone parameters via a Gaussian log-density term in the loss, but the optimizer is also configured with weight decay applied to all parameters indiscriminately (both backbone and head). This weight decay corresponds to an additional zero-mean isotropic Gaussian prior. As a result, the backbone receives two priors in the implementation: the learned prior (via the loss) and an extra isotropic prior (via weight decay), whereas the paper only describes applying the isotropic prior to added parameters, not to the backbone. The reproducing authors therefore removed this extra weight decay on the backbone to align with the paper\u2019s description.","relevant_paper_sections":["Finally, we plug the re-scaled prior into a Bayesian inference algorithm, along with a zero-mean isotropic Gaussian prior over added parameters (e.g. classification head) to form a posterior on the downstream task. We then either optimize the posterior, or use it to perform full Bayesian inference with SGLD and SGHMC samplers [50, 5] (Section 3.3).","After learning a prior and re-scaling it, we finally must draw samples from the downstream task's posterior over the parameters of the entire model, including additional modules, such as classification heads, which were added after pre-training specifically for a particular downstream task. We use a zero-mean isotropic Gaussian prior over these additional parameters, with a scaling that is again tuned on held-out training data from the downstream task.","For both torchvision and SimCLR models, we learn the prior using the associated loss function - cross-entropy and InfoNCE, respectively, regularized by weight decay. In both settings, the regularized loss function can be represented as the sum of a negative log-likelihood and the negative log Gaussian density (weight decay)."],"relevant_code_files":["priorBox/sghmc/sghmc_model.py","priorBox/sghmc/sghmc.py","priorBox/sghmc/sgld.py","priorBox/sghmc/losses.py","priorBox/bayesian_learning/args.py","priorBox/bayesian_learning/run_tune.py"],"discrepancy_id":"f2b47a1a","removed_in_postproc":false}
{"paper_url":"https://proceedings.neurips.cc/paper_files/paper/2022/file/b1e7f61f40d68b2177857bfcb195a507-Paper-Conference.pdf","paper_url_versioned":"https://proceedings.neurips.cc/paper_files/paper/2022/file/b1e7f61f40d68b2177857bfcb195a507-Paper-Conference.pdf","code_url":"https://github.com/hsouri/BayesianTransferLearning","code_url_versioned":"https://github.com/hsouri/BayesianTransferLearnin/tree/2bb409a25ab5154ed1fa958752c54842e34e9087","code_license":null,"discrepancy_date":"2024-05-23T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://openreview.net/pdf?id=BbvSU02jLg","origin_discrepancy_text":"Data augmentation procedures differ between the paper and the released code. While the paper states using random cropping and random horizontal flips, the code also employs random vertical flips for some datasets. This adds an augmentation not described in the paper\u2019s methodology.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The released code applies RandomVerticalFlip for some downstream datasets (e.g., CIFAR and Oxford Flowers), whereas the paper only describes standard augmentations (random cropping and random horizontal flips). This constitutes an extra augmentation not stated in the paper.","discrepancy_description":"The paper describes using standard image augmentations for downstream training, namely random cropping and random horizontal flips. In the released code, the data-loading pipeline for downstream classification adds RandomVerticalFlip for certain datasets. Specifically, the CIFAR pipeline includes both RandomVerticalFlip and RandomHorizontalFlip, and the Oxford Flowers pipeline includes RandomVerticalFlip (and additional rotations). Thus, the code implements a stronger augmentation set than what is described in the paper\u2019s methodology, introducing vertical flips that are not mentioned in the paper.","relevant_paper_sections":["We adopt the ResNet-50 and ResNet-101 architectures [18], and we scale the input image to 224 \u00d7 224 pixels to accommodate these feature extractors designed for ImageNet data. ... We provide a detailed description of hyperparameters in Appendix C.1."],"relevant_code_files":["priorBox/solo_learn/utils/classification_dataloader.py"],"discrepancy_id":"34ddeb91","removed_in_postproc":false}
{"paper_url":"https://proceedings.neurips.cc/paper_files/paper/2022/file/b1e7f61f40d68b2177857bfcb195a507-Paper-Conference.pdf","paper_url_versioned":"https://proceedings.neurips.cc/paper_files/paper/2022/file/b1e7f61f40d68b2177857bfcb195a507-Paper-Conference.pdf","code_url":"https://github.com/hsouri/BayesianTransferLearning","code_url_versioned":"https://github.com/hsouri/BayesianTransferLearnin/tree/2bb409a25ab5154ed1fa958752c54842e34e9087","code_license":null,"discrepancy_date":"2024-05-23T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://openreview.net/pdf?id=BbvSU02jLg","origin_discrepancy_text":"The code adds an extra \u03b5 term to the prior variance (default \u03b5 = 0.1), modifying the covariance from \u03bb\u03a3 to \u03bb\u03a3 + \u03b5I, whereas the paper does not mention this addition. This change alters the effective prior specified in the paper\u2019s formulation of N(w | \u03bc, \u03bb\u03a3). The repro authors explicitly include this \u03b5 in their reimplementation to match the code behavior.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper specifies re-scaling the learned covariance by a scalar \u03bb (i.e., using N(w | \u03bc, \u03bb\u03a3)) and does not mention any additional epsilon term, whereas the code explicitly adds a prior_eps (default 0.1) to the diagonal variance, effectively using \u03bb\u03a3 + \u03b5I.","discrepancy_description":"The paper describes forming the downstream prior by re-scaling the learned pre-training posterior covariance by a single scalar \u03bb, i.e., using a Gaussian N(w | \u03bc, \u03bb\u03a3). There is no mention of adding any additional regularization term to the covariance. In the released code, when constructing the prior for downstream inference, the diagonal variance component is computed as variance = \u03bb\u00b7variance + prior_eps with prior_eps defaulting to 0.1, and the low-rank component is optionally scaled by \u03bb. This implementation therefore modifies the covariance from \u03bb\u03a3 to \u03bb\u03a3 + \u03b5I (by adding \u03b5 to the diagonal), a detail not described in the paper. Consequently, the code implements a prior with an added isotropic jitter term that is absent from the paper's formulation.","relevant_paper_sections":["SWAG starts from a pre-trained model, and runs a small number M of fine-tuning epochs with a modified learning rate schedule [34]. The SWAG approximate posterior distribution is given by N( w\u0304, 1/2 \u03a3_diag + 1/2 \u03a3_low-rank ), where w\u0304=1/M \u2211_{t=1}^M w_t is the SWA solution, \u03a3_diag=1/(L\u22121) \u2211_{t=M\u2212L+1}^M diag(w_t\u2212w\u0304), and \u03a3_low-rank=1/(L\u22121) \u2211_{t=M\u2212L+1}^M (w_t\u2212w\u0304)(w_t\u2212w\u0304)^\u22a4, where L is a hyperparameter controlling the rank of the low-rank component of the covariance matrix.","We now put this intuition to the test. We fit the SimCLR pre-training loss using a Gaussian prior with mean \u03bc and covariance matrix \u03a3. ... To rectify this problem, we instead assign prior covariance matrix \u03bb\u03a3 with \u03bb \u2265 1."],"relevant_code_files":["priorBox/sghmc/utils.py","priorBox/bayesian_learning/args.py"],"discrepancy_id":"a950db48","removed_in_postproc":false}
{"paper_url":"https://openreview.net/pdf?id=nA5AZ8CEyow","paper_url_versioned":"https://openreview.net/pdf?id=nA5AZ8CEyow","code_url":"https://github.com/mertyg/post-hoc-cbm","code_url_versioned":"https://github.com/mertyg/post-hoc-cbm/tree/115152a7523754754e9eed2408fe84bf49484e02","code_license":"MIT License","discrepancy_date":"2024-05-08T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://openreview.net/pdf?id=8UfhCZjOV7","origin_discrepancy_text":"Backbone architecture mismatch for the model-editing experiments. The paper specifies that both the user study and controlled model-editing experiments use a CLIP-ResNet50 backbone, yet the reproduction team reports encountering conflicting information indicating the use of ResNet18. This suggests an inconsistency between the paper\u2019s stated backbone and what is implied by the available code/documentation, prompting the authors to try multiple backbones.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper gives conflicting backbone descriptions for the controlled model-editing experiments (main text says ResNet50, appendix says ResNet18), and the released code does not implement an ImageNet ResNet50 backbone for these experiments, instead supporting CLIP RN50 or a ResNet18 model trained on CUB. This mismatch corroborates the report's claim of ambiguity between the paper and the code regarding the backbone used in model-editing experiments.","discrepancy_description":"The paper\u2019s model-editing sections describe different backbones: the controlled editing experiment (Section 4.1) states using an ImageNet-pretrained ResNet50 variant, while the appendix explicitly states using ResNet18 pretrained on ImageNet for those controlled MetaShift experiments. The user study (Section 4.2) clearly uses CLIP\u2019s ResNet50. In the released code, the backbone options are limited to CLIP (e.g., \"clip:RN50\"), a ResNet18 model tailored for CUB (\"resnet18_cub\"), and an Inception model for HAM10000; there is no implementation for an ImageNet-pretrained ResNet50, nor MetaShift-specific code. Thus, there is a clear inconsistency between the paper\u2019s backbone description for the controlled editing experiments and what is available or implied by the code (which does not support the stated ResNet50 and points toward ResNet18 or CLIP), forcing ambiguity around which backbone should be used to reproduce those experiments.","relevant_paper_sections":["4.1 Controlled EXPERIMENT: EDITING PCBM WHEN THE SPURIOUS CONCEPT IS KNOWN\n... We use an ImageNet pretrained ResNet50 variant as a backbone and the visual concept bank described in the Experiments section.","4.2 USER STUDY: EDITING PCBM WITH HUMAN GUIDANCE\n... We use CLIP's ResNet50 variant as our backbone, and leverage ConceptNet to construct the concept subspace.","D Controlled Metashift Experiments for Model Editing\n... For each of these, we use a ResNet18 pretrained on ImageNet as the backbone of the P-CBM, and then use 100 images per class to train the concept bottleneck."],"relevant_code_files":["models/model_zoo.py","train_pcbm.py","train_pcbm_h.py","README.md","learn_concepts_dataset.py"],"discrepancy_id":"d586de2b","removed_in_postproc":false}
{"paper_url":"https://proceedings.neurips.cc/paper_files/paper/2022/file/df334022279996b07e0870a629c18857-Paper-Conference.pdf","paper_url_versioned":"https://proceedings.neurips.cc/paper_files/paper/2022/file/df334022279996b07e0870a629c18857-Paper-Conference.pdf","code_url":"https://github.com/uvanlp/valda","code_url_versioned":"https://github.com/uvanlp/valda/tree/4244c7e20c1cafb34adc6a14eca54733423219a6","code_license":"MIT License","discrepancy_date":"2024-07-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://openreview.net/pdf?id=srFEYJkqD7","origin_discrepancy_text":"The official CS\u2011Shapley implementation introduces an undocumented constraint on the conditioning contexts: it does not condition on sets S with size |S| &lt; c\u22121 (where c is the number of classes), meaning that for binary tasks it never conditions on the empty set. The paper\u2019s formal definition averages over all possible out\u2011of\u2011class environments S_{\u2212y_i} and does not mention excluding small contexts. The repro authors note that PyDVL\u2019s default implementation does include conditioning on the empty set (by setting u(\u2205|S)=u(S)). This indicates a paper\u2013code divergence in which contexts are considered during value computation.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper defines CS\u2011Shapley as averaging over all out\u2011of\u2011class environments S_{\u2212y_i} (i.e., all subsets of T_{\u2212y_i}), but the official code samples S_{\u2212y_i} with a hard minimum size: at least one example from each non\u2011target class. For binary tasks this excludes the empty set entirely. This constraint is not documented in the paper, so the reported paper\u2013code divergence is valid.","discrepancy_description":"The paper\u2019s canonical CS\u2011Shapley definition averages the conditional Shapley value over all possible out\u2011of\u2011class environments S_{\u2212y_i} \u2286 T_{\u2212y_i} with equal weight, implying inclusion of the empty set. In the official code, the subset S_{\u2212y_i} is always sampled to include at least one instance per non\u2011target class: for binary classification s is chosen from 1 to N (never 0), and for multi\u2011class a positive number is drawn from each other class, guaranteeing |S_{\u2212y_i}| \u2265 c\u22121. Consequently, the code never conditions on the empty environment and more generally excludes small contexts, whereas the paper does not state such a restriction and describes averaging over all subsets. This constitutes a clear divergence in which conditioning contexts are considered during value computation.","relevant_paper_sections":["To compute the marginal CS-SHAPLEY value of instance i, we then simply average over all possible environmental data S_{-y_{i}} \u2286 T_{-y_{i}} with equal weight, which leads to our following definition of the Canonical CS-SHAPLEY","\u03d5_i = 1 / 2^{|T_{-y_i}|} \u2211_{S_{-y_i}} [\u03d5_i | S_{-y_i}]","We remark that the word \u201ccanonical\u201d here refers to our simple choice of equal weight 1/2^{|T_{-y_i}|} for each sampled out-of-class environment S_{-y_i} \u2286 T_{-y_i}."],"relevant_code_files":["src/valda/cs_shapley.py"],"discrepancy_id":"ecdd2970","removed_in_postproc":false}
{"paper_url":"https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_ITI-GEN_Inclusive_Text-to-Image_Generation_ICCV_2023_paper.pdf","paper_url_versioned":"https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_ITI-GEN_Inclusive_Text-to-Image_Generation_ICCV_2023_paper.pdf","code_url":"https://github.com/humansensinglab/ITI-GEN","code_url_versioned":"https://github.com/humansensinglab/ITI-GEN/tree/e80df449d3d94c8dc32385eea4087694ad0bf03d","code_license":"Other","discrepancy_date":"2024-06-25T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://openreview.net/pdf?id=d3Vj360Wi2","origin_discrepancy_text":"The original paper presents a directional alignment loss and a semantic consistency loss, but the released code implements an additional fallback: when the directional alignment loss is undefined (e.g., batch lacks all categories), it replaces it with a cosine similarity loss between image and text embeddings. This fallback changes the optimization objective and training dynamics and is not described in the paper, creating a paper\u2013code mismatch.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper defines and optimizes only the directional alignment loss plus a semantic consistency loss, whereas the released code adds an unmentioned fallback: if the directional loss becomes undefined (NaN), it switches to a cosine similarity loss between images and text embeddings. This fallback is implemented in the training loop and not described in the paper, creating a paper\u2013code mismatch.","discrepancy_description":"The paper describes two losses for training: a directional alignment loss that aligns differences between prompt embeddings and image embeddings across categories, and a semantic consistency loss that keeps the learned prompts close to the original prompt. The paper\u2019s optimization objective is explicitly the sum of these two losses across pairs of categories and attributes. In the released code, however, the training implements an additional fallback mechanism: when the directional loss becomes undefined (NaN), typically when a batch lacks samples from some categories, the code replaces it with a cosine similarity loss that encourages each image to be similar to all inclusive prompts that match its attribute category (across other attribute combinations). This fallback changes the training objective used in practice and is not described anywhere in the paper.","relevant_paper_sections":["Direction Alignment Loss. ... we define the direction alignment loss L_dir^m to maximize the cosine similarity between the image direction and the prompt direction as follows: L_dir^m(S_i^m, S_j^m) = 1 - <\u0394_I^m(i, j), \u0394_P^m(i, j)> ... By inducing the direction alignment, we aim to facilitate the prompt learning of more meaningful and nuanced differences between images from different categories.","Semantic Consistency Loss. ... we design a semantic consistency objective to regularize the training by maximizing the cosine similarity between the learning prompts and the original input prompt: L_sem^m(S_i^m, S_j^m) = max(0, \u03bb - <E_text(P), E_text(T)>).","Optimization. Building upon L_dir^m and L_sem^m, our total training loss for learning the inclusive tokens of a pair of categories in attribute A_m is written as follows: L_pair^m(S_i^m, S_j^m) = L_dir^m(S_i^m, S_j^m) + L_sem^m(S_i^m, S_j^m). ... The final objective during the whole learning process is: L_total = \u03a3_m \u03a3_{1 \u2264 i \u2264 j \u2264 K_m} L_pair^m(S_i^m, S_j^m)."],"relevant_code_files":["iti_gen/model.py","dataloader/image_dataset.py"],"discrepancy_id":"530cb4ff","removed_in_postproc":false}
{"paper_url":"https://openreview.net/pdf?id=RgUPdudkWlN","paper_url_versioned":"https://openreview.net/pdf?id=RgUPdudkWlN","code_url":"https://github.com/sumyeongahn/CUDA_LTR","code_url_versioned":"https://github.com/sumyeongahn/CUDA_LTR/tree/f4c8d35ffff7242e97eadb71c6880f1cb5cee01a","code_license":null,"discrepancy_date":"2024-06-11T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://openreview.net/pdf?id=Wm6d44I8St","origin_discrepancy_text":"The original paper does not explicitly mention using the Cutout augmentation, yet the official codebase includes a flag to enable/disable Cutout, and the reproducers found that enabling Cutout is necessary to match the reported accuracies. This suggests the original experiments likely used Cutout, creating a mismatch between the paper\u2019s stated methodology and the implementation. The omission in the paper could mislead readers about the exact training recipe needed to achieve the reported results. The reproducers\u2019 analysis shows that CUDA\u2019s reported gains depend on Cutout, implying an unreported dependency in the paper. Thus, there is a paper\u2013code discrepancy regarding the use of Cutout augmentation.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper\u2019s experimental setup does not mention using Cutout augmentation, while the official code exposes and uses a --cutout option (and the README example enables it). The CIFAR training pipeline applies Cutout when this flag is set. This creates a clear mismatch between the paper\u2019s described training recipe and the implementation.","discrepancy_description":"The paper describes standard data preprocessing (random crop and horizontal flip for CIFAR-100-LT and large-scale datasets) and does not state that Cutout is part of the training recipe. In the official codebase, the CIFAR pipeline includes an explicit --cutout flag and, when enabled, applies a Cutout transform after the basic preprocessing; the README\u2019s training command for CIFAR enables this flag by default. Thus, the implementation uses an additional augmentation (Cutout) not reported in the paper\u2019s methods. Reproducers noted that activating Cutout is necessary to match reported accuracies, suggesting that Cutout was effectively part of the original training although omitted from the paper.","relevant_paper_sections":["For data preprocessing, we follow the default settings of Cao et al. (2019). For CIFAR-100-LT, each side of the image is padded with 4 pixels, and a 32 \u00d7 32 crop is randomly selected from the padded image or its horizontal flip. For ImageNet-LT and iNaturalist 2018, after resizing each image by setting the shorter side to 256 pixels, a 224 \u00d7 224 crop is randomly sampled from an image or its horizontal flip.","We present in detail the augmentation operations that CUDA utilizes. There have been numerous data augmentation operations in vision tasks. We used totally 22 augmentations for CUDA with their own parameter set."],"relevant_code_files":["README.md","cifar/utils/config.py","cifar/aug/transforms.py","cifar/aug/cutout.py","cifar/datasets/cifar100.py","large_scale/opts.py"],"discrepancy_id":"fbdbe39a","removed_in_postproc":false}
{"paper_url":"https://proceedings.neurips.cc/paper_files/paper/2023/file/c2eac51b6353a4441e8b7426f8e8db78-Paper-Conference.pdf","paper_url_versioned":"https://proceedings.neurips.cc/paper_files/paper/2023/file/c2eac51b6353a4441e8b7426f8e8db78-Paper-Conference.pdf","code_url":"https://github.com/ymLeiFDU/LICO","code_url_versioned":"https://github.com/ymLeiFDU/LICO/tree/fabb62ced62f12ac86736c711cb5f3b1631aba06","code_license":null,"discrepancy_date":"2024-06-02T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://openreview.net/pdf?id=Mf1H8X5DVb","origin_discrepancy_text":"The paper describes the OT loss as creating a fine-grained alignment between individual image feature maps and individual prompt tokens, but the released code pools the text encoder outputs into a single vector per class using an EOT/[CLS]-like token. This aggregation removes token-level information and undermines the intended token-to-feature-map correspondence central to the OT objective. As a result, the OT loss in the code cannot implement the paper\u2019s stated mechanism of aligning specific tokens with specific feature maps, potentially rendering the OT loss ineffective for its claimed purpose.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The released code pools the text encoder outputs into a single per-class vector using the EOT token, and the OT is computed between image feature maps and these class-level vectors (mapped to 64-D), not between feature maps and individual prompt tokens as described in the paper.","discrepancy_description":"The paper states that the OT loss performs fine-grained alignment between image feature maps and specific prompt tokens: it forms G_i \u2208 R^{M\u00d7d\u2032} containing all token embeddings (context tokens and class token) and aligns them with F_i \u2208 R^{N\u00d7d\u2032} using OT, establishing token-to-feature-map correspondences. In the code, prompts are fed through a CLIP-like text encoder and then aggregated via the EOT token into a single 512-D text feature per class; this single vector is then passed through an MLP to 64-D and compared via the Sinkhorn OT solver to image feature maps (channels \u00d7 spatial) treated as 64-D vectors. Consequently, the implemented OT aligns image feature maps with a single class-level text vector (per class) rather than with individual prompt tokens, removing the token-level structure central to the paper\u2019s stated OT objective.","relevant_paper_sections":["we impose each prompt token to correlate with certain feature maps. Considering that the feature maps are redundant to the final classification decision, we propose to encourage the context tokens to guide certain feature maps through distribution alignment using optimal transport (OT) theory.","In order to measure the distances between image and language features in latent space, we append a mapping net h_\u03c8, a multilayer perceptron (MLP) with only one hidden layer, to map the d-dimension language features t_i \u2208 R^{M \u00d7 d} to the space of d\u2032-dimension, i.e., G_i \u2208 R^{M \u00d7 d\u2032}, which is the same as that of image features F_i.","we propose to align G_i=[g_1, g_2, \u2026, g_{M-1}, g_{l_i}]^\u22a4 \u2208 R^{M\u00d7d\u2032} and F_i=[f_1, f_2, \u2026, f_N]^\u22a4 \u2208 R^{N\u00d7d\u2032} for achieving consistency between feature maps and specific prompt tokens.","For the given image feature maps F_i \u2208 R^{N \u00d7 d\u2032} and a prompt tokens G_i \u2208 R^{M \u00d7 d\u2032}, we construct two discrete distributions: \u03bc=\u2211_{n=1}^N u_n \u03b4_{f_n}, v=\u2211_{m=1}^M v_m \u03b4_{g_m} \u2026 D_OT(\u03bc, v)=min_{T\u2208\u03a0(\u03bc,v)} \u2211_{n=1}^N \u2211_{m=1}^M T_{n,m}\u00b7c(f_n, g_m) \u2026"],"relevant_code_files":["models/text_encoder.py","models/wideresnet_prompt.py","models/modules/sinkhorn_distance.py","models/clip/model.py"],"discrepancy_id":"f3debb36","removed_in_postproc":false}
{"paper_url":"https://proceedings.neurips.cc/paper_files/paper/2023/file/c2eac51b6353a4441e8b7426f8e8db78-Paper-Conference.pdf","paper_url_versioned":"https://proceedings.neurips.cc/paper_files/paper/2023/file/c2eac51b6353a4441e8b7426f8e8db78-Paper-Conference.pdf","code_url":"https://github.com/ymLeiFDU/LICO","code_url_versioned":"https://github.com/ymLeiFDU/LICO/tree/fabb62ced62f12ac86736c711cb5f3b1631aba06","code_license":null,"discrepancy_date":"2024-06-02T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://openreview.net/pdf?id=Mf1H8X5DVb","origin_discrepancy_text":"The paper\u2019s prompt design uses shared learnable context tokens across classes, with only the class label token varying per class. In contrast, the codebase assigns a separate learned context for each class prompt. This deviates from the paper\u2019s formulation and changes the nature of the language supervision by making prompt context class-specific rather than shared, which could affect both the manifold matching and the intended generalization behavior.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper\u2019s formulation implies shared learnable context tokens across classes (only the class label token varies), while the code explicitly initializes and learns a separate set of context tokens for each class.","discrepancy_description":"The paper defines prompts as a sequence of shared learnable context tokens followed by a class label token, and its notation G_i = [g_1, ..., g_{M-1}, g_{l_i}] makes only the class token depend on the sample/class, implying the context tokens are shared across all classes. It also discusses dynamic context as shuffling these learnable context tokens, again suggesting a single shared pool of context tokens. In contrast, the code\u2019s PromptLearner creates class-specific contexts by initializing a separate context tensor of shape [n_cls, n_ctx, ctx_dim] and even prints \"Initializing class-specific contexts,\" thereby learning a distinct context per class prompt. Thus, the implementation deviates from the paper by making the prompt context class-specific rather than shared.","relevant_paper_sections":["For the language modeling branch, the text encoder g_{\u03c6} takes as input the constructed learnable prompt: t_i = [X_1, X_2, ..., X_{M-1}, t_i], where [...] t_i is the text corresponding to label of the i-th sample, X_m are learnable context tokens, and M\u22121 is the number of context tokens.","In this paper, we propose to align G_i = [g_1, g_2, ..., g_{M-1}, g_{l_i}]^\u22a4 \u2208 R^{M\u00d7d\u2032} and F_i = [f_1, f_2, ..., f_N]^\u22a4 \u2208 R^{N\u00d7d\u2032} for achieving consistency between feature maps and specific prompt tokens.","Dynamic context (DC). To endow each image with diverse prompt tokens, we shuffle the learnable context tokens in each training iteration..."],"relevant_code_files":["models/text_encoder.py","models/wideresnet_prompt.py"],"discrepancy_id":"7a9a3f93","removed_in_postproc":false}
{"paper_url":"https://proceedings.mlr.press/v162/crabbe22a/crabbe22a.pdf","paper_url_versioned":"https://proceedings.mlr.press/v162/crabbe22a/crabbe22a.pdf","code_url":"https://github.com/JonathanCrabbe/Label-Free-XAI","code_url_versioned":"https://github.com/JonathanCrabbe/Label-Free-XAI/tree/3f6fd5e98ab2fe431d6c15abeb5896c3c7747c2b","code_license":"MIT License","discrepancy_date":"2023-07-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/8173656/files/article.pdf","origin_discrepancy_text":"The paper states that each VAE configuration was run 20 times, but the released codebase only implements (and is set up for) 5 runs. This reflects a mismatch between the experimental protocol reported in the paper and what the public repository enables by default. Such a difference could impact aggregated statistics and stability analyses (e.g., Pearson correlations and boxplots) that depend on the number of runs.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper states that 20 runs were conducted for each VAE configuration, while the public code\u2019s VAE experiment scripts default to 5 runs (n_runs=5). Running the repository as provided will therefore execute 5 runs, not 20, unless the user manually changes the parameter.","discrepancy_description":"The paper\u2019s Section 4.3 describes the disentangled VAE experiments as being repeated 20 times per configuration (\u03b2-VAE and TC-VAE, for \u03b2 in {1,5,10}), amounting to 120 VAEs per dataset. In contrast, the released repository\u2019s experiment scripts for both MNIST and dSprites define n_runs with a default value of 5 and loop over range(1, n_runs+1), so the setup performs 5 runs unless explicitly overridden. Thus, there is a mismatch between the reported experimental protocol (20 runs) and the code\u2019s default configuration (5 runs), which affects the number of repetitions used to aggregate the reported metrics.","relevant_paper_sections":["These VAEs are trained on the MNIST and dSprites datasets (90%-10% train-test split) to minimize their objective. We use d_H=3 latent units for MNIST and d_H=6 for dSprites. We train 20 disentangled VAEs of each type for \u03b2\u2208{1,5,10}.","\u2026we use this criterion to select a VAE to analyse among the 120 VAEs we trained on each dataset."],"relevant_code_files":["experiments/dsprites.py","experiments/mnist.py"],"discrepancy_id":"3bdc7c6f","removed_in_postproc":false}
{"paper_url":"https://openreview.net/pdf?id=wbPObLm6ueA","paper_url_versioned":"https://openreview.net/pdf?id=wbPObLm6ueA","code_url":"https://github.com/sgiguere/Fairness-Guarantees-under-Demographic-Shift","code_url_versioned":"https://github.com/sgiguere/Fairness-Guarantees-under-Demographic-Shif/tree/d081307d34cde75ca74e07ddbe059e8273095aee","code_license":"Other","discrepancy_date":"2023-07-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/8173680/files/article.pdf","origin_discrepancy_text":"The paper states that the dataset should be split evenly between the candidate selection set (Dc) and the fairness test set (Df), but the released code uses a 60%\u201340% split. This is a methodological inconsistency between the paper and implementation that can affect the number of samples available for model selection versus fairness testing. The reproducing authors adhered to the code\u2019s 60\u201340 split when running experiments, acknowledging the deviation from the paper.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper explicitly states an even split between the candidate selection set and the fairness test set, while the released code sets a default split of 40% candidate and 60% fairness (i.e., a non-even 60\u201340 split). This directly contradicts the paper's stated experimental protocol.","discrepancy_description":"The paper\u2019s Appendix C.1 states that, in their experiments, the dataset is split evenly into the candidate selection set (Dc) and the fairness test set (Df). In contrast, the public code builds the training split by allocating a fraction r_cand_v_safe of the training data to the candidate set and the remainder to the safety set, with the default r_cand_v_safe=0.4. This means the implementation uses a 40%\u201360% split (candidate\u2013safety), i.e., a non-even 60\u201340 split between the two halves of the Seldonian pipeline. Therefore, there is a methodological inconsistency: the paper describes a 50\u201350 split, whereas the default implementation uses 40\u201360.","relevant_paper_sections":["Finally, we note that one area of future research might consider the optimal way to split an input dataset into parts used for candidate selection, Dc, and for evaluating the fairness test, Df. Specifically, increasing the size of Dc improves the ability of the candidate selection step to identify models that are accurate and generalize well, but reduces the size of Df and makes the fairness test more difficult to pass. In our experiments, we split the input data evenly between Dc and Df, but we hypothesize that there may be more effective techniques for determining the optimal splitting proportion."],"relevant_code_files":["Python/experiments/classification/adult_demographic_shift.py","Python/experiments/classification/brazil_demographic_shift.py"],"discrepancy_id":"f9a805be","removed_in_postproc":false}
{"paper_url":"https://proceedings.mlr.press/v139/yuan21c/yuan21c.pdf","paper_url_versioned":"https://proceedings.mlr.press/v139/yuan21c/yuan21c.pdf","code_url":"https://github.com/divelab/DIG","code_url_versioned":"https://github.com/divelab/DIG/tree/341a24c15c7c6a9a7414d5a6961e41e9277b9594","code_license":"GNU General Public License v3.0","discrepancy_date":"2023-07-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/8173753/files/article.pdf","origin_discrepancy_text":"The paper specifies that child selection during MCTS uses an Upper Confidence Bound (UCB) formula with Shapley-based rewards, but the authors' code introduces an additional tie-break heuristic based solely on node degree when visit counts are zero (making UCB undefined). This degree-based tie-break is not described in the paper and deviates from the model-driven selection implied by the UCB rule. As a result, in unexplored parts of the tree the implementation may prioritize nodes by structural degree rather than by Shapley estimates, potentially affecting the exploration behavior and explanation quality.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper describes child selection using a UCB rule with Shapley-based rewards but does not specify how ties are broken when all visit counts are zero at a newly expanded node. In the code, when all children have zero visits, the argmax over identical UCB scores falls back to the ordering of candidate pruning actions, which is determined purely by a node-degree-based pruning strategy (high-to-low or low-to-high). This degree-based tie-break is not described in the UCB formulation in the paper.","discrepancy_description":"The paper states that during MCTS, child selection follows a UCB formula that combines average reward and an exploration term weighted by a Shapley-based reward for each action, and \u201cthe child is selected following Eq.(2, 3)\u201d. However, at the start of exploring a new part of the search tree all child visit counts are zero, making the UCB values equal (Q=0 and the exploration term uses sqrt of total counts, which is also zero at first), leaving the selection undefined by the UCB alone. In the authors\u2019 implementation, when this happens selection defaults to the order in which candidate pruning actions are enumerated, which is controlled by a pruning strategy that ranks nodes by degree (high-to-low or low-to-high) and limits to a fixed number of candidates. Thus, in unexplored regions, the code effectively uses a degree-based tie-break to choose among children rather than differentiating by Shapley estimates as implied by the UCB rule. While the paper later discusses degree-based pruning strategies for efficiency, it does not describe their use as a tie-breaker for the UCB selection under zero-visit conditions.","relevant_paper_sections":["Formally, the action selection criteria of node N_i are defined as\n\na* = argmax_{a_j} Q(N_i, a_j) + U(N_i, a_j)\nU(N_i, a_j) = \u03bb R(N_i, a_j) \u221a(\u2211_k C(N_i, a_k)) / (1 + C(N_i, a_j))\n\nwhere \u03bb is a hyperparameter to control the trade-off between exploration and exploitation.","Algorithm 1 ... for all possible pruning actions of h(curNode) do Obtain child node N_j and its subgraph G_j. Compute R(curNode, a_j) = Score(f(\u00b7), G, G_j) with Algorithm 2. end for Select the child N_next following Eq.(2, 3).","Finally, we discuss the pruning actions in our MCTS. ... Instead of exploring all possible node pruning actions, we explore two strategies: Low2high and High2low. First, Low2high arranges the nodes based on their node degrees from low to high and only considers the pruning actions corresponding to the first k low degree nodes. Meanwhile, High2low arranges the nodes in order from high degree to low degree and only considers the first k high degree nodes for pruning."],"relevant_code_files":["dig/xgraph/method/subgraphx.py","benchmarks/xgraph/subgraphx.py","benchmarks/xgraph/config/explainers/subgraphx.yaml"],"discrepancy_id":"a1e977d5","removed_in_postproc":false}
{"paper_url":"https://openreview.net/pdf?id=BXewfAYMmJw","paper_url_versioned":"https://openreview.net/pdf?id=BXewfAYMmJw","code_url":"https://github.com/autonomousvision/counterfactual_generative_networks","code_url_versioned":"https://github.com/autonomousvision/counterfactual_generative_networks/tree/335bebdb47e21a617db9c8ebddce07b670330e7c","code_license":"MIT License","discrepancy_date":"2022-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/6574635/files/article.pdf","origin_discrepancy_text":"MNIST training protocol mismatch: the paper suggests training classifiers on a combination of original and CGN-generated counterfactual (CF) data, whereas the released code trains using only CF data. This alters the experimental setup and can affect reported performance. The authors of the reproducibility study explicitly note this inconsistency and evaluate both setups, observing that the choice influences results (notably on W-MNIST).","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper states that MNIST classifiers are trained on both real and counterfactual (CF) images (and explicitly evaluates \u201cOriginal + CGN\u201d), while the released MNIST classifier code trains on a single dataset at a time and the README instructs training on the counterfactual dataset only, with no option to mix in original data.","discrepancy_description":"The paper describes training classifiers using a combination of original MNIST data and CGN-generated counterfactual images. This is explicitly stated in Section 3.2 and reinforced by the MNIST ablation (Appendix A.3) and Table 2 which presents an \u201cOriginal + CGN\u201d setting. In contrast, the released MNIST training code (mnists/train_classifier.py) only supports training on a single dataset per run; the provided datasets and README usage instruct training on the counterfactual dataset alone (e.g., wildlife_MNIST_counterfactual) without combining with original data. Thus, the code implements CF-only training for MNIST classifiers rather than the paper\u2019s combined original+CF setup, constituting a mismatch in the MNIST training protocol.","relevant_paper_sections":["3.2 Generating Counterfactuals to Train Classifiers: We train on both real and counterfactual images. For MNIST, more counterfactual images always increase the test domain results; see the ablation study in Appendix A.3.","4.3 MNIST Classification: Original + CGN is additionally trained on counterfactual data to predict the input labels of the shape IM. Original + GAN is a baseline that is trained on real and generated, non-counterfactual samples.","Appendix A.3 Ablation Studies: In Figure 7, we study the effects of the number of counterfactual data points on the test accuracy. We increase the amount of counterfactual data while the amount of real data is fixed (50k for MNIST)."],"relevant_code_files":["mnists/train_classifier.py","mnists/dataloader.py","mnists/generate_data.py","README.md"],"discrepancy_id":"aa357802","removed_in_postproc":false}
{"paper_url":"https://www.ijcai.org/proceedings/2021/0051.pdf","paper_url_versioned":"https://www.ijcai.org/proceedings/2021/0051.pdf","code_url":"https://github.com/naveenr414/ijcai-rideshare","code_url_versioned":"https://github.com/naveenr414/ijcai-rideshare/tree/94c528980e079d87757df7711deba17decb2473a","code_license":null,"discrepancy_date":"2022-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/6574637/files/article.pdf","origin_discrepancy_text":"The paper\u2019s income-redistribution formula uses Shapley values in both the kept-income term and the pooled-redistribution term (r\u00b7v_i and \u2211(1\u2212r)\u00b7v_j), but the authors later clarified this was a typo; the implementation should use pre-redistribution earnings \u03c0_i in those places (r\u00b7\u03c0_i and \u2211(1\u2212r)\u00b7\u03c0_j). The authors updated the original code accordingly, indicating a mismatch between the paper\u2019s stated equation and the corrected implementation.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper\u2019s redistribution equation (Eq. 12) uses Shapley values in the kept-income term and in the pooled term (r\u00b7v_i and \u2211(1\u2212r)\u00b7v_j), while the implementation uses pre-redistribution earnings (\u03c0_i) in those places (r\u00b7\u03c0_i and \u2211(1\u2212r)\u00b7\u03c0_j). The code reflects the corrected formula the authors later communicated, so there is a mismatch between the paper\u2019s printed equation and the code.","discrepancy_description":"The paper states that drivers keep an r fraction of their income but then presents Equation (12) where the kept-income term is r\u00b7v_i and the pooled redistribution uses \u2211(1\u2212r)\u00b7v_j, i.e., both terms use Shapley values. The narrative sentence immediately before the equation also says the pool is collected from income \u2211(1\u2212r)\u00b7\u03c0_i, creating an internal inconsistency. In the code, the redistribution calculation uses pre-redistribution income \u03c0_i for the kept portion (r\u00b7\u03c0_i) and for the pooled amount (\u2211(1\u2212r)\u00b7\u03c0_j), while allocating the pool proportional to max(0, v_i \u2212 r\u00b7\u03c0_i), which aligns with the authors\u2019 later correction. Therefore, the code implements the corrected formula whereas the paper\u2019s printed equation contains the Shapley terms in places where \u03c0 should be used, confirming a paper-code discrepancy.","relevant_paper_sections":["Abstract: '... we explore income redistribution as a way to combat income inequality by having drivers keep an r fraction of their income, and contributing the rest to a redistribution pool.'","Section 5.1: 'We collect \\(\\sum_{i=1}^{n} (1 - r) \\pi_i\\) from all drivers, and redistribute it proportional to the difference between their value and earnings, which is \\(\\max(0, v_i - r \\pi_i)\\). After redistribution, each driver earns \\(q_i\\), defined by\n\n\\( q_i = r v_i + \frac{\\max(0, v_i - r \\pi_i)}{\\sum_{j=1}^{n} \\max(0, v_j - r \\pi_j)} \\sum_{j=1}^{n} (1 - r) v_j. \\) (Equation 12)'"],"relevant_code_files":["src/plots/shapley.py","src/plots/util.py"],"discrepancy_id":"866db454","removed_in_postproc":false}
{"paper_url":"https://aclanthology.org/2021.acl-long.556.pdf","paper_url_versioned":"https://aclanthology.org/2021.acl-long.556.pdf","code_url":"https://github.com/1783696285/SKS","code_url_versioned":"https://github.com/1783696285/SKS/tree/1dce1dff817acc0d77b08d3a59c79d60759a095d","code_license":null,"discrepancy_date":"2022-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/6574639/files/article.pdf","origin_discrepancy_text":"The architecture of the expert units\u2019 second feed-forward network differs between the paper and the released code. The paper specifies that the second feed-forward network has 200 hidden units, whereas the code implements the second network as two layers with 150 units each. This constitutes a clear paper\u2013code mismatch in the model\u2019s capacity/configuration within the sentiment knowledge sharing (SKS) component and could plausibly affect performance.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper states the second feed-forward network has two layers with 200 neurons, while the released code implements it as two Dense layers of size 150 each; this mismatch is directly visible in the code.","discrepancy_description":"The paper describes each expert unit in the sentiment knowledge sharing layer as comprising a multi-head attention layer followed by two feed-forward networks: the first with one layer of 400 neurons and the second with two layers of 200 neurons. In the released code, the ExpertModule_trm builds the expert with a Dense(400) layer and then two Dense layers whose sizes are given by expert_units, which are set to [150, 150] when constructing the HSMMBottom module. Thus, the implemented second feed-forward network uses two layers of 150 units each, not the two layers of 200 units reported in the paper. This constitutes a clear discrepancy between the reported model configuration and the actual implementation of the SKS component\u2019s expert units.","relevant_paper_sections":["Our feature extraction units layer is composed of a multi-head attention layer and two feed forward neural networks.","For the sentiment knowledge sharing layer, the multi-head attention has 4 heads. The first Feed-Forward network has one layer with 400 neurons and the second has two layers with 200 neurons. The dropout is used after each layer, and the rate is 0.1."],"relevant_code_files":["DNN/my_layers.py","DNN/models.py"],"discrepancy_id":"66841ab0","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/pdf/2212.06322","paper_url_versioned":"https://arxiv.org/pdf/2212.06322v1.pdf","code_url":"https://github.com/gaow0007/ATSPrivacy","code_url_versioned":"https://github.com/gaow0007/ATSPrivacy/tree/b9bf5da58d515167f94d859e256fd8b18d8a1f7c","code_license":"MIT License","discrepancy_date":"2022-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/6574647/files/article.pdf","origin_discrepancy_text":"The code scales the training loss by a factor of 0.5, but the reconstruction attack code does not apply this factor, causing the attacker to optimize under a different loss than the one used to generate the victim gradients. This mismatch is not documented in the paper and can make reconstructions artificially more difficult by changing gradient magnitudes. The original authors acknowledged this as a bug, and the replicators fixed it in their experiments.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The code uses a loss wrapper that multiplies the classification loss by 0.5 during training and when creating the victim gradients, while the reconstruction algorithm uses the unscaled CrossEntropy loss to generate its predicted gradients. This creates a mismatch between the gradients being matched and the gradients used during the attacker\u2019s optimization, and this scaling difference is not documented in the paper.","discrepancy_description":"The paper does not document any special scaling or modification of the training loss beyond using standard classification loss, nor does it describe any difference between the loss used to generate gradients and the loss used during reconstruction attacks. In the code, the training (and victim-gradient generation) uses a Classification loss wrapper that returns 0.5 times torch.nn.CrossEntropyLoss, thereby halving the gradient magnitudes. However, the reconstruction algorithm (GradientReconstructor) internally defines and uses torch.nn.CrossEntropyLoss without the 0.5 factor to compute model gradients for its candidate images. Consequently, the attacker optimizes under a different loss scaling than the one used to generate the victim gradients, producing a systematic mismatch that is not reflected in the paper.","relevant_paper_sections":[null],"relevant_code_files":["inversefed/data/loss.py","inversefed/training/training_routine.py","benchmark/cifar100_attack.py","inversefed/reconstruction_algorithms.py"],"discrepancy_id":"36c92030","removed_in_postproc":false}
{"paper_url":"https://proceedings.mlr.press/v139/mahajan21b/mahajan21b.pdf","paper_url_versioned":"https://proceedings.mlr.press/v139/mahajan21b/mahajan21b.pdf","code_url":"https://github.com/microsoft/robustdg","code_url_versioned":"https://github.com/microsoft/robustd/tree/3eee1730ae9efaae2bd763f9835ac540b9bde02a","code_license":"MIT License","discrepancy_date":"2022-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/6574661/files/article.pdf","origin_discrepancy_text":"The slab dataset generation script did not create datasets with a spurious correlation level of 1.0, which are needed for reproducing the paper\u2019s reported experiments (e.g., Table 1). The reproducing authors had to modify the code to append 1.0 to the spurious correlation list to generate the required data. This indicates a mismatch between the experimental settings used in the paper and the defaults/behavior of the provided code.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper\u2019s slab experiments specify a test domain with spurious correlation (noise) of 1.0, but the provided data generation script only creates datasets for spur_corr values [0.0, 0.10, 0.20, 0.90]. Thus, without modifying the code to include 1.0, the required test data is not produced.","discrepancy_description":"The paper\u2019s slab dataset setup states that source domains use noise probabilities 0.0 and 0.1, and the target (test) domain uses noise 1.0. In the released code, the slab dataset generator (data_gen_syn.py) defines spur_corr_list as [0.0, 0.10, 0.20, 0.90], which omits 1.0. Consequently, when trying to reproduce experiments that require a test domain at 1.0 noise (as per Table 1 and Section 3.2), the necessary dataset is not generated by default. The reproducing authors therefore had to add 1.0 to the spur_corr_list to create the required data files, evidencing a mismatch between the paper\u2019s experimental description and the default behavior of the code.","relevant_paper_sections":["The relationship of the linear feature with the label changes with domains (A.1); we do so by adding noise with probability \u03b5=0 for domain 1 and \u03b5=0.1 for domain 2. On the third (test) domain, we add noise with probability 1 (see Figure 1(b)).","Table 1. Slab Dataset: Source domains with noisy linear component with probability 0.0 and 0.1, target domain with noise 1.0."],"relevant_code_files":["data_gen_syn.py","reproduce_scripts/reproduce_slab.py","utils/slab_data.py"],"discrepancy_id":"03255952","removed_in_postproc":false}
{"paper_url":"https://arxiv.org/pdf/2012.08723","paper_url_versioned":"https://arxiv.org/pdf/2012.08723v1.pdf","code_url":"https://github.com/Ninarehm/attack","code_url_versioned":"https://github.com/Ninarehm/attack/tree/0d5a6b842d4e81484540151d879036e9fe2184f1","code_license":"MIT License","discrepancy_date":"2022-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/6574669/files/article.pdf","origin_discrepancy_text":"The anchoring attacks labeled as 'random' in the paper are effectively deterministic in the original code due to the random seed being reset every attack iteration. This causes the same point to be sampled each iteration, violating the intended randomness of the Random Anchoring Attack (RAA) and altering its behavior relative to the paper\u2019s description.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The code resets the Python RNG seed inside the per-iteration function for the Random Anchoring Attack (RAA), causing the same targets to be selected every iteration, while the paper specifies that targets should be sampled uniformly at random each iteration.","discrepancy_description":"The paper describes the Random Anchoring Attack (RAA) as sampling target points uniformly at random for each demographic group, and Algorithm 2 indicates this sampling occurs within an iterative loop (\u201cfor t = 1, 2, ...\u201d). In the released code, the function that executes each attack iteration resets the Python random seed (random.seed(0)) every time before selecting the targets and then uses random.randint to pick indices. Because the seed is reset on each iteration, the same indices are chosen repeatedly, making the RAA effectively deterministic across iterations. Thus, the implemented behavior diverges from the paper\u2019s description of per-iteration random sampling of x_target. The NRAA branch also calls random.seed(0), but its selection is deterministic by design and does not use random draws; the core discrepancy pertains to RAA\u2019s intended randomness.","relevant_paper_sections":["Random Anchoring Attack. In random anchoring attack, x_target is sampled uniformly at random for each demographic group.","Algorithm 2: Anchoring Attack ... for t = 1, 2, ... do ... Sample negative x_target from D_a and positive x_target from D_d with random or non-random technique."],"relevant_code_files":["Fairness_attack/iterative_attack.py","Fairness_attack/run_gradient_em_attack.py"],"discrepancy_id":"84acd23b","removed_in_postproc":false}
{"paper_url":"https://openaccess.thecvf.com/content/CVPR2021/papers/Oh_Background-Aware_Pooling_and_Noise-Aware_Loss_for_Weakly-Supervised_Semantic_Segmentation_CVPR_2021_paper.pdf","paper_url_versioned":"https://openaccess.thecvf.com/content/CVPR2021/papers/Oh_Background-Aware_Pooling_and_Noise-Aware_Loss_for_Weakly-Supervised_Semantic_Segmentation_CVPR_2021_paper.pdf","code_url":"https://github.com/cvlab-yonsei/BANA","code_url_versioned":"https://github.com/cvlab-yonsei/BANA/tree/407e92e521dc2db90055470f7ff2ccb5aa474093","code_license":"GNU General Public License v3.0","discrepancy_date":"2022-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/6574677/files/article.pdf","origin_discrepancy_text":"The paper defines the retrieval-based correlation map for Y_ret as the cosine similarity between normalized feature vectors and class prototypes, without any nonlinearity. However, in the authors' official code, this cosine similarity is passed through a ReLU before label assignment. This clamps negative similarities to zero, altering the intended behavior and potentially changing which pixels are assigned to classes during pseudo-label generation. This is a methodological mismatch between the paper's equations and the released implementation.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper defines the retrieval-based correlation map C_c(p) purely as a cosine similarity (dot product between L2-normalized features and prototypes) with no nonlinearity, whereas the official code applies a ReLU to this cosine similarity before taking argmax to form Y_ret.","discrepancy_description":"The paper\u2019s Section 3.2 defines the retrieval-based pseudo label Y_ret by first computing class prototypes q_c and then a correlation map C_c(p) as the cosine similarity between normalized feature vectors and these prototypes; labels are obtained by argmax over C_c(p). No nonlinearity is applied to C_c in the paper\u2019s formulation. In the official implementation (stage2.py), when generating Y_ret, the cosine similarity is passed through F.relu, clamping all negative values to zero before the argmax. Thus, the code alters the defined correlation by rectifying it, which deviates from the paper\u2019s stated method and can change which pixels are selected during pseudo-label generation.","relevant_paper_sections":["We extract a prototypical feature for each class as follows:\nq_{c} = \frac{1}{|\\mathcal{Q}_{c}|} \\sum_{\\mathbf{p} \\in \\mathcal{Q}_{c}} f(\\mathbf{p})","We use prototypical features as queries to retrieve similar ones from the feature map f, and compute a correlation map for each class as follows:\nC_{c}(\\mathbf{p}) = \frac{f(\\mathbf{p})}{\\|f(\\mathbf{p})\\|} \\cdot \frac{q_{c}}{\\|q_{c}\\|}","We then obtain pseudo segmentation labels Y_{\text{ret}} by applying the argmax function over the correlation maps C_{c}."],"relevant_code_files":["stage2.py"],"discrepancy_id":"b019c5d8","removed_in_postproc":false}
{"paper_url":"https://openreview.net/forum?id=FGqiDsBUKL0","paper_url_versioned":"https://openreview.net/forum?id=FGqiDsBUKL0","code_url":"https://github.com/XingangPan/GAN2Shape","code_url_versioned":"https://github.com/XingangPan/GAN2Shape/tree/deb9ac081d4f9243361a66befa7b9cfdb670d88c","code_license":"MIT License","discrepancy_date":"2022-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/6574685/files/article.pdf","origin_discrepancy_text":"The loss functions described in the paper do not match those implemented in the official code. The reproducibility authors report that several components\u2014particularly the training losses\u2014differ from the paper\u2019s description, leading them to follow the code rather than the paper. They also note that the paper did not properly report the exact losses and iterations, which had to be inferred from the code, reinforcing a paper\u2013code inconsistency around the optimization objectives.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The official code adds loss terms and uses a different perceptual metric than what is described in the paper, and the step\u20112 optimization objective in code differs from the paper\u2019s equation.","discrepancy_description":"The paper describes losses as follows: in Step 1 it optimizes the albedo network with a weighted sum of L1 and a perceptual loss (citing Johnson et al., 2016), in Step 2 it minimizes a feature-space distance (L1 on discriminator features) plus an L2 penalty on the latent offset, and in Step 3 it uses a reconstruction loss plus a smoothness regularizer on depth only. In the code, Step 2 adds an extra pixel-space L1 photometric loss on top of the discriminator-feature loss and offset regularization, which is not in the paper\u2019s Eq. (2). The code also uses an LPIPS-style learned perceptual loss (from the lpips package) rather than the Johnson et al. VGG perceptual loss stated in the paper, and applies a smoothness term not only on depth but also on diffuse shading. Therefore, while the high-level structure matches the paper, the actual implemented training objectives differ in several components from the paper\u2019s descriptions.","relevant_paper_sections":["Then the albedo network A is optimized with the reconstruction objective L(I, \u00ce), where \u00ce is calculated via Eq. (1), and L is a weighted combination of L1 loss and perceptual loss (Johnson et al., 2016).","Thus the optimization goal is: \u03b8_E = arg min_{\u03b8_E} (1/m) \u2211_{i=0}^m L\u2032(I_i, G(E(I_i)+w)) + \u03bb_1 ||E(I_i)||_2 ... Following Pan et al. (2020), we adopt the L1 distance of the discriminator features as the distance metric L\u2032...","The four networks are jointly optimized with the following reconstruction objective: \u03b8_D, \u03b8_A, \u03b8_V, \u03b8_L = arg min ... (1/m) \u2211 L(\u0128_i, \u03a6(D(I), A(I), V(\u0128_i), L(\u0128_i))) + \u03bb_2 L_smooth(D(I)), where ... L_smooth(D(I)) is a smoothness term defined the same way as in (Zhou et al., 2017)."],"relevant_code_files":["gan2shape/model.py","gan2shape/losses.py","gan2shape/utils.py","gan2shape/stylegan2/stylegan2-pytorch/lpips/__init__.py"],"discrepancy_id":"b2baef99","removed_in_postproc":false}
{"paper_url":"https://openaccess.thecvf.com/content/CVPR2021/papers/Ranjan_Learning_To_Count_Everything_CVPR_2021_paper.pdf","paper_url_versioned":"https://openaccess.thecvf.com/content/CVPR2021/papers/Ranjan_Learning_To_Count_Everything_CVPR_2021_paper.pdf","code_url":"https://github.com/cvlab-stonybrook/LearningToCountEverything","code_url_versioned":"https://github.com/cvlab-stonybrook/LearningToCountEverythin/tree/00ea1888c3a7c495ae06db0bddc3b90b7db8d52f","code_license":"MIT License","discrepancy_date":"2022-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/6574703/files/article.pdf","origin_discrepancy_text":"The Min-Count loss is implemented differently in the released code than defined in the paper. The paper defines a hinge on (1 \u2212 ||Z_b||_1), whereas the code squares this term inside the hinge (i.e., max(0, (1 \u2212 ||Z_b||_1)^2)). This alters the scale and gradients of the loss used during test-time adaptation. The authors of the reproducibility report verified the discrepancy by inspecting the code and also evaluated the impact of correcting it, finding little change in MAE/RMSE but confirming the mismatch.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper defines a linear hinge loss for Min-Count (sum_b max(0, 1 - ||Z_b||_1)), while the released code implements a squared-hinge version (zero when ||Z_b||_1 > 1, otherwise (1 - ||Z_b||_1)^2). This mismatch is evident from the MinCountLoss function in the code and differs from the paper\u2019s stated formula.","discrepancy_description":"The paper defines the Min-Count loss used for test-time adaptation as a linear hinge on the sum of predicted density in each exemplar box: L_MinCount = \u03a3_b max(0, 1 \u2212 ||Z_b||_1). In the released code, the MinCountLoss computes a squared error to 1 for each box sum X when X \u2264 1 and 0 otherwise (i.e., squared hinge: (max(0, 1\u2212X))^2). This means the code penalizes violations quadratically rather than linearly as described in the paper. The code\u2019s loss therefore differs in scale and gradients from the paper definition, even though both enforce the same constraint direction (encouraging at least one count per exemplar box).","relevant_paper_sections":["Min-Count Loss. For each exemplar bounding box b, the sum of the density values within Z_b should be at least one... This observation leads to an inequality constraint: ||Z_b||_1 \u2265 1... we define the following Min-Count loss to quantify the amount of constraint violation:\n\nL_MinCount = \u2211_{b \u2208 B} max(0, 1 \u2212 ||Z_b||_1)","The combined adaptation Loss. The loss used for test-time adaptation is the weighted combination of the Min-Count loss and the Perturbation loss.\n\nL_Adapt = \u03bb1 L_MinCount + \u03bb2 L_Per"],"relevant_code_files":["utils.py","test.py","demo.py"],"discrepancy_id":"29a1eeb5","removed_in_postproc":false}
{"paper_url":"https://proceedings.nips.cc/paper/2020/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf","paper_url_versioned":"https://proceedings.nips.cc/paper/2020/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf","code_url":"https://github.com/flyingdoog/PGExplainer","code_url_versioned":"https://github.com/flyingdoog/PGExplainer/tree/ec57ea2db3b196da7eb93179acffc64ec6958828","code_license":null,"discrepancy_date":"2021-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/4834242/files/article.pdf","origin_discrepancy_text":"Node-classification model architecture differs between the paper and the released code. The paper describes three consecutive GCN layers feeding directly into a fully connected classifier, whereas the code concatenates the intermediate outputs of the three GCN layers before the final classifier. The authors note they could not train the paper-described model to comparable accuracy with the provided hyperparameters.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper's description implies using the last GNN layer's representation for classification (standard three-layer GCN feeding into a classifier), while the released code concatenates the intermediate outputs of all GCN layers before the final classifier by default. This architectural difference is present in the code and not described in the paper.","discrepancy_description":"The paper states that in GNNs the hidden representation from the last layer is used as the final node representation and mentions training a three-layer GNN before applying explainers. In contrast, the released TensorFlow code for node classification builds a three-layer GCN and, by default, concatenates the outputs from each layer (args.concat=True) to form the node embedding, which is then fed into the final dense prediction layer. Thus, while the paper suggests a standard pipeline using the last layer's output, the code aggregates information across all layers via concatenation before classification, representing a discrepancy between the described and implemented architectures.","relevant_paper_sections":["Hidden representation of the last GNN layer serves as the final node representation: $\\mathbf{z}_{i}=\\mathbf{h}_{i}^{L}$, which is then used for downstream tasks, such as node/graph classification and link prediction.","We follow the experimental settings in GNNExplainer [53]. Specifically, for post-hoc methods including ATT, GNNExplainer, and PGExplainer, we first train a three-layer GNN and then apply these methods to explain predictions made by the GNN."],"relevant_code_files":["codes/models.py","codes/config.py"],"discrepancy_id":"3c7fd06f","removed_in_postproc":false}
{"paper_url":"https://proceedings.nips.cc/paper/2020/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf","paper_url_versioned":"https://proceedings.nips.cc/paper/2020/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf","code_url":"https://github.com/flyingdoog/PGExplainer","code_url_versioned":"https://github.com/flyingdoog/PGExplainer/tree/ec57ea2db3b196da7eb93179acffc64ec6958828","code_license":null,"discrepancy_date":"2021-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/4834242/files/article.pdf","origin_discrepancy_text":"Undocumented batch-normalization layers are present in the code for node-classification models, but are not described in the paper. Specifically, batch-norm layers are applied after the first and second GCN layers.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The node-classification code enables batch normalization by default and applies BatchNorm after each GCN layer except the last (i.e., after the first and second layers in a 3-layer GCN), while the paper does not describe using batch normalization in the backbone GCN.","discrepancy_description":"The paper states that, following GNNExplainer\u2019s setup, a three-layer GNN is first trained and then explained, but it does not mention any batch normalization in this backbone network. In the public code for node-classification tasks (synthetic datasets), the GCN model enables batch normalization by default (args.bn=True) and applies tf.keras.layers.BatchNormalization after all GCN layers except the last, i.e., after the first and second layers in a 3-layer GCN. This BatchNorm usage is not described in the paper\u2019s model description or experimental setup, making it an undocumented architectural difference between the paper and the released code.","relevant_paper_sections":["Experimental setup. We follow the experimental settings in GNNExplainer [53]. Specifically, for post-hoc methods including ATT, GNNExplainer, and PGExplainer, we first train a three-layer GNN and then apply these methods to explain predictions made by the GNN. Since weights in attention layers are jointly optimized with the GNN model in ATT, we thus train another GNN model with self-attention layers. We follow [1] to tune temperature \u03c4. We refer readers to the Appendix for more training details."],"relevant_code_files":["codes/models.py","codes/config.py","train.py","BA-shapes.py","BA-community.py","Tree-Cycles.py","Tree-Grid.py"],"discrepancy_id":"77ce66a1","removed_in_postproc":false}
{"paper_url":"https://proceedings.nips.cc/paper/2020/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf","paper_url_versioned":"https://proceedings.nips.cc/paper/2020/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf","code_url":"https://github.com/flyingdoog/PGExplainer","code_url_versioned":"https://github.com/flyingdoog/PGExplainer/tree/ec57ea2db3b196da7eb93179acffc64ec6958828","code_license":null,"discrepancy_date":"2021-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/4834242/files/article.pdf","origin_discrepancy_text":"Graph-classification model pooling differs from the paper\u2019s description. The code uses both max and mean pooling over the final GCN layer and concatenates them before the fully connected layers, whereas the paper does not describe using both.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The graph-classification code applies two pooled readouts (max and sum) of the final GCN layer and concatenates them before the classifier, while the paper does not describe using multiple pooling operations in its base GNN description.","discrepancy_description":"The paper states that a three-layer GNN is trained as the base model for graph classification but does not specify any special readout beyond the standard setup, nor does it mention combining multiple pooling functions. In the released code for graph classification, the GCN model computes two graph-level readouts from the last GCN layer (a max-pooling over features and a sum-pooling over features) and concatenates them before passing to a final dense layer for prediction. Thus, the implementation uses a concatenation of two pooling operations, whereas the paper does not describe this detail; specifically, the code uses max and sum (not mean) pooling.","relevant_paper_sections":["Experimental setup. We follow the experimental settings in GNNExplainer [53]. Specifically, for post-hoc methods including ATT, GNNExplainer, and PGExplainer, we first train a three-layer GNN and then apply these methods to explain predictions made by the GNN.","We denote these two functions by GNNE_{\u03a60}(\u00b7) and GNNC_{\u03a61}(\u00b7), respectively. For GNNs without explicit classification layers, we use the last layer instead. As a result, we can represent a GNN model with: Z = GNNE_{\u03a60}(G_o, X), Y = GNNC_{\u03a61}(Z)."],"relevant_code_files":["codes/forgraph/models.py","train_BA-2motif.py","MUTAG.py"],"discrepancy_id":"ecc908bc","removed_in_postproc":false}
{"paper_url":"https://proceedings.nips.cc/paper/2020/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf","paper_url_versioned":"https://proceedings.nips.cc/paper/2020/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf","code_url":"https://github.com/flyingdoog/PGExplainer","code_url_versioned":"https://github.com/flyingdoog/PGExplainer/tree/ec57ea2db3b196da7eb93179acffc64ec6958828","code_license":null,"discrepancy_date":"2021-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/4834242/files/article.pdf","origin_discrepancy_text":"The AUC evaluation for node-classification is computed only on 3-hop neighborhoods around each node in the code, a detail not mentioned in the paper. This changes which edges are considered when scoring explanations.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The code explicitly extracts and evaluates explanations on k-hop subgraphs (k equals the number of GCN layers, default 3), while the paper does not state that AUC for node-classification is computed only within such neighborhoods. The paper describes AUC as a binary edge classification over motif vs. non-motif edges without noting this restriction.","discrepancy_description":"In the paper, the node-classification evaluation is described as treating motif edges as positives and all other edges as negatives, with AUC used to quantify explanation quality; no restriction to a local k-hop neighborhood is stated. In the code, for node-classification datasets (BA-Shapes, BA-Community, Tree-Cycles, Tree-Grid), the evaluation is performed on subgraphs restricted to nodes within k hops of the target node, where k equals the number of GCN layers (default 3). The Extractor builds the k-hop subgraph and both explanation generation and AUC scoring only consider edges in this subgraph. Thus, the set of edges included in AUC computation in code is a 3-hop neighborhood around each node, whereas the paper does not mention this locality restriction.","relevant_paper_sections":["Quantitative evaluation. We follow the experimental settings in GNNExplainer [53] and formalize the explanation problem as a binary classification of edges. We treat edges inside motifs as positive edges, and negative otherwise. Importance weights provided by explanation methods are considered as prediction scores. AUC is adopted as the metric for quantitative evaluation.","Experimental setup. We follow the experimental settings in GNNExplainer [53]. Specifically, for post-hoc methods including ATT, GNNExplainer, and PGExplainer, we first train a three-layer GNN and then apply these methods to explain predictions made by the GNN.","Explanation network for node classification. ... We implement the \u03a9 = g_\u03a8(G_o, Z) to explain the prediction of node v with: \u03c9_ij = MLP_\u03a8([z_i; z_j; z_v])."],"relevant_code_files":["BA-shapes.py","BA-community.py","Tree-Cycles.py","Tree-Grid.py","codes/Extractor.py","codes/Explainer.py"],"discrepancy_id":"5a1bb334","removed_in_postproc":false}
{"paper_url":"https://proceedings.nips.cc/paper/2020/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf","paper_url_versioned":"https://proceedings.nips.cc/paper/2020/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf","code_url":"https://github.com/flyingdoog/PGExplainer","code_url_versioned":"https://github.com/flyingdoog/PGExplainer/tree/ec57ea2db3b196da7eb93179acffc64ec6958828","code_license":null,"discrepancy_date":"2021-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/4834242/files/article.pdf","origin_discrepancy_text":"The AUC evaluation includes only nodes that are part of a motif for node-classification datasets in the code, but this restriction is not reported in the paper. This effectively filters the evaluation set in a way not described in the manuscript.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The official code evaluates AUC for node-classification datasets only on nodes that belong to motifs (via explicit node filtering), while the paper does not report this restriction and only mentions filtering MUTAG to mutagenic graphs.","discrepancy_description":"In the paper, the evaluation protocol for node-classification datasets is described as treating edges inside motifs as positives and using AUC to measure performance, with an explicit note only for MUTAG that non-mutagenic graphs are excluded. In the released code, for all synthetic node-classification datasets (BA-Shapes, BA-Community, Tree-Cycles, Tree-Grid), the evaluation set is filtered to nodes that are part of motifs: the scripts construct an allnodes list that either ranges over index regions corresponding to motifs or filters by labels to exclude base-graph nodes (e.g., label == 1 or label != 0). AUC is then computed only over subgraphs extracted around these selected motif nodes. Thus, the code evaluates exclusively on motif nodes, a filtering step that is not stated in the paper for the node-classification experiments (the paper only mentions a similar filtering for MUTAG graphs).","relevant_paper_sections":["We follow the experimental settings in GNNExplainer [53] and formalize the explanation problem as a binary classification of edges. We treat edges inside motifs as positive edges, and negative otherwise. AUC is adopted as the metric for quantitative evaluation. Especially, for the MUTAG dataset, we only consider the mutagen graphs because no explicit motifs exist in nonmutagenic ones.","In these datasets, node/graph labels are determined by the motifs, which are treated as ground truth explanations. These motifs are utilized to calculate explanation accuracy for PGExplainer as well as other baselines."],"relevant_code_files":["BA-shapes.py","BA-community.py","Tree-Cycles.py","Tree-Grid.py","codes/config.py"],"discrepancy_id":"6a50c278","removed_in_postproc":false}
{"paper_url":"https://proceedings.nips.cc/paper/2020/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf","paper_url_versioned":"https://proceedings.nips.cc/paper/2020/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf","code_url":"https://github.com/flyingdoog/PGExplainer","code_url_versioned":"https://github.com/flyingdoog/PGExplainer/tree/ec57ea2db3b196da7eb93179acffc64ec6958828","code_license":null,"discrepancy_date":"2021-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/4834242/files/article.pdf","origin_discrepancy_text":"For the BA-2Motif graph-classification dataset, the AUC is computed on only a subset of graphs (to reduce computation) in the code, but this sampling is not disclosed in the paper.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The repository\u2019s BA-2motifs evaluation script computes AUC on only a subset of graphs (default: first 100 graphs, or 200 with another setting) via the allnodes list, while the paper does not disclose any such sampling for BA-2motifs; it only mentions filtering for MUTAG. Thus, the described difference between paper and code exists.","discrepancy_description":"The paper describes the BA-2motifs dataset and states that AUC is used to quantitatively evaluate explanations, repeating experiments 10 times, without noting any sampling of graphs for evaluation (it only mentions filtering MUTAG to mutagenic graphs). In the code for BA-2motifs, the evaluation set is explicitly restricted to a subset of graphs through the allnodes list: by default (args.setting==1) it evaluates only the first 100 graphs, with an alternative (args.setting==2) that evaluates 200 graphs, and only when args.setting==3 does it evaluate all 1000 graphs. The test() function computes the AUC exclusively over this allnodes subset, so the reported AUC is based on a sampled portion of the dataset. The paper does not disclose this sampling for BA-2motifs, creating a discrepancy between the described experimental setup and the implementation.","relevant_paper_sections":["For graph classification, we build the BA-2motifs dataset of 800 graphs. We adopt the BA graphs as base graphs. Half graphs are attached with \"house\" motifs and the rest are attached with five-node cycle motifs. Graphs are assigned to one of 2 classes according to the type of attached motifs.","Quantitative evaluation. We follow the experimental settings in GNNExplainer [53] and formalize the explanation problem as a binary classification of edges. We treat edges inside motifs as positive edges, and negative otherwise. Importance weights provided by explanation methods are considered as prediction scores. AUC is adopted as the metric for quantitative evaluation. Especially, for the MUTAG dataset, we only consider the mutagen graphs because no explicit motifs exist in nonmutagenic ones. For PGExplainer, we repeat each experiment 10 times and report the average AUC scores and standard deviations here."],"relevant_code_files":["BA-2motifs.py"],"discrepancy_id":"5cfb9a3a","removed_in_postproc":false}
{"paper_url":"https://aclanthology.org/2020.acl-main.387.pdf","paper_url_versioned":"https://aclanthology.org/2020.acl-main.387.pdf","code_url":"https://github.com/akashkm99/Interpretable-Attention","code_url_versioned":"https://github.com/akashkm99/Interpretable-Attention/tree/2d8dd378ad1f72e97d41c98781a08c8b1db2e84d","code_license":"GNU General Public License v3.0","discrepancy_date":"2021-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/4835592/files/article.pdf","origin_discrepancy_text":"For DS tasks, the code computes the final prediction using a tanh layer over an affine combination of the attention context vector and the question encoder\u2019s last hidden state, followed by a softmax. In contrast, Section 2.1 of the paper describes predicting solely from the attention-derived context vector via a simple softmax over W_0 c_\u03b1. This indicates a clear paper\u2013code mismatch in the formulation of the final prediction layer for DS tasks. Although the reproducibility authors note this may not change the core LSTM+attention architecture, the implemented decision function diverges from the paper\u2019s stated model.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper states that for dual-sequence tasks the final prediction is computed solely from the attention-derived context vector via softmax(W_o c_\u03b1). In contrast, the code for dual-sequence tasks combines the attention context vector with the question encoder\u2019s last hidden state through additional linear layers and a tanh nonlinearity before the final projection, i.e., it uses both c_\u03b1 and h_n^q. This constitutes a clear paper\u2013code mismatch in the final prediction layer.","discrepancy_description":"The paper\u2019s Section 2.1 describes a standard attention model for tasks with two input sequences P and Q where attention over P is computed using the last hidden state of Q as the query, and the final prediction is made solely from the attention-derived context vector, specifically \u0177 = softmax(W_o c_\u03b1). In the released code, for all dual-sequence (NLI, paraphrase, QA) tasks, the AttnDecoderQA combines the attended context vector from P with the last hidden state of Q: predict = Linear2(tanh(Linear_p(c_\u03b1) + Linear_q(h_n^q))), with masking and then cross-entropy, i.e., a tanh over an affine combination of c_\u03b1 and h_n^q precedes the final linear/softmax. Thus, while the attention mechanism uses h_n^q as a query in both paper and code, the code\u2019s decision layer also directly conditions on h_n^q, diverging from the paper\u2019s simpler softmax(W_o c_\u03b1) formulation.","relevant_paper_sections":["We attend to the intermediate representations of P, H^p = {h_1^p, ..., h_m^p} \u2208 R^{m\u00d7d} using the last hidden state h_n^q \u2208 R^d as the query, using the attention mechanism (Bahdanau et al., 2014), ... c_\u03b1 = \u03a3_{t=1}^m \u03b1_t h_t^p ... Finally, we use the attended context vector c_\u03b1 to make a prediction \\u007etilde{y} = softmax(W_o c_\u03b1).","For tasks with a single input sequence, we use a single LSTM to encode the sequence, followed by an attention mechanism (without query) and a final output projection layer."],"relevant_code_files":["model/modules/Decoder.py","model/Question_Answering.py"],"discrepancy_id":"51a0c305","removed_in_postproc":false}
{"paper_url":"https://papers.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf","paper_url_versioned":"https://papers.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf","code_url":"https://github.com/ermongroup/ncsn","code_url_versioned":"https://github.com/ermongroup/ncsn/tree/adb98fb5c9533b79933bfee0b3e883dc33f84e7a","code_license":"GNU General Public License v3.0","discrepancy_date":"2020-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/3818609/files/article.pdf","origin_discrepancy_text":"RefineNet dilation schedule differs between paper and code. The paper states that dilation is increased by a factor of two at each cascade (implying 1,2,4,8 across four cascades), whereas the official code uses dilation rates 1,1,2,4. This is a direct mismatch between the architectural description and the implementation.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The official code sets the RefineNet residual stages to use dilation rates 1, 1, 2, and 4, whereas the paper states that dilation is increased by a factor of two at each cascade, which implies 1, 2, 4, 8 across four cascades. This is a direct mismatch between the architectural description and the implementation.","discrepancy_description":"The paper explains that the score network architecture uses dilated/atrous convolutions and (in Appendix A) specifies that the dilation is doubled at each RefineNet cascade, which would correspond to dilation rates 1, 2, 4, and 8 for the four cascades. In the official code, the conditional RefineNet uses no dilation in the first two cascades and then uses dilations of 2 and 4 in the third and fourth cascades, respectively (i.e., dilation rates 1, 1, 2, 4). Thus, the implementation does not follow the paper\u2019s stated schedule of doubling dilation at every cascade; it leaves the second cascade undilated and does not reach dilation 8 in the final cascade.","relevant_paper_sections":["In the experiments, our model s_{\u03b8}(x, \u03c3) combines the architecture design of U-Net [46] with dilated/atrous convolution [64, 65, 8]\u2014both of which have been proved very successful in semantic segmentation. ... More details on our architecture can be found in Appendix A."],"relevant_code_files":["models/cond_refinenet_dilated.py","models/refinenet_dilated_baseline.py"],"discrepancy_id":"d1c77c19","removed_in_postproc":false}
{"paper_url":"https://papers.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf","paper_url_versioned":"https://papers.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf","code_url":"https://github.com/ermongroup/ncsn","code_url_versioned":"https://github.com/ermongroup/ncsn/tree/adb98fb5c9533b79933bfee0b3e883dc33f84e7a","code_license":"GNU General Public License v3.0","discrepancy_date":"2020-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/3818609/files/article.pdf","origin_discrepancy_text":"Filter-doubling and skip-connection implementation details differ between paper and code. The paper\u2019s description suggests doubling filters in cascades 2\u20134 (with standard 1\u00d71 convs on skip paths when channel counts change), but the code instead increases channel count at the end of the 1st cascade and uses 3\u00d73 convolutions on skip connections, including cases where the number of filters does not change.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The repository\u2019s CondRefineNet/RefineNet implementations change channel counts and implement skip connections differently from what is described/assumed in the paper\u2019s architecture description. In code, channel doubling during \u201cdown\u201d blocks happens in the second (final) convolution of the residual block, and the skip path sometimes uses 3\u00d73 convolutions (with dilation) even when the number of channels does not change. The paper defers architectural specifics to Appendix A and describes a RefineNet-like design, which the reproducibility report interpreted as standard practice (doubling channels per cascade and 1\u00d71 skip convs only when channel counts change). The code behavior indeed differs in these details.","discrepancy_description":"The paper states that the score network combines a U-Net-style architecture with dilated convolutions and refers readers to Appendix A for architectural details. This was interpreted (and implemented by the reproducibility authors) as a RefineNet-style setup where the number of filters doubles across cascades 2\u20134, and skip connections use standard 1\u00d71 convolutions when the number of channels changes. In the official code, the ConditionalResidualBlock increases the number of channels within the \u201cdown\u201d block via the second (final) convolution (e.g., in res2), rather than in the first convolution of the subsequent cascade. Moreover, for \u201cdown\u201d blocks that use dilation (e.g., in res3 and res4), the skip path applies a 3\u00d73 dilated convolution even when the number of channels does not change; only in the non-dilated \u201cdown\u201d case (e.g., res2) does the skip path use a 1\u00d71 ConvMeanPool. Thus, compared to the paper\u2019s (RefineNet-style) description, the code moves where the doubling happens within a block and uses 3\u00d73 convolutions on some skip connections, including when channel counts do not change.","relevant_paper_sections":["In the experiments, our model s_\u03b8(x, \u03c3) combines the architecture design of U-Net with dilated/atrous convolution\u2014both of which have been proved very successful in semantic segmentation. In addition, we adopt instance normalization in our score network, inspired by its superior performance in some image generation tasks, and we use a modified version of conditional instance normalization to provide conditioning on \u03c3_i. More details on our architecture can be found in Appendix A."],"relevant_code_files":["models/cond_refinenet_dilated.py","models/refinenet_dilated_baseline.py","runners/anneal_runner.py"],"discrepancy_id":"ffb14e4c","removed_in_postproc":false}
{"paper_url":"https://proceedings.neurips.cc/paper_files/paper/2019/file/fe663a72b27bdc613873fbbb512f6f67-Paper.pdf","paper_url_versioned":"https://proceedings.neurips.cc/paper_files/paper/2019/file/fe663a72b27bdc613873fbbb512f6f67-Paper.pdf","code_url":"https://github.com/polo5/ZeroShotKnowledgeTransfer","code_url_versioned":"https://github.com/polo5/ZeroShotKnowledgeTransfer/tree/e24915d713672554800372c777c9014e3399e13b","code_license":null,"discrepancy_date":"2020-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/3818623/files/article.pdf","origin_discrepancy_text":"In the Attention Transfer (AT) component, the paper states that the distance between teacher and student attention maps is computed using the Euclidean (L2) distance between normalized maps. However, the official code implements this term as a mean squared error (squared mean) over the batch and spatial dimensions. This alters the objective from an L2 norm to an averaged squared difference, potentially changing the optimization dynamics. The report also notes that the code computes attention maps using a squared mean over channels (contrary to [3]'s sum of squares suggestion), but the explicit paper\u2013code mismatch pertains to the distance metric (Euclidean vs squared mean). This discrepancy affects both the KD-AT baseline and the zero-shot model where AT is applied.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper defines the AT loss as an L2 (Euclidean) norm between L2-normalized attention maps, whereas the official code computes the mean of squared differences (MSE) of the normalized attention maps across batch and spatial dimensions. This is a genuine mismatch between the written objective and the implemented one.","discrepancy_description":"In Section 3.2, the paper adds an Attention Transfer (AT) term to the student loss that is written as the Euclidean (L2) norm of the difference between L2-normalized teacher and student attention maps, summed over selected layers. In the official code, the attention maps are first L2-normalized and then the distance term is computed as the mean squared error: the elementwise squared difference is averaged over batch and spatial dimensions ((a \u2212 b)^2). Thus, the code implements an averaged squared L2 distance rather than the unsquared L2 norm described in the paper. Additionally, the code forms attention maps via mean of squares across channels rather than a sum, although L2 normalization largely cancels the mean vs sum scaling; the core paper\u2013code mismatch is the distance metric (Euclidean norm vs mean squared error).","relevant_paper_sections":["and so we can add an attention term to the student loss:\n\n\\(\\mathcal{L}_{S}=D_{K L}\\left(T\\left(\boldsymbol{x}_{p}\right) \\| S\\left(\boldsymbol{x}_{p}\right)\right)+\beta \\sum_{l}^{N_{L}}\\left\\|\frac{\boldsymbol{f}\\left(A_{l}^{(t)}\right)}{\\left\\|\boldsymbol{f}\\left(A_{l}^{(t)}\right)\right\\|_{2}}-\frac{\boldsymbol{f}\\left(A_{l}^{(s)}\right)}{\\left\\|\boldsymbol{f}\\left(A_{l}^{(s)}\right)\right\\|_{2}}\right\\|_{2}\\)\nwhere \\(\beta\\) is a hyperparameter.","Here, \\(A_{l}^{(t)}\\) and \\(A_{l}^{(s)}\\) are the teacher and student activation blocks for layer \\(l\\), both made up of \\(N_{A_{l}}\\) channels. If we denote by \\(\boldsymbol{a}_{lc}\\) the \\(c\\)th channel of activation block \\(A_{l}\\), then we use the spatial attention map \\(\boldsymbol{f}\\left(A_{l}\right)=\\left(1 / N_{A_{l}}\right) \\sum_{c} \boldsymbol{a}_{lc}^{T}\\) as suggested by the authors of AT (Zagoruyko and Komodakis, 2016a)."],"relevant_code_files":["solver.py"],"discrepancy_id":"743a1998","removed_in_postproc":false}
{"paper_url":"https://proceedings.neurips.cc/paper_files/paper/2019/file/5faf461eff3099671ad63c6f3f094f7f-Paper.pdf","paper_url_versioned":"https://proceedings.neurips.cc/paper_files/paper/2019/file/5faf461eff3099671ad63c6f3f094f7f-Paper.pdf","code_url":"https://github.com/JannerM/mbpo","code_url_versioned":"https://github.com/JannerM/mbpo/tree/28b1e3b1382dcda34b421961286b59f77770d48c","code_license":"MIT License","discrepancy_date":"2020-05-01T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://zenodo.org/record/3818611/files/article.pdf","origin_discrepancy_text":"The paper's pseudocode specifies that policy updates are conducted solely on model-generated data (D_model), whereas the authors' released implementation mixes model rollouts with a portion of real-environment data during policy optimization. The reproducibility report notes that the code uses a ratio of model to real data when updating the policy, a substantive algorithmic difference not indicated in the paper's Algorithm 1. The authors also empirically verified that following the paper's pseudocode (no real data in policy updates) fails to reproduce the reported performance, while incorporating real data as in the code yields the expected results. This indicates a concrete mismatch between the algorithmic description in the paper and the behavior of the official code.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper\u2019s Algorithm 2 explicitly states that policy updates use only model-generated data (D_model), whereas the released code mixes model rollouts with real environment data in each policy update using a configurable real_ratio (default 0.1).","discrepancy_description":"The paper\u2019s Algorithm 2 for MBPO describes generating short model rollouts starting from states in the real replay buffer to build a model dataset (D_model), and then updating the policy solely on this model-generated dataset. Specifically, step 10 in Algorithm 2 states to update policy parameters on model data J_\u03c0(\u03c6, D_model), without mentioning any use of real environment samples during optimization. In contrast, the official code constructs each training batch by mixing a fraction of real environment transitions with model-generated transitions according to a real_ratio parameter (default 0.1), concatenating env and model batches for both actor and critic updates. Thus, while the paper prescribes policy updates only on D_model, the implementation performs policy optimization on a mixture of D_model and D_env, which is a substantive algorithmic deviation from the pseudocode.","relevant_paper_sections":["Algorithm 2 Model-Based Policy Optimization with Deep Reinforcement Learning\n...\n6: for M model rollouts do\n7: \tSample s_t uniformly from D_env\n8: \tPerform k-step model rollout starting from s_t using policy \u03c0_\u03c6; add to D_model\n9: for G gradient updates do\n10: Update policy parameters on model data: \u03c6 \u2190 \u03c6-\u03bb_\u03c0 \u2207_\u03c6 J_\u03c0(\u03c6, D_model)","Model usage. ... A practical implementation of MBPO is described in Algorithm 2. ... Even when the horizon length k is short, we can perform many such short rollouts to yield a large set of model samples for policy optimization."],"relevant_code_files":["mbpo/algorithms/mbpo.py"],"discrepancy_id":"120335b4","removed_in_postproc":false}
{"paper_url":"https://openreview.net/pdf?id=r1xMH1BtvB","paper_url_versioned":"https://openreview.net/pdf?id=r1xMH1BtvB","code_url":"https://github.com/google-research/electra","code_url_versioned":"https://github.com/google-research/electra/tree/8a46635f32083ada044d7e9ad09604742600ee7b","code_license":"Apache License 2.0","discrepancy_date":"2021-04-06T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://arxiv.org/pdf/2104.02756","origin_discrepancy_text":"The official ELECTRA code applies a different layer-wise learning rate decay than what is specified in the paper due to a bug. While the paper describes a particular layer-wise LR decay schedule (e.g., 0.8), the implementation inadvertently uses a lower effective decay, leading to smaller learning rates for lower layers than intended. This constitutes a mismatch between the optimization procedure described in the paper and what is implemented in the original repository.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper specifies a layer-wise learning rate decay (e.g., 0.8 for Base/Small, 0.9 for Large) during fine-tuning, but the official code applies an extra decay step due to how depths and exponents are computed, effectively producing lower learning rates for lower layers than intended (e.g., the top encoder layer gets lr * decay^2 instead of lr * decay^1). This matches the reported bug that the implementation uses a lower effective layer-wise LR than described.","discrepancy_description":"The paper states that ELECTRA uses a layer-wise learning rate decay (LLRD) during fine-tuning, searching over values like 0.9, 0.8, 0.7, and ultimately using 0.8 for Base/Small and 0.9 for Large. In the code, LLRD is implemented by mapping variable name substrings to a per-variable learning rate computed as lr * (decay ** (n_layers + 2 - depth)), where the classification head is assigned depth n_layers + 2 and encoder layers have depths 1..n_layers. This implementation makes the top encoder layer use lr * decay^2 (e.g., 0.64\u00d7lr for decay=0.8) and lower layers even smaller, which is an extra decay factor compared to the typical interpretation where the top encoder layer would be at lr * decay^1 relative to the head. Consequently, while the paper\u2019s hyperparameter is \u201c0.8\u201d (or \u201c0.9\u201d), the code applies a lower effective learning rate for lower layers than intended due to an off-by-one (actually plus-two) depth/exponent scheme.","relevant_paper_sections":["For Base-sized models we searched for a learning rate out of [3e-5, 5e-5, 1e-4, 1.5e-4] and the layer-wise learning-rate decay out of [0.9,0.8,0.7], but otherwise used the same hyperparameters as for Large models.","Layerwise LR decay | 0.8 for Base/Small, 0.9 for Large"],"relevant_code_files":["configure_finetuning.py","model/optimization.py","run_finetuning.py"],"discrepancy_id":"09062f79","removed_in_postproc":false}
{"paper_url":"https://dl.acm.org/doi/pdf/10.1145/3539618.3591626","paper_url_versioned":"https://dl.acm.org/doi/pdf/10.1145/3539618.3591626","code_url":"https://github.com/HansiZeng/UIA","code_url_versioned":"https://github.com/HansiZeng/UIA/tree/c939bb8bfcc1674f6de1ddeece0fd94b40031081","code_license":null,"discrepancy_date":"2025-04-03T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://link.springer.com/content/pdf/10.1007/978-3-031-88717-8_10.pdf","origin_discrepancy_text":"The code performs an additional downsampling of positive/relevant items after the dataset split that is not described in the paper. Specifically, for QBE and CIR, the code samples 5 random relevant items per anchor item, and for KS it limits to 10 relevant items per query when constructing instances. This second sampling reduces dataset size and limits the influence of very popular queries/items, but it diverges from the paper\u2019s data construction procedure. The reproducibility report attributes this difference to efficiency reasons and retains it for their runs.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The released code performs additional subsampling of positive pairs that the paper does not describe. In particular, the data construction scripts cap the number of positive/relevant items used per anchor (for QBE and CIR) and per query (for search) after the train/validation/test split, whereas the paper describes constructing all pairs implied by the labels without such caps.","discrepancy_description":"The paper describes, for Amazon ESCI, building three datasets directly from the labeled relations without further caps, e.g., for QBE using all (i1, i2) pairs with i1 in E(q) and i2 in S(q), and for CIR using all (i1, i2) where i2 in C(q), and for search using all (q, i) with i in E(q). In the code, after splitting queries/anchors into train/val/test, the scripts subsample positives when creating the final training triples: for QBE and CIR they sample at most 5 relevant items per anchor (randomly, via [:5] slices), and for search they cap the number of positives per query in the constructed q\u2192item triples (different scripts use different caps, e.g., 2 or 5; the paper does not mention any such cap). This additional downsampling reduces the number of instances and alters the distribution relative to the procedure described in the paper.","relevant_paper_sections":["We used the following procedure to construct our three datasets: (1) Keyword Search: {(q, i): \u2200q \u2208 Q \u2227 i \u2208 I_E(q)}, (2) Query by Example: {(i1, i2): \u2200q \u2208 Q \u2227 i1 \u2208 I_E(q) \u2227 i2 \u2208 I_S(q)}, and (3) Complementary Item Recommendation: {(i1, i2): \u2200q \u2208 Q \u2227 i1 \u2208 I_E(q) \u2227 i2 \u2208 I_C(q)}.","4.1.1 Dataset. ... We modify the data to fit our experimental setting. Each data point in KDD Cup 2022 - Task 2 is a (query, item, label) triplet. ... We used the following procedure to construct our three datasets..."],"relevant_code_files":["preprocess/unified_kgc/create_unified_train.py","preprocess/unified_kgc/create_similar_train.py","preprocess/unified_user/create_user_sequential_train_test.py"],"discrepancy_id":"03a374de","removed_in_postproc":false}
{"paper_url":"https://dl.acm.org/doi/pdf/10.1145/3539618.3591626","paper_url_versioned":"https://dl.acm.org/doi/pdf/10.1145/3539618.3591626","code_url":"https://github.com/HansiZeng/UIA","code_url_versioned":"https://github.com/HansiZeng/UIA/tree/c939bb8bfcc1674f6de1ddeece0fd94b40031081","code_license":null,"discrepancy_date":"2025-04-03T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://link.springer.com/content/pdf/10.1007/978-3-031-88717-8_10.pdf","origin_discrepancy_text":"For QBE, the Phase 1 negative sampling implemented in the code does not match the paper\u2019s description. The paper states Phase 1 negatives should be sampled from BM25 retrievals for each request, but the code randomly samples negatives from all items during Phase 1 for QBE. The authors modified the code to align it with the paper by using BM25-based negatives for QBE.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper states that in Phase 1 negatives are drawn from BM25 retrievals for each request, but the code constructs QBE (similar item) training triples with negatives sampled uniformly at random from all items (not from BM25 results). This mismatch is visible in the preprocessing scripts that generate the Phase 1 training data used by the trainers.","discrepancy_description":"The paper\u2019s training procedure describes a two-phase negative sampling strategy. In Phase 1 (non-personalized pre-training), for each request, negatives are sampled from the top 200 BM25-retrieved items; and in personalized fine-tuning, negatives are again sampled from BM25 results in addition to in-batch negatives. In the released code, when creating the Phase 1 training triples for Query by Example (the \u201csimilar item\u201d task, a2sp/h2sp), negatives are sampled uniformly at random from all items (sampler=None), while BM25-based negatives are only used for other tasks (e.g., complementary items and query-to-item). The same pattern appears in the personalized sequential dataset construction: even under a \u201cbm25\u201d setting, sim_rec negatives are randomly sampled rather than taken from BM25. Thus, the code\u2019s Phase 1 negative sampling for QBE does not follow the paper\u2019s BM25-based prescription; the authors note they later modified the code to use BM25-based negatives for QBE to align it with the paper.","relevant_paper_sections":["Phase 1: for each request in training data, we randomly sample negative items from the top 200 items retrieved by BM25. We set the ratio of negative samples to positive training instances to 1. We then train the model using a cross entropy loss function. Note that in addition to BM25 negative, we also use in-batch negatives.","Personalized Fine-Tuning. ... For negative sampling, we use BM25 results in addition to inbatch negatives (similar to Phase 1 in non-personalized pre-training). We use cross entropy loss function for training."],"relevant_code_files":["preprocess/unified_kgc/create_unified_train.py","preprocess/unified_kgc/create_similar_train.py","preprocess/unified_user/create_user_sequential_train_test.py","kgc-dr/trainer/unified_train.py"],"discrepancy_id":"e85ad122","removed_in_postproc":false}
{"paper_url":"https://dl.acm.org/doi/pdf/10.1145/3331184.3331265","paper_url_versioned":"https://dl.acm.org/doi/pdf/10.1145/3331184.3331265","code_url":"https://github.com/aliannejadi/qulac","code_url_versioned":"https://github.com/aliannejadi/qulac/tree/89d156280b75b2107bec9d719bfcbab76e54ce96","code_license":"MIT License","discrepancy_date":"2023-03-16T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://link.springer.com/content/pdf/10.1007/978-3-031-28241-6_3.pdf","origin_discrepancy_text":"The LTR feature files provided by the original authors appear to apply ad-hoc smoothing to retrieval scores (e.g., replacing zero scores with a small epsilon), whereas the paper does not mention this preprocessing and the reproduction used raw scores. Although the authors note this smoothing likely had minimal practical impact on the learned models, it remains an implementation detail that differs from what is described (or implied) in the paper.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper states that BM25/QL/RM3 scores are used as LTR features and does not mention any preprocessing or smoothing, while the feature files provided by the authors (used to train their LTR baselines) contain smoothed non-zero values where raw BM25 would be exactly zero for query\u2013question pairs with no term overlap. The repository does not contain code for generating these LTR features (they were created externally with Galago), so the implementation detail (smoothing) is not documented in the paper or reflected in the released code, confirming a paper\u2013implementation mismatch.","discrepancy_description":"The paper describes the LTR baselines for question retrieval as using BM25, RM3, and QL scores as features, retrieved with Galago, without mentioning any additional preprocessing or smoothing of those scores. In practice, the feature files supplied by the authors for training the LTR baselines appear to smooth retrieval scores by replacing zeros with a small epsilon for cases like query\u2013question pairs with no term overlap, which would otherwise yield BM25=0. The released repository does not include code to produce these LTR feature files (they were generated offline), and there is no mention of such smoothing in the methods description. Therefore, there is a clear discrepancy between the paper\u2019s description (using raw retrieval scores) and the implementation artifacts (smoothed feature values).","relevant_paper_sections":["Question retrieval: BM25, RM3, QL: we index all the questions using Galago. Then, for a given query we retrieve the documents using BM25 [38], RM3 [25], and QL [30] models.","LambdaMART, RankNet: for every query-question pair, we use the scores obtained by BM25, RM3, and QL as features to train LambdaMART [48] and RankNet [10] implemented in RankLib."],"relevant_code_files":["README.md","src/utils/constants.py"],"discrepancy_id":"71f50403","removed_in_postproc":false}
{"paper_url":"https://aclanthology.org/D18-1032.pdf","paper_url_versioned":"https://aclanthology.org/D18-1032.pdf","code_url":"https://github.com/1049451037/GCN-Align","code_url_versioned":"https://github.com/1049451037/GCN-Align/tree/4fc90c438e5a609b96df03daff170fbcf03fde94","code_license":null,"discrepancy_date":"2020-04-08T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://link.springer.com/content/pdf/10.1007/978-3-030-45442-5_1.pdf","origin_discrepancy_text":"The paper defines adjacency weights using relation functionality and inverse functionality ratios, without any clipping. In contrast, the code clamps these functionality measures to a minimum of 0.3 when constructing the adjacency matrix. This modification changes the edge weighting scheme relative to the paper\u2019s formula.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper defines adjacency weights purely as sums of relation functionality and inverse functionality ratios, while the code clamps these ratios to be at least 0.3 when constructing the adjacency matrix. This clipping is not described in the paper and changes the edge weighting scheme.","discrepancy_description":"The paper specifies that the connectivity matrix A is built by summing over relations using functionality fun(r) = #Head_Entities_of_r / #Triples_of_r and inverse functionality ifun(r) = #Tail_Entities_of_r / #Triples_of_r, and sets A_ij = \u2211_{(e_i,r,e_j)} ifun(r) + \u2211_{(e_j,r,e_i)} fun(r), with no mention of clipping. In the code, these same measures are computed, but when accumulating edge weights, the implementation replaces fun(r) and ifun(r) with max(fun(r), 0.3) and max(ifun(r), 0.3), effectively clamping the minimum edge contribution per triple to 0.3. Thus, the implemented adjacency matrix deviates from the paper by enforcing a minimum weight per relation contribution, which is not part of the paper\u2019s formula.","relevant_paper_sections":["Computation of Connectivity Matrix. In a GCN model, the connectivity matrix A defines the neighborhoods of entities in the convolutional computation. ... Therefore, we compute two measures, which are called functionality and inverse functionality, for each relation:","& \text { fun }(r)=\frac{\\# \text { Head_Entities_of_ } r}{\\# \text { Triples_of_ } r} \\\\ & \text { ifun }(r)=\frac{\\# \text { Tail_Entities_of_ } r}{\\# \text { Triples_of_ } r}","To measure the influence of the i-th entity over the j-the entity, we set a_{ij} \\in A as: a_{i j}=\\sum_{\\left\\langle e_{i}, r, e_{j}\right\rangle \\in G} i f u n(r)+\\sum_{\\left\\langle e_{j}, r, e_{i}\right\rangle \\in G} f u n(r)"],"relevant_code_files":["utils.py","train.py"],"discrepancy_id":"02922e82","removed_in_postproc":false}
{"paper_url":"https://dl.acm.org/doi/pdf/10.1145/3404835.3462891","paper_url_versioned":"https://dl.acm.org/doi/pdf/10.1145/3404835.3462891","code_url":"https://github.com/sebastian-hofstaetter/matchmaker","code_url_versioned":"https://github.com/sebastian-hofstaetter/matchmaker/tree/210b9da0c46ee6b672f59ffbf8603e0f75edb2b6","code_license":"Apache License 2.0","discrepancy_date":"2023-07-18T00:00:00.000Z","origin_type":"Reproducibility Paper","origin_url":"https://dl.acm.org/doi/pdf/10.1145/3539618.3591915","origin_discrepancy_text":"Validation set sampling for early stopping differs between the paper and the released code. The paper describes uniformly sampling a small subset of queries for validation, whereas the code stratifies the validation set by per-query effectiveness using a baseline model (binning by score and sampling uniformly across bins). This mismatch could affect early-stopping behavior and reported validation metrics.","is_valid_discrepancy":true,"is_valid_discrepancy_reason":"The paper states that the early-stopping validation set is formed by uniformly sampling queries, but the released code includes a script that creates this validation set by stratifying queries into bins based on per\u2011query effectiveness from a baseline and sampling uniformly across bins. The training loop expects a prebuilt validation file, and the provided script to build it uses stratified sampling, not uniform.","discrepancy_description":"The paper describes constructing an approximate early-stopping set by uniformly sampling 3,200 queries from a larger DEV set, retrieving top-100 baseline candidates, and adding any missing relevant passages. In the code, the early-stopping validation file is expected to be prepared beforehand (validation_cont.tsv), and the provided helper script generate_smart_earlystopping_retrieval.py creates this file by first computing a per\u2011query effectiveness metric from a baseline, binning queries into 5 bins by that metric, and then sampling an equal number of queries from each bin (targeting 4,000 total), before collecting top-100 candidates and adding missing relevant passages. Thus, the paper\u2019s uniform sampling differs from the code\u2019s stratified-by-effectiveness sampling (and total query count also differs), which could change early-stopping behavior and validation measurements.","relevant_paper_sections":["We created an approximated early stopping set for all our experiments by indexing a pairwise trained baseline model and retrieving the top 100 passages for 3,200 queries uniformly sampled from the larger DEV-49K set, which are distinct from the DEV-7K and TREC evaluation sets. Additionally, we added all relevant passages if they have not been retrieved by the baseline already. Evaluating our early stopping set takes 5 minutes and we evaluate it every 4 K steps; we stop training a model after 30 evaluations have not improved the nDCG@10 metric, which usually stops after 700-800K steps."],"relevant_code_files":["preprocessing/generate_smart_earlystopping_retrieval.py","matchmaker/train.py","config/train/defaults.yaml"],"discrepancy_id":"6b3125af","removed_in_postproc":false}
